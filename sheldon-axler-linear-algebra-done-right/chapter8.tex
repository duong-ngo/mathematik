\chapter{Operators on Complex Vector Spaces}

\section{Generalized Eigenvectors and Nilpotent Operators}

% chapter8:sectionA:exercise1
\begin{exercise}\label{chapter8:sectionA:exercise1}
    Suppose $T\in\lmap{V}$. Prove that if $\dim\kernel{T^{4}} = 8$ and $\dim\kernel{T^{6}} = 9$, then $\dim\kernel{T^{m}} = 9$ for all integers $m\geq 5$.
\end{exercise}

\begin{proof}
    Because $\kernel{T^{4}}\subseteq \kernel{T^{5}}\subseteq \kernel{T^{6}}$, then
    \[
        \dim\kernel{T^{4}}\leq \dim\kernel{T^{5}}\leq \dim\kernel{T^{6}}.
    \]

    So $8\leq \dim\kernel{T^{5}}\leq 9$. If $\dim\kernel{T^{5}} = 8$ then $\dim\kernel{T^{m}} = 8$ for every $m\geq 4$, which is a contradiction to $\dim\kernel{T^{6}} = 9$. Therefore $\dim\kernel{T^{5}} = 9 = \dim\kernel{T^{6}}$. Hence $\dim\kernel{T^{m}} = 9$ for all integers $m\geq 5$.
\end{proof}
\newpage

% chapter8:sectionA:exercise2
\begin{exercise}\label{chapter8:sectionA:exercise2}
    Suppose $T\in\lmap{V}$, $m$ is a positive integer, $v\in V$, and $T^{m-1}v\ne 0$ but $T^{m}v = 0$. Prove that $v, Tv, T^{2}v, \ldots, T^{m-1}v$ is linearly independent.
\end{exercise}

\begin{proof}
    Assume
    \[
        a_{0}v + a_{1}Tv + a_{2}T^{2}v + \cdots + a_{m-1}T^{m-1}v = 0.
    \]

    So
    \[
        0 = T^{m-1}(a_{0}v + a_{1}Tv + a_{2}T^{2}v + \cdots + a_{m-1}T^{m-1}v) = a_{0}T^{m-1}v
    \]

    which implies $a_{0} = 0$ because $T^{m-1}v\ne 0$.

    Assume $a_{j} = 0$ for every $0\leq j < k \leq m-1$, then we have
    \[
        a_{k}T^{k}v + \cdots + a_{m-1}T^{m-1}v = 0.
    \]

    Apply $T^{m-k-1}$, we obtain that
    \[
        0 = T^{m-k-1}(a_{k}T^{k}v + \cdots + a_{m-1}T^{m-1}v) = a_{k}T^{m-1}v.
    \]

    Because $T^{m-1}v\ne 0$, we conclude that $a_{k} = 0$. By the principle of mathematical induction, $a_{k} = 0$ for every $k\in\{0,1,\ldots,m-1\}$.

    Thus $v, Tv, T^{2}v, \ldots, T^{m-1}v$ is linearly independent.
\end{proof}
\newpage

% chapter8:sectionA:exercise3
\begin{exercise}\label{chapter8:sectionA:exercise3}
    Suppose $T\in\lmap{V}$. Prove that
    \[
        V = \kernel{T}\oplus\range{T} \Longleftrightarrow \kernel{T}^{2} = \kernel{T}.
    \]
\end{exercise}

\begin{proof}
    Due to the fundamental theorem of linear maps, $\dim V = \dim\kernel{T} + \dim\range{T}$. On the other hand,
    \[
        \dim{(\kernel{T} + \range{T})} = \dim\kernel{T} + \dim\range{T} - \dim{(\kernel{T} \cap \range{T})}
    \]

    so $V = \kernel{T}\oplus\range{T}$ if and only if $\kernel{T} \cap \range{T} = \{0\}$.

    \bigskip
    $(\Rightarrow)$ $V = \kernel{T}\oplus \range{T}$.

    Then $\kernel{T}\cap \range{T} = \{0\}$. Let $v\in\kernel{T^{2}}$, then $T(Tv) = T^{2}v = 0$. Therefore $Tv\in\kernel{T}$. On the other hand, $Tv\in\range{T}$, so $Tv = 0$, which means $v\in\kernel{T}$. Hence $\kernel{T^{2}}\subseteq\kernel{T}$.

    Moreover, $\kernel{T}\subseteq\kernel{T^{2}}$. Therefore $\kernel{T^{2}} = \kernel{T}$.

    \bigskip
    $(\Leftarrow)$ $\kernel{T^{2}} = \kernel{T}$.

    Let $v\in \kernel{T}\cap\range{T}$. Then $Tv = 0$ and there exists $u\in V$ such that $Tu = v$. Hence $T^{2}u = Tv = 0$, so $u\in\kernel{T^{2}} = \kernel{T}$. Therefore $v = Tu = 0$, so $\kernel{T}\cap\range{T} = \{0\}$. Thus $V = \kernel{T}\oplus\range{T}$.
\end{proof}
\newpage

% chapter8:sectionA:exercise4
\begin{exercise}\label{chapter8:sectionA:exercise4}
    Suppose $T\in\lmap{V}$, $\lambda\in\mathbb{F}$, and $m$ is a positive integer such that the minimal polynomial of $T$ is a polynomial multiple of ${(z - \lambda)}^{m}$. Prove that
    \[
        \dim\kernel{{(T - \lambda I)}^{m}} \geq m.
    \]
\end{exercise}

\begin{proof}
    Let the minimal polynomial of $T$ be $p(z) {(z - \lambda)}^{m}$. Due to the definition of minimal polynomial, there exists a vector $v\in V$ such that $p(T){(T - \lambda I)}^{m-1}v\ne 0$.

    I will prove that the list $p(T)v, p(T){(T - \lambda I)}v, \ldots, p(T){(T - \lambda I)}^{m-1}v$ is linearly independent. Assume
    \[
        a_{0}p(T)v + a_{1}p(T){(T - \lambda I)}v + \cdots + a_{m-1}p(T){(T - \lambda I)}^{m-1}v = 0.
    \]

    Apply ${(T - \lambda I)}^{m-1}$ to both sides, we obtain $a_{0}p(T){(T - \lambda I)}^{m-1}v = 0$, so $a_{0} = 0$ because
    \[
        p(T){(T - \lambda I)}^{m-1}v\ne 0.
    \]

    Suppose $a_{j} = 0$ for every $0\leq j < k\leq m-1$, then
    \[
        a_{k}p(T){(T - \lambda I)}^{k}v + \cdots + a_{m-1}p(T){(T - \lambda I)}^{m-1}v = 0.
    \]

    Apply ${(T - \lambda I)}^{m-k-1}$ to both side, we get $a_{k}p(T){(T - \lambda I)}^{m-1}v = 0$, so $a_{k} = 0$.

    Hence $a_{k} = 0$ for every $k\in\{ 0, 1,\ldots, m-1 \}$, by the principle of mathematical induction. So $p(T)v, p(T)(T - \lambda I)v, \ldots, p(T){(T - \lambda I)}^{m-1}v$ is a linearly independent list of length $m$.

    Moreover, $p(T)v, p(T)(T - \lambda I)v, \ldots, p(T){(T - \lambda I)}^{m-1}v$ are in $\kernel{{(T - \lambda I)}^{m}}$, since the minimal polynomial of $T$ is $p(z){(z - \lambda)}^{m}$. Thus $\dim\kernel{{(T - \lambda I)}^{m}}\geq m$.
\end{proof}
\newpage

% chapter8:sectionA:exercise5
\begin{exercise}\label{chapter8:sectionA:exercise5}
    Suppose $T\in\lmap{V}$ and $m$ is a positive integer. Prove that
    \[
        \dim\kernel{T^{m}}\leq m\dim\kernel{T}.
    \]
\end{exercise}

\begin{quote}
    Hint: Exercise~\ref{chapter3:sectionB:exercise21} may be useful.
\end{quote}

\begin{proof}
    The inequality is true for $m = 1$.

    Assume the inequality is true for $m = k$. By Exercise~\ref{chapter3:sectionB:exercise22} and the induction hypothesis,
    \[
        \dim\kernel{T^{k+1}} \leq \dim\kernel{T^{k}} + \dim\kernel{T} \leq k\dim\kernel{T} + \dim\kernel{T} = (k + 1)\dim\kernel{T}.
    \]

    By the principle of mathematical induction, for every positive integer $m$,
    \[
        \dim\kernel{T^{m}} \leq m\dim\kernel{T}.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionA:exercise6
\begin{exercise}\label{chapter8:sectionA:exercise6}
    Suppose $T\in\lmap{V}$. Show that
    \[
        V = \range{T^{0}} \supseteq \range{T^{1}} \supseteq \cdots \supseteq \range{T^{k}} \supseteq \range{T^{k+1}} \supseteq \cdots.
    \]
\end{exercise}

\begin{proof}
    $V = \range{I} = \range{T^{0}}$.

    Let $k$ be a nonnegative integer. Assume $v\in \range{T^{k+1}}$, then there exists $u\in V$ such that $T^{k}(Tu) = T^{k+1}u = v$. So $v\in \range{T^{k}}$. Hence $\range{T^{k}}\supseteq \range{T^{k+1}}$ for every nonnegative integer $k$. Thus
    \[
        V = \range{T^{0}}\supseteq \range{T}\supseteq \cdots \supseteq \range{T^{k}}\supseteq \range{T^{k+1}}\supseteq \cdots.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionA:exercise7
\begin{exercise}\label{chapter8:sectionA:exercise7}
    Suppose $T\in\lmap{V}$ and $m$ is a nonnegative integer such that
    \[
        \range{T^{m}} = \range{T^{m+1}}.
    \]

    Prove that $\range{T^{k}} = \range{T^{m}}$ for all $k > m$.
\end{exercise}

\begin{quote}[Additional notes]
    I don't know whether it is possible to prove this result without the fundamental theorem of linear maps.
\end{quote}

\begin{proof}
    Because $\kernel{T^{m}}\subseteq \kernel{T^{m+1}}$ and by the fundamental theorem of linear maps, we conclude that $\kernel{T^{m}} = \kernel{T^{m+1}}$.

    Therefore, $\kernel{T^{k}} = \kernel{T^{m}}$ for all $k > m$. Once again, by the fundamental theorem of linear maps, $\dim\range{T^{k}} = \dim\range{T^{m}}$. Moreover, $\range{T^{m}}\supseteq \range{T^{k}}$ for all $k > m$. So $\range{T^{k}} = \range{T^{m}}$ for all $k > m$.
\end{proof}
\newpage

% chapter8:sectionA:exercise8
\begin{exercise}\label{chapter8:sectionA:exercise8}
    Suppose $T\in\lmap{V}$. Prove that
    \[
        \range{T^{\dim V}} = \range{T^{\dim V + 1}} = \range{T^{\dim V + 2}} = \cdots.
    \]
\end{exercise}

\begin{quote}[Additional notes]
    I don't know whether it is possible to prove this result without the fundamental theorem of linear maps.
\end{quote}

\begin{proof}
    By Exercise~\ref{chapter8:sectionA:exercise6},
    \[
        \range{T^{\dim V}} \supseteq \range{T^{\dim V + 1}} \supseteq \range{T^{\dim V + 2}} \supseteq \cdots.
    \]

    On the other hand,
    \[
        \kernel{T^{\dim V}} = \kernel{T^{\dim V + 1}} = \kernel{T^{\dim V + 2}} = \cdots.
    \]

    By the fundamental theorem of linear maps, for every nonnegative integer $k$,
    \[
        \dim\range{T^{\dim V + k}} = \dim V - \dim\kernel{T^{\dim V + k}} = \dim V - \dim\kernel{T^{\dim V}} = \dim\range{T^{\dim V}}
    \]

    so $\range{T^{\dim V + k}} = \range{T^{\dim V}}$ for every nonnegative integer $k$. Thus
    \[
        \range{T^{\dim V}} = \range{T^{\dim V + 1}} = \range{T^{\dim V + 2}} = \cdots.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionA:exercise9
\begin{exercise}\label{chapter8:sectionA:exercise9}
    Suppose $T\in\lmap{V}$ and $m$ is a nonnegative integer. Prove that
    \[
        \kernel{T^{m}} = \kernel{T^{m+1}} \Longleftrightarrow \range{T^{m}} = \range{T^{m+1}}.
    \]
\end{exercise}

\begin{proof}
    We have $\kernel{T^{m}} \subseteq \kernel{T^{m+1}}$ and $\range{T^{m}} \subseteq \range{T^{m+1}}$ for every nonnegative integer $m$.

    By the fundamental theorem of linear maps,
    \[
        \dim\kernel{T^{m}} + \dim\range{T^{m}} = \dim V = \dim\kernel{T^{m+1}} + \dim\range{T^{m+1}}
    \]

    so $\dim\kernel{T^{m+1}} - \dim\kernel{T^{m}} = \dim\range{T^{m}} - \dim\range{T^{m+1}}$. Thus
    \[
        \kernel{T^{m}} = \kernel{T^{m+1}} \Longleftrightarrow \range{T^{m}} = \range{T^{m+1}}.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionA:exercise10
\begin{exercise}\label{chapter8:sectionA:exercise10}
    Define $T\in\lmap{\mathbb{C}^{2}}$ by $T(w, z) = (z, 0)$. Find all generalized eigenvectors of $T$.
\end{exercise}

\begin{proof}
    $T^{2}(w, z) = T(z, 0) = (0, 0)$. The minimal polynomial of $T$ is $z^{2}$, so the only eigenvalue of $T$ is $0$. For every $v\in \mathbb{C}^{2}$, ${(T - 0I)}^{2}v = T^{2}v = 0$, so every vector in $\mathbb{C}^{2}$ is a generalized eigenvectors of $T$.
\end{proof}
\newpage

% chapter8:sectionA:exercise11
\begin{exercise}\label{chapter8:sectionA:exercise11}
    Suppose that $T\in\lmap{V}$. Prove that there is a basis of $V$ consisting of generalized eigenvectors of $T$ if and only if the minimal polynomial of $T$ equals $(z - \lambda_{1})\cdots (z - \lambda_{m})$ for some $\lambda_{1}, \ldots, \lambda_{m}\in\mathbb{F}$.
\end{exercise}

\begin{quote}
    This exercise states that the condition for there to be a basis of $V$ consisting
    of generalized eigenvectors of $T$ is the same as the condition for there to be
    a basis with respect to which $T$ has an upper-triangular matrix.

    \textbf{Caution:} If $T$ has an upper-triangular matrix with respect to a basis
    $v_{1}, \ldots, v_{n}$ of $V$, then $v_{1}$ is an eigenvector of $T$ but it is not necessarily true that $v_{2}, \ldots v_{n}$ are generalized eigenvectors of $T$.
\end{quote}

\begin{proof}
    $(\Rightarrow)$ There is a basis of $V$ consisting of generalized eigenvectors of $T$.

    Let such a basis be $e_{1}, \ldots, e_{n}$. For every $j\in\{1,\ldots,n\}$, $e_{j}$ is a generalized eigenvectors of $T$, so there exists a positive integer $r_{j}$ and $\alpha_{j}\in\mathbb{F}$ such that ${(T - \alpha_{j})}^{r_{j}}e_{j} = 0$.

    Let $p(z) = {(z - \alpha_{1})}^{r_{1}}\cdots {(z - \alpha_{n})}^{r_{n}}$, then $p(T)e_{j} = 0$ for every $j\in\{ 1,\ldots, n \}$ (this is true because ${(T - \alpha_{j})}^{r_{j}}e_{j} = 0$ and $a(T)$ commutes with $b(T)$ for all polynomials $a, b$). The minimal polynomial of $T$ is a divisor of $p$, so it is also a product of polynomials of degree $1$, equivalently, the minimal polynomial of $T$ equals $(z - \lambda_{1})\cdots (z - \lambda_{m})$ for some $\lambda_{1}, \ldots, \lambda_{m}\in\mathbb{F}$.

    \bigskip
    $(\Leftarrow)$ The minimal polynomial of $T$ equals $(z - \lambda_{1})\cdots (z - \lambda_{m})$ for some $\lambda_{1}, \ldots, \lambda_{m}\in\mathbb{F}$.

    It is possible that there are duplicated values in the list $\lambda_{1}, \ldots, \lambda_{m}$. We dedupe this list to get the list $\alpha_{1}, \ldots, \alpha_{r}$ of distinct values. Let $n_{j}$ be the numbers of $\alpha_{j}$ in the list $\lambda_{1}, \ldots, \lambda_{m}$ then
    \[
        (z - \lambda_{1})\cdots (z - \lambda_{m}) = {(z - \alpha_{1})}^{n_{1}}\cdots {(z - \alpha_{r})}^{n_{r}}.
    \]

    I give a proof using mathematical induction on $r$.

    If $r = 1$, then ${(T - \alpha_{1}I)}^{r_{1}}v = 0$ for every $v\in V$. Therefore there exists a basis of $V$ consisting of generalized eigenvectors of $T$.

    Suppose the proposition is true for every positive integer $j < r$.

    $V = \kernel{{(T - \alpha_{r}I)}^{\dim V}}\oplus \range{{(T - \alpha_{r}I)}^{\dim V}}$. Let $v$ be a vector in $\range{{(T - \alpha_{r}I)}^{\dim V}}$.

    $U = \range{{(T - \alpha_{r}I)}^{\dim V}}$ is invariant under $T$, because $\range{p(T)}$ is invariant under $T$ for every polynomial $p$. Let $w$ be a vector in $U$, then there exists $u\in U$ such that ${(T - \alpha_{r}I)}^{\dim V}u = w$. We have
    \begin{align*}
        {(T - \alpha_{1}I)}^{n_{1}}\cdots {(T - \alpha_{r-1}I)}^{n_{r-1}}w & = {(T - \alpha_{1}I)}^{n_{1}}\cdots {(T - \alpha_{r-1}I)}^{n_{r-1}}{(T - \alpha_{r}I)}^{\dim V}u                                      \\
                                                                           & = {(T - \alpha_{1}I)}^{n_{1}}\cdots {(T - \alpha_{r-1}I)}^{n_{r-1}}{(T - \alpha_{r}I)}^{n_{r}}{(T - \alpha_{r}I)}^{(\dim V) - n_{r}}u \\
                                                                           & = 0.
    \end{align*}

    Therefore ${(z - \alpha_{1})}^{n_{1}}\cdots {(z - \alpha_{r-1})}^{n_{r-1}}$ is a polynomial multiple of the minimal polynomial of $T\vert_{U}$. Due to the induction hypothesis, there exists a basis of $U$ consisting of generalized eigenvectors of $T\vert_{U}$, which are also generalized eigenvectors of $T$.

    Combine a basis of $\kernel{{(T - \alpha_{r}I)}^{\dim V}}$ and a basis of $\range{{(T - \alpha_{r}I)}^{\dim V}}$, which consist of generalized eigenvectors of $T$, we obtain a basis of $V$ consisting of generalized eigenvectors of $T$. Due to the principle of mathematical induction, there exists a basis of $V$ consisting of generalized eigenvectors of $V$.

    Thus there is a basis of $V$ consisting of generalized eigenvectors of $T$.
\end{proof}

\newpage

% chapter8:sectionA:exercise12
\begin{exercise}\label{chapter8:sectionA:exercise12}
    Suppose $T\in\lmap{V}$ is such that every vector in $V$ is a generalized eigenvector of $T$. Prove that there exists $\lambda\in\mathbb{F}$ such that $T - \lambda I$ is nilpotent.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter8:sectionA:exercise11}, there exists a basis of $V$ consisting of generalized eigenvectors of $T$. Let $\lambda_{1}, \ldots, \lambda_{m}$ be the eigenvalues of $T$.

    Assume $m > 1$. Let $v_{1}$ be a vector in $\kernel{{(T - \lambda_{1}I)}^{\dim V}}$ and $v_{2}$ be a vector in $\kernel{{(T - \lambda_{2}I)}^{\dim V}}$, then $v_{1}, v_{2}$ is a linearly independent list, and $v_{1} + v_{2}\ne 0$.

    Due to the hypothesis, $v_{1} + v_{2}$ is also a generalized eigenvector of $T$.
    \begin{itemize}
        \item If $v_{1} + v_{2}$ corresponds to $\lambda_{1}$ then $v_{2}$ is a generalized eigenvector corresponding to $\lambda_{1}$, which is a contradiction.
        \item If $v_{1} + v_{2}$ corresponds to $\lambda_{2}$ then $v_{1}$ is a generalized eigenvector corresponding to $\lambda_{2}$, which is a contradiction.
        \item If $v_{1} + v_{2}$ corresponds to $\lambda_{k}$ where $k\ne 1$ and $k\ne 2$ then $v_{1} + v_{2}\in\kernel{{(T - \lambda_{k}I)}^{\dim V}}$ and
              \[
                  v_{1} + v_{2} = v_{k} \in \kernel{{(T - \lambda_{k}I)}^{\dim V}} \cap \left(\kernel{{(T - \lambda_{1}I)}^{\dim V}}\oplus\kernel{{(T - \lambda_{2}I)}^{\dim V}}\right)
              \]

              which is impossible because $v_{1} + v_{2} - v_{k} = 0$ implies linear dependence, meanwhile, eigenvectors corresponding to different eigenvalues are linearly independent.
    \end{itemize}

    Hence $m = 1$ ($m\ne 0$ because $T$ has a generalized eigenvector). Thus there exists $\lambda\in\mathbb{F}$ such that $\kernel{{(T - \lambda I)}^{\dim V}} = V$, so $T - \lambda I$ is nilpotent.
\end{proof}
\newpage

% chapter8:sectionA:exercise13
\begin{exercise}\label{chapter8:sectionA:exercise13}
    Suppose $S, T\in\lmap{V}$ and $ST$ is nilpotent. Prove that $TS$ is nilpotent.
\end{exercise}

\begin{proof}
    $ST$ is nilpotent, so there exists a positive integer $k$ such that ${(ST)}^{k} = 0$.
    \[
        {(TS)}^{k+1} = T{(ST)}^{k}S = 0.
    \]

    Therefore $TS$ is nilpotent.
\end{proof}
\newpage

% chapter8:sectionA:exercise14
\begin{exercise}\label{chapter8:sectionA:exercise14}
    Suppose $T\in\lmap{V}$ is nilpotent and $T\ne 0$. Prove that $T$ is not diagonalizable.
\end{exercise}

\begin{proof}
    Assume $T$ is diagonalizable then there exists a basis $e_{1}, \ldots, e_{\dim V}$ of $V$ consisting of eigenvectors of $T$. Because $T$ is nilpotent, if $T$ has an eigenvalue then the eigenvalue must be $0$. Hence the matrix of $T$ with respect to $e_{1}, \ldots, e_{\dim V}$ is the zero matrix. This is a contradiction because $T\ne 0$. Thus $T$ is not diagonalizable.
\end{proof}
\newpage

% chapter8:sectionA:exercise15
\begin{exercise}\label{chapter8:sectionA:exercise15}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$. Prove that $T$ is diagonalizable if and only if every generalized eigenvector of $T$ is an eigenvector of $T$.
\end{exercise}

\begin{proof}
    Suppose every generalized eigenvector of $T$ is an eigenvector of $T$. By the fundamental theorem of algebra, the minimal polynomial of $T$ is a product of polynomials of degree $1$, so by Exercise~\ref{chapter8:sectionA:exercise11}, there is a basis of $V$ consisting generalized eigenvectors of $T$. Because every generalized eigenvector of $T$ is an eigenvector of $T$, then the basis consists of eigenvectors of $T$. Therefore $T$ is diagonalizable.

    \bigskip
    Suppose $T$ is diagonalizable. Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$.

    $T$ is diagonalizable then so is $T - \lambda_{k}I$. On the other hand,
    \[
        {(T - \lambda_{k}I)}\vert_{\kernel{(T - \lambda_{k}I)}^{\dim V}}
    \]

    is nilpotent. By Exercise~\ref{chapter8:sectionA:exercise15}, we conclude that ${(T - \lambda_{k}I)}\vert_{\kernel{(T - \lambda_{k}I)}^{\dim V}} = 0$. Therefore $(T - \lambda_{k}I)v = 0$ if ${(T - \lambda_{k}I)}^{\dim V}v = 0$.

    Hence every generalized eigenvector of $T$ is an eigenvector of $T$.
\end{proof}
\newpage

% chapter8:sectionA:exercise16
\begin{exercise}\label{chapter8:sectionA:exercise16}
    \begin{enumerate}[label={(\alph*)}]
        \item Give an example of nilpotent operators $S$, $T$ on the same vector space such that neither $S + T$ nor $ST$ is nilpotent.
        \item Suppose $S, T \in \lmap{V}$ are nilpotent and $ST = TS$. Prove that $S + T$ and $ST$ are nilpotent.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item On $\mathbb{C}^{2}$, define $S(w, z) = (0, w)$ and $T(w, z) = (z, 0)$.
              \[
                  \begin{split}
                      S^{2}(w, z) = S(0, w) = (0, 0) \\
                      T^{2}(w, z) = T(z, 0) = (0, 0)
                  \end{split}
              \]

              so $S, T$ are nilpotent. However, $S + T = I$ and $(ST)(z, w) = (z, 0)$ and ${ST}\ne 0$, ${(ST)}^{2}\ne 0$, so $S + T$ and $ST$ are not nilpontent.
        \item Because $S, T$ are nilpotent, there exist positive integers $m, n$ such that $S^{m} = 0$ and $T^{n} = 0$.

              Let $p = \max\{ m, n \}$, then because $ST = TS$, we can apply the binomial theorem
              \[
                  {(S + T)}^{2p} = \sum^{2p}_{k=0}\binom{2p}{k}S^{k}T^{2p-k}
              \]

              Because $k + (2p - k) = 2p$ so at least one integer in the list $k, 2p - k$ is greater than or equal to $p$. Therefore ${(S + T)}^{2p} = 0$. Moreover, because $ST = TS$
              \[
                  {(ST)}^{p} = S^{p}T^{p} = 0.
              \]

              Hence $S + T$ and $ST$ are nilpotent.
    \end{enumerate}
\end{proof}
\newpage

% chapter8:sectionA:exercise17
\begin{exercise}\label{chapter8:sectionA:exercise17}
    Suppose $T\in \lmap{V}$ is nilpotent and $m$ is a positive integer such that $T^{m} = 0$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $I - T$ is invertible and that ${(I - T)}^{-1} = I + T + \cdots + T^{m-1}$.
        \item Explain how you would guess the formula above.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Assume $I - T$ is not invertible, then $0$ is an eigenvalue of $I - T$. Let $v$ be an eigenvector of $I - T$ corresponding to $0$, then $(I - T)v = 0$, which implies $Tv = v$. Therefore $1$ is an eigenvalue of $T$, this is a contradiction because $T$ is nilpotent. Hence $I - T$ is invertible.
              \[
                  \begin{split}
                      (I - T)(I + T + \cdots + T^{m-1}) = (I - T) + (T - T^{2}) + \cdots + (T^{m-1} - T^{m}) = I - T^{m} = I, \\
                      (I + T + \cdots + T^{m-1})(I - T) = (I + T + \cdots + T^{m-1}) - (T + T^{2} + \cdots + T^{m}) = I - T^{m} = I.
                  \end{split}
              \]

              Thus ${(I - T)}^{-1} = I + T + \cdots + T^{m-1}$.
        \item I could have guessed the formula if I had thought of the Taylor expansion of ${(1 - x)}^{-1}$
              \[
                  {(1 - x)}^{-1} = 1 + x + x^{2} + \cdots.
              \]
    \end{enumerate}
\end{proof}
\newpage

% chapter8:sectionA:exercise18
\begin{exercise}\label{chapter8:sectionA:exercise18}
    Suppose $T\in\lmap{V}$ is nilpotent. Prove that $T^{1 + \dim\range{T}} = 0$.
\end{exercise}

\begin{quote}
    If $\dim\range{T} < \dim V - 1$, then this exercise improves 8.16.
\end{quote}

\begin{proof}
    Let $v$ be an arbitrary vector in $V$, then $Tv\in\range{T}$. $T$ is nilpotent, then so is $T\vert_{\range{T}}$. Moreover, $\range{T}$ is invariant under $T$, so $T\vert_{\range{T}}$ is an operator on $\range{T}$ and we have
    \[
        {(T\vert_{\range{T}})}^{\range{T}} = 0.
    \]

    Therefore
    \[
        T^{1 + \dim\range{T}}v = T^{\dim\range{T}}(Tv) = {(T\vert_{\range{T}})}^{\dim\range{T}}(Tv) = 0.
    \]

    Thus $T^{1 + \dim\range{T}} = 0$.
\end{proof}
\newpage

% chapter8:sectionA:exercise19
\begin{exercise}\label{chapter8:sectionA:exercise19}
    Suppose $T\in\lmap{V}$ is not nilpotent. Show that
    \[
        V = \kernel{T^{\dim V - 1}} \oplus \range{T^{\dim V - 1}}.
    \]
\end{exercise}

\begin{quote}
    For operators that are not nilpotent, this exercise improves 8.4.
\end{quote}

\begin{proof}
    Because $T$ is not nilpotent, $T^{\dim V}\ne 0$, equivalently, $\kernel{T^{\dim V}}\ne V$. Since
    \[
        \{0\} = \kernel{T^{0}}\subseteq \kernel{T}\subseteq \cdots \subseteq \kernel{T^{\dim V}} \subsetneq V
    \]

    so there exists $k < \dim V$ such that $\kernel{T^{k}} = \kernel{T^{k+1}}$. Therefore
    \[
        \dim \kernel{T^{k}} = \dim \kernel{T^{\dim V - 1}} = \dim \kernel{T^{\dim V}}.
    \]

    Let $v$ be a vector in $\kernel{T^{\dim V - 1}} \cap \range{T^{\dim V - 1}}$, then
    \[
        T^{\dim V - 1}v = 0
    \]

    and there exists a vector $u$ in $V$ such that $T^{\dim V - 1}u = v$. Therefore $T^{2\dim V - 2}u = T^{\dim V - 1}v = 0$, so
    \[
        u\in \kernel{T^{2\dim V - 2}} = \kernel{T^{\dim V - 1}}.
    \]

    Hence $v = T^{\dim V - 1}u = 0$, so $\kernel{T^{\dim V - 1}} \cap \range{T^{\dim V - 1}} = \{0\}$, which implies
    \[
        \kernel{T^{\dim V - 1}} + \range{T^{\dim V - 1}} = \kernel{T^{\dim V - 1}} \oplus \range{T^{\dim V - 1}}.
    \]

    $\kernel{T^{\dim V - 1}} \oplus \range{T^{\dim V - 1}}$ is a subspace of $V$, and by the fundamental theorem of linear maps, the dimension of $\kernel{T^{\dim V - 1}} \oplus \range{T^{\dim V - 1}}$ is $\dim V$. Thus
    \[
        V = \kernel{T^{\dim V - 1}} \oplus \range{T^{\dim V - 1}}.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionA:exercise20
\begin{exercise}\label{chapter8:sectionA:exercise20}
    Suppose $V$ is an inner product space and $T\in\lmap{V}$ is normal and nilpotent. Prove that $T = 0$.
\end{exercise}

\begin{proof}
    Because $T$ is nilpotent, there exists a positive integer $k$ such that $T^{k} = 0$, so ${(T^{*})}^{k} = 0$. $T$ is normal so $T$ commutes with $T^{*}$, therefore
    \[
        {(T^{*}T)}^{k} = {(T^{*})}^{k}T^{k} = 0.
    \]

    $T^{*}T$ is diagonalizable according to the real and complex spectral theorem. Moreover, $T^{*}T$ is nilpotent. By Exercise~\ref{chapter8:sectionA:exercise14}, $T^{*}T = 0$. By Exercise~\ref{chapter7:sectionA:exercise2}, we conclude that $T = 0$.
\end{proof}
\newpage

% chapter8:sectionA:exercise21
\begin{exercise}\label{chapter8:sectionA:exercise21}
    Suppose $T\in\lmap{V}$ is such that $\kernel{T^{\dim V - 1}} \ne \kernel{T^{\dim V}}$. Prove that $T$ is nilpotent and that $\dim\kernel{T^{k}} = k$ for every $k$ with $0\leq k\leq \dim V$.
\end{exercise}

\begin{proof}
    For every $k\geq 0$,
    \[
        \kernel{T^{k}}\subseteq \kernel{T^{k+1}}.
    \]

    Assume there is a nonnegative integer $k$ less than $\dim V - 1$ such that $\kernel{T^{k}} = \kernel{T^{k+1}}$, then for every positive integer $m > k$
    \[
        \kernel{T^{m}} = \kernel{T^{k}}
    \]

    so $\kernel{T^{\dim V - 1}} = \kernel{T^{\dim V}}$, which is a contradiction. Hence, together with the hypothesis, we obtain $\dim\kernel{T^{k}} < \dim\kernel{T^{k+1}}$ (so $\dim\kernel{T^{k}} + 1 \leq \dim\kernel{T^{k+1}}$) for every $0\leq k\leq \dim V-1$.

    Therefore $\dim\kernel{T^{\dim V}}\geq \dim\kernel{T^{0}} + \dim V = \dim V$, so $\dim\kernel{T^{\dim V}} = \dim V$, which means $\kernel{T^{\dim V}} = V$, so $T^{\dim V} = 0$. Hence $T$ is nilpotent.

    For every $k$ with $0\leq k\leq \dim V$,
    \begin{align*}
        \dim\kernel{T^{k}} & \geq \dim\kernel{T^{0}} + k = k,                                         \\
        \dim\kernel{T^{k}} & \leq \dim\kernel{T^{\dim V}} - (\dim V - k) = \dim V - (\dim V - k) = k.
    \end{align*}

    Hence $\dim\kernel{T^{k}} = k$.
\end{proof}
\newpage

% chapter8:sectionA:exercise22
\begin{exercise}\label{chapter8:sectionA:exercise22}
    Suppose $T \in \lmap{\mathbb{C}^{5}}$ is such that $\range{T^{4}} \ne \range T^{5}$. Prove that $T$ is nilpotent.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter8:sectionA:exercise21}, $T^{5} = 0$. Hence $T$ is nilpotent.
\end{proof}
\newpage

% chapter8:sectionA:exercise23
\begin{exercise}\label{chapter8:sectionA:exercise23}
    Give an example of an operator $T$ on a finite-dimensional real vector space such that $0$ is the only eigenvalue of $T$ but $T$ is not nilpotent.
\end{exercise}

\begin{proof}
    I come up with the following example by using companion matrix.

    On $\mathbb{R}^{3}$, define $T(x, y, z) = (0, x - z, y)$. Then
    \begin{align*}
        T^{2}(x, y, z) & = T(0, x-z, y) = (0, -y, x-z),  \\
        T^{3}(x, y, z) & = T(0, -y, x-z) = (0, z-x, -y).
    \end{align*}

    The minimal polynomial of $T$ is $z^{3} + z$, which has only one real root $0$, so $0$ is the only eigenvalue of $T$. However $T$ is not nilpotent because its minimal polynomial is $z^{3} + z$.
\end{proof}
\newpage

% chapter8:sectionA:exercise24
\begin{exercise}\label{chapter8:sectionA:exercise24}
    For each item in Example 8.15, find a basis of the domain vector space such that the matrix of the nilpotent operator with respect to that basis has the upper-triangular form promised by 8.18(c).
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item The operator $T\in\lmap{\mathbb{F}^{4}}$ defined by
              \[
                  T(z_{1}, z_{2}, z_{3}, z_{4}) = (0, 0, z_{1}, z_{2}).
              \]

              The matrix of $T$ with respect to the basis $(0, 0, 1, 0), (0, 0, 0, 1)$, $(1, 0, 0, 0)$, $(0, 1, 0, 0)$ is
              \[
                  \begin{pmatrix}
                      0 & 0 & 1 & 0 \\
                      0 & 0 & 0 & 1 \\
                      0 & 0 & 0 & 0 \\
                      0 & 0 & 0 & 0
                  \end{pmatrix}.
              \]
        \item The operator $T\in\lmap{\mathbb{F}^{3}}$ whose matrix with respect to the standard basis is
              \[
                  \begin{pmatrix}
                      -3 & 9 & 0  \\
                      -7 & 9 & 6  \\
                      4  & 0 & -6
                  \end{pmatrix}.
              \]

              The minimal polynomial of $T$ is $z^{3}$. The matrix of $T$ with respect to the basis
              \[
                  (3, 1, 2), (1/2, 1/2, 0), (3, 19/18, 2)
              \]

              is
              \[
                  \begin{pmatrix}
                      0 & 1 & 0 \\
                      0 & 0 & 1 \\
                      0 & 0 & 0
                  \end{pmatrix}.
              \]
        \item The operator of differentiation on $\mathscr{P}_{m}(\mathbb{R})$. The matrix of the differentiation operator with respect to the basis $1, z, \ldots, z^{m}$ is upper triangular. On the other hand, $D$ is nilpotent so the entries on the diagonal of this upper-triangular matrix are $0$. The matrix of $D$ with respect to the basis $1, z, \ldots, z^{m}$ is
              \[
                  \begin{pmatrix}
                      0      & 1      & 0      & 0      & \cdots & 0      \\
                      0      & 0      & 2      & 0      & \cdots & 0      \\
                      0      & 0      & 0      & 3      & \cdots & 0      \\
                      \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
                      0      & 0      & 0      & 0      & \cdots & m      \\
                      0      & 0      & 0      & 0      & \cdots & 0
                  \end{pmatrix}.
              \]
    \end{enumerate}
\end{proof}
\newpage

% chapter8:sectionA:exercise25
\begin{exercise}\label{chapter8:sectionA:exercise25}
    Suppose that $V$ is an inner product space and $T \in \lmap{V}$ is nilpotent. Show that there is an orthonormal basis of $V$ with respect to which the matrix of $T$ has the upper-triangular form promised by 8.18(c).
\end{exercise}

\begin{proof}
    $T$ is nilpotent, so there exists a least positive integer $k\leq \dim V$ such that $T^{k} = 0$. Therefore the minimal polynomial of $T$ is $z^{k}$. Because the minimal polynomial of $T$ is a product of polynomials of degree $1$, so there exists a basis $v_{1}, \ldots, v_{\dim V}$ of $V$ to which $T$ has an upper-triangular matrix $A$.

    $A$ is upper triangular, so $Tv_{k}\in\operatorname{span}(v_{1}, \ldots, v_{k})$ for every $k\in\{1,\ldots,\dim V\}$. Apply the Gram-Schmidt's procedure to the basis $v_{1}, \ldots, v_{\dim V}$, we get an orthonormal basis $e_{1}, \ldots, e_{\dim V}$ of $V$ and this orthonormal basis satisfies $\operatorname{span}(v_{1}, \ldots, v_{k}) = \operatorname{span}(e_{1}, \ldots, e_{k})$ for every $k\in\{1,\ldots,\dim V\}$. Therefore $Te_{k}\in \operatorname{span}(e_{1}, \ldots, e_{k})$ for every $k\in\{1,\ldots,\dim V\}$, so the matrix of $T$ with respect to this orthonormal basis is upper triangular.

    On the other hand, because the minimal polynomial of $T$ is $z^{k}$ so the entries on the diagonal on the upper-triangular matrix of $T$ with respect to $e_{1}, \ldots, e_{\dim V}$ are $0$.

    Thus there is an orthonormal basis of $V$ to which $T$ has an upper-triangular matrix.
\end{proof}
\newpage

\section{Generalized Eigenspace Decomposition}

% chapter8:sectionB:exercise1
\begin{exercise}\label{chapter8:sectionB:exercise1}
    Define $T\in\lmap{\mathbb{C}^{2}}$ by $T(w, z) = (-z, w)$. Find the generalized eigenspaces corresponding to the distinct eigenvalues of $T$.
\end{exercise}

\begin{proof}
    $T^{2}(w, z) = T(-z, w) = (-w, -z)$, so the minimal polynomial of $T$ is $z^{2} + 1$, hence the eigenvalues of $T$ are $\iota$ and $-\iota$.
    \begin{align*}
        {(T - \iota I)}^{2}(w, z) & = (T^{2} - 2\iota T - I)(w, z)     \\
                                  & = (-2\iota T - 2I)(w, z)           \\
                                  & = -2\iota(-z, w) -2(w, z)          \\
                                  & = (2\iota z - 2w, -2\iota w - 2z), \\
        {(T + \iota I)}^{2}(w, z) & = (T^{2} + 2\iota T - I)(w, z)     \\
                                  & = (2\iota T - 2I)(w, z)            \\
                                  & = 2\iota (-z, w) - 2(w, z)         \\
                                  & = (-2\iota z - 2w, 2\iota w - 2z).
    \end{align*}

    Hence $G(\iota, T) = \operatorname{span}((1, -\iota))$, $G(-\iota, T) = \operatorname{span}((1, \iota))$.
\end{proof}
\newpage

% chapter8:sectionB:exercise2
\begin{exercise}\label{chapter8:sectionB:exercise2}
    Suppose $T\in\lmap{V}$ is invertible. Prove that $G(\lambda, T) = G\left(\frac{1}{\lambda}, T^{-1}\right)$ for every $\lambda\in\mathbb{F}$ with $\lambda\ne 0$.
\end{exercise}

\begin{proof}
    $T$ commutes with $T^{-1} - \frac{1}{\lambda}I$.
    \begin{align*}
        v\in G(\lambda, T) & \Longleftrightarrow {(T - \lambda I)}^{\dim V}v = 0                                                                                                        \\
                           & \Longleftrightarrow {(\lambda I - T)}^{\dim V}v = 0                                                                                                        \\
                           & \Longleftrightarrow {(\lambda T^{-1}T - T)}^{\dim V}v = 0                                                                                                  \\
                           & \Longleftrightarrow {\left(T^{-1}T - \frac{1}{\lambda}T\right)}^{\dim V}v = 0                                                                              \\
                           & \Longleftrightarrow T^{\dim V}{\left(T^{-1} - \frac{1}{\lambda}I\right)}^{\dim V}v = 0 & \text{(because $T$ commutes with $T^{-1} - \frac{1}{\lambda}I $)} \\
                           & \Longleftrightarrow {\left(T^{-1} - \frac{1}{\lambda}I\right)}^{\dim V}v = 0           & \text{(because $T$ is invertible)}                                \\
                           & \Longleftrightarrow v\in G\left(\frac{1}{\lambda}, T^{-1}\right).
    \end{align*}

    Thus $G(\lambda, T) = G\left(\frac{1}{\lambda}, T^{-1}\right)$ for every $\lambda\in\mathbb{F}$ with $\lambda\ne 0$.
\end{proof}
\newpage

% chapter8:sectionB:exercise3
\begin{exercise}\label{chapter8:sectionB:exercise3}
    Suppose $T\in\lmap{V}$. Suppose $S\in\lmap{V}$ is invertible. Prove that $T$ and $S^{-1}TS$ have the same eigenvalues with the same multiplicities.
\end{exercise}

\begin{proof}
    Let $p$, $q$ be the minimal polynomials of $T$ and $S^{-1}TS$, respectively. Then by Exercise~\ref{chapter5:sectionA:exercise40}
    \[
        p(S^{-1}TS) = S^{-1}p(T)S = 0\qquad q(T) = q(SS^{-1}TSS^{-1}) = Sq(S^{-1}TS)S^{-1} = 0.
    \]

    So $p$ is a polynomial multiple of $q$ and $q$ is a polynomial multiple of $p$. Therefore $p = q$. The two linear operators have the same minimal polynomial so they have the same eigenvalues.

    Let $\lambda$ be an eigenvalue of $T$ and $S^{-1}TS$.
    \begin{align*}
        v\in \kernel{{(T - \lambda I)}^{\dim V}} & \Longleftrightarrow {(T - \lambda I)}^{\dim V}v = 0                                                              \\
                                                 & \Longleftrightarrow {(SS^{-1}TSS^{-1} - \lambda SIS^{-1})}^{\dim V}v = 0                                         \\
                                                 & \Longleftrightarrow {{(S(S^{-1}TS - \lambda I)S^{-1})}^{\dim V}}v = 0                                            \\
                                                 & \Longleftrightarrow S{(S^{-1}TS - \lambda I)}^{\dim V}S^{-1}v = 0                                                \\
                                                 & \Longleftrightarrow {(S^{-1}TS - \lambda I)}^{\dim V}S^{-1}v = 0            & \text{(because $S$ is invertible)} \\
                                                 & \Longleftrightarrow S^{-1}v \in \kernel{{(S^{-1}TS - \lambda I)}^{\dim V}}.
    \end{align*}

    Therefore $\dim\kernel{{(T - \lambda I)}^{\dim V}} = \dim\kernel{{(S^{-1}TS - \lambda I)}^{\dim V}}$ because there is an isomorphism between the two subspaces. Hence the eigenvalue $\lambda$ of $T$ and $S^{-1}TS$ has the same multiplicity.

    Thus $T$ and $S^{-1}TS$ have the same eigenvalues with the same multiplicities.
\end{proof}
\newpage

% chapter8:sectionB:exercise4
\begin{exercise}\label{chapter8:sectionB:exercise4}
    Suppose $\dim V\geq 2$ and $T\in\lmap{V}$ is such that $\kernel{T^{\dim V - 2}}\ne \kernel{T^{\dim V - 1}}$. Prove that $T$ has at most two distinct eigenvalues.
\end{exercise}

\begin{proof}
    Because
    \[
        \{0\} = \kernel{T^{0}}\subseteq \kernel{T}\subseteq \cdots \subseteq \kernel{T^{\dim V - 2}} \subseteq \kernel{T^{\dim V - 1}}
    \]

    and if $\kernel{T^{m}} = \kernel{T^{m+1}}$, then $\kernel{T^{m}} = \kernel{T^{m+k}}$ for every positive integer $k$, so we conclude that
    \[
        \dim\kernel{T^{\dim V - 1}}\geq \dim V - 1.
    \]

    Let's consider all cases.

    If $\dim\kernel{T^{\dim V - 1}} = \dim V$, then $T^{\dim V - 1} = 0$, so the only eigenvalue of $T$ is $0$.

    If $\dim\kernel{T^{\dim V - 1}} = \dim V - 1$ and $\dim\kernel{T^{\dim V}} = \dim V$, then $T^{\dim V} = 0$, so the only eigenvalue of $T$ is $0$.

    Otherwise, $\dim\kernel{T^{\dim V - 1}} = \dim V - 1$ and $\dim\kernel{T^{\dim V}} = \dim V - 1$, then $T$ is not nilpotent. By Exercise~\ref{chapter8:sectionA:exercise19},
    \[
        V = \kernel{T^{\dim V -  1}}\oplus\range{T^{\dim V - 1}}.
    \]

    By the fundamental theorem of linear maps, $\dim\range{T^{\dim V - 1}} = 1$. Moreover, $\range{T^{\dim V - 1}}$ is invariant under $T$ and has dimension $1$ so $T\vert_{\range{T^{\dim V - 1}}}$ has an eigenvalue. $0$ is also an eigenvalue of $T$, because $\dim\kernel{T^{\dim V - 1}} = \dim V - 1 \geq 1$. So $T$ has at most two distinct eigenvalues.

    In conclusion, $T$ has at most two distinct eigenvalues.
\end{proof}
\newpage

% chapter8:sectionB:exercise5
\begin{exercise}\label{chapter8:sectionB:exercise5}
    Suppose $T\in\lmap{V}$ and $3$ and $8$ are eigenvalues of $T$. Let $n = \dim V$. Prove that $V = {(\kernel{T^{n-2}})}\oplus {(\range{T^{n-2}})}$.
\end{exercise}

\begin{proof}
    Assume $\kernel{T^{n-2}}\ne \kernel{T^{n-1}}$, then by Exercise~\ref{chapter8:sectionB:exercise4}, $T$ has at most two different eigenvalues, including $0$. This is a contradiction since $3$ and $8$ are eigenvalues of $T$. Hence the assumption is false and we conclude that $\kernel{T^{n-2}} = \kernel{T^{n-1}} = \kernel{T^{n}} = \cdots$.

    Therefore $V = \kernel{T^{n}}\oplus\range{T^{n}} = (\kernel{T^{n-2}})\oplus(\range{T^{n-2}})$.
\end{proof}
\newpage

% chapter8:sectionB:exercise6
\begin{exercise}\label{chapter8:sectionB:exercise6}
    Suppose $T \in \lmap{V}$ and $\lambda$ is an eigenvalue of $T$. Explain why the exponent of $z - \lambda$ in the factorization of the minimal polynomial of $T$ is the smallest positive integer $m$ such that ${(T - \lambda I)}^{m}\vert_{G( \lambda, T)} = 0$.
\end{exercise}

\begin{proof}
    Because ${(T - \lambda I)}^{\dim V}\vert_{G(\lambda, T)} = 0$ so there exists a smallest positive integer $m$ such that ${(T - \lambda I)}^{m}\vert_{G(\lambda, T)} = 0$. Therefore the minimal polynomial of $T\vert_{G(\lambda, T)}$ is ${(z - \lambda)}^{m}$.

    Due to the definition of $m$, and three theorems ($\kernel{T^{k}}\subseteq \kernel{T^{k+1}}$ for every nonnegative integer $k$; $\kernel{T^{k}} = \kernel{T^{k+1}}$ implies $\kernel{T^{k+m}} = \kernel{T^{k+m+1}}$ for every positive integer $m$; $\kernel{T^{\dim V}} = \kernel{T^{\dim V + 1}} = \cdots$), it follows that
    \[
        \kernel{{(T - \lambda I)}^{m}} = \kernel{{(T - \lambda I)}^{\dim V}}.
    \]

    Moreover,
    \[
        V = \kernel{{(T - \lambda I)}^{\dim V}}\oplus\range{{(T - \lambda I)}^{\dim V}}
    \]

    and $\kernel{{(T - \lambda I)}^{k}} = \kernel{{(T - \lambda I)}^{\dim V}}$ if and only if $\range{{(T - \lambda I)}^{k}} = \range{{(T - \lambda I)}^{\dim V}}$, so
    \[
        V = \kernel{{(T - \lambda I)}^{m}}\oplus\range{{(T - \lambda I)}^{m}}.
    \]

    Let $\mu_{T}$ be the minimal polynomial of $T$, then ${(z - \lambda)}^{m}$ divides $\mu_{T}$, because $\mu_{T}(v) = 0$ for every $v\in G(\lambda, T)$. Therefore $m\leq m_{\lambda}$, where $m_{\lambda}$ is the exponent of ${z - \lambda}$ in the factorization of $\mu_{T}$.

    $\kernel{{(T - \lambda I)}^{m}}$ and $\range{{(T - \lambda I)}^{m}}$ are invariant subspaces under $T$. Let $q$ be the minimal polynomial of $T\vert_{\range{{(T - \lambda I)}^{m}}}$. For every $v\in V$, there exist unique vectors $u\in\kernel{{(T - \lambda I)}^{m}}$ and $w\in\range{{(T - \lambda I)}^{m}}$ such that $v = u + w$. Because
    \[
        {(T - \lambda I)}^{m}q(T)w = 0 = q(T){(T - \lambda I)}^{m}u = 0
    \]

    so ${(T - \lambda I)}^{m}q(T)v = 0$ for every $v\in V$. Hence ${(z - \lambda)}^{m}q(z)$ is a polynomial multiple of $\mu_{T}$, so the exponent of $z - \lambda$ in  the factorization of ${(z - \lambda)}^{m}q(z)$ is greater than or equal to that of the factorization of $\mu_{T}$. On the other hand, $z - \lambda$ does not divide $q(z)$ because $\lambda$ is not a generalized eigenvector of $T\vert_{\range{{(T - \lambda I)}^{m}}}$, due to the direct sum $V = \kernel{{(T - \lambda I)}^{m}}\oplus\range{{(T - \lambda I)}^{m}}$. Therefore the exponent of $z - \lambda$ in the factorization of ${(z - \lambda)}^{m}q(T)$ is $m$.

    Thus $m = m_{\lambda}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise7
\begin{exercise}\label{chapter8:sectionB:exercise7}
    Suppose $T \in \lmap{V}$ and $\lambda$ is an eigenvalue of $T$ with multiplicity $d$. Prove that $G(\lambda, T) = \kernel{{(T - \lambda I)}^{d}}$.
\end{exercise}

\begin{proof}
    Due to the definition of (algebraic) multiplicity, $d = \dim\kernel{{(T - \lambda I)}^{\dim V}}$. Because $d\leq\dim V$, then $\kernel{{(T - \lambda I)}^{d}}\subseteq G(\lambda, T) = \kernel{{(T - \lambda I)}^{\dim V}}$.

    The operator ${(T - \lambda I)}\vert_{G(\lambda, T)}$ is nilpotent, so there exists a basis $v_{1}, \ldots, v_{d}$ of $G(\lambda, T)$ to which the matrix of ${(T - \lambda I)}\vert_{G(\lambda, T)}$ is upper triangular with entries on the diagonal are $0$.

    Therefore $(T - \lambda I)v_{1} = 0$.

    Assume ${(T - \lambda I)}^{k}v_{k} = 0$ for $1\leq k < d$, then $Tv_{k+1} \in \operatorname{span}(v_{1}, \ldots, v_{k})$ (because the matrix is upper triangular and the entry in the $(k+1)$th row and $(k+1)$th column is $0$). By the induction hypothesis, ${(T - \lambda I)}^{k+1}v_{k+1} = 0$. By the principle of mathematical induction, we conclude that ${(T - \lambda I)}^{k}v_{k} = 0$ for every $k\in\{1,\ldots,d\}$. Therefore ${(T - \lambda I)}^{d}v_{k} = 0$ for every $k\in\{1,\ldots,d\}$, so ${(T - \lambda I)}^{d}v = 0$ for every $v\in G(\lambda, T)$. Hence $G(\lambda, T)\subseteq \kernel{{(T - \lambda I)}^{d}}$.

    Thus $G(\lambda, T) = \kernel{{(T - \lambda I)}^{d}}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise8
\begin{exercise}\label{chapter8:sectionB:exercise8}
    Suppose $T\in\lmap{V}$ and $\lambda_{1}, \ldots, \lambda_{m}$ are the distinct eigenvalues of $T$. Prove that
    \[
        V = G(\lambda_{1}, T)\oplus \cdots \oplus G(\lambda_{m}, T)
    \]

    if and only if the minimal polynomial of $T$ equals ${(z - \lambda_{1})}^{k_{1}}\cdots {(z - \lambda_{m})}^{k_{m}}$ for some positive integers $k_{1}, \ldots, k_{m}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $V = G(\lambda_{1}, T)\oplus \cdots \oplus G(\lambda_{m}, T)$.

    Let $r_{n}$ be the smallest positive integer such that ${(T - \lambda_{n}I)}^{r_{n}}\vert_{G(\lambda_{n}, T)} = 0$. For every $v\in V$, there exist unique vectors $v_{1}, \ldots, v_{m}$ in $G(\lambda_{1}, T), \ldots, G(\lambda_{m}, T)$, respectively such that $v = v_{1} + \cdots + v_{m}$.
    \[
        {(T - \lambda_{1}I)}^{r_{1}}\cdots {(T - \lambda_{m}I)}^{r_{m}}v_{n} = 0
    \]

    so
    \[
        {(T - \lambda_{1}I)}^{r_{1}}\cdots {(T - \lambda_{m}I)}^{r_{m}}v = 0.
    \]

    Therefore ${(z - \lambda_{1})}^{r_{1}}\cdots {(z - \lambda_{m})}^{r_{m}}$ is a polynomial multiple of the minimal polynomial of $T$. Because $\lambda_{1}, \ldots, \lambda_{m}$ are the distinct eigenvalues of $T$, so $z - \lambda_{1}, \ldots, z - \lambda_{m}$ are divisors of the minimal polynomial of $T$.

    Hence the minimal polynomial of $T$ equals ${(z - \lambda_{1})}^{k_{1}}\cdots {(z - \lambda_{m})}^{k_{m}}$ for some positive integers $k_{1}, \ldots, k_{m}$.
    \bigskip

    $(\Leftarrow)$ The minimal polynomial of $T$ equals ${(z - \lambda_{1})}^{k_{1}}\cdots {(z - \lambda_{m})}^{k_{m}}$ for some positive integers $k_{1}, \ldots, k_{m}$.

    I give a proof using mathematical induction on $m$.

    If $m = 1$, then ${(T - \lambda_{1})}^{k_{1}}v = 0$ for every $v\in V$, so $V = G(\lambda_{1}, T)$.

    Assume the result is true for all $m < n$. By the proof of Exercise~\ref{chapter8:sectionB:exercise6}
    \[
        V = \kernel{{(T - \lambda_{n}I)}^{k_{n}}}\oplus\range{{(T - \lambda_{n}I)}^{k_{n}}}
    \]

    and $G(\lambda_{n}, T) = \kernel{{(T - \lambda_{n})}^{k_{n}}}$. The subspace $\range{{(T - \lambda_{n}I)}^{k_{n}}}$ is invariant under $T$ and the minimal polynomial of $T\vert_{\range{{(T - \lambda_{n}I)}^{k_{n}}}}$ is a polynomial multiple of ${(z - \lambda_{1})}^{k_{1}}\cdots {(z - \lambda_{n-1})}^{k_{n-1}}$. So by the induction hypothesis,
    \[
        \range{{(T - \lambda_{n}I)}^{k_{n}}} = G(\lambda_{1}, T)\oplus \cdots \oplus G(\lambda_{n-1}, T).
    \]

    Hence
    \[
        V = \range{{(T - \lambda_{n}I)}^{k_{n}}} \oplus \kernel{{(T - \lambda_{n}I)}^{k_{n}}} = G(\lambda_{1}, T)\oplus \cdots \oplus G(\lambda_{n-1}, T) \oplus G(\lambda_{n}, T).
    \]

    Thus, by the principle of mathematical induction,
    \[
        V = G(\lambda_{1}, T)\oplus \cdots \oplus G(\lambda_{m}, T).\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionB:exercise9
\begin{exercise}\label{chapter8:sectionB:exercise9}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$. Prove that there exist $D, N\in \lmap{V}$ such that $T = D + N$, the operator $D$ is diagonalizable, $N$ is nilpotent, and $DN = ND$.
\end{exercise}

\begin{proof}
    Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$.

    Because $V$ is a complex vector space, then there exists a basis $v_{1}, \ldots, v_{\dim V}$ of $V$ to which the matrix of $T$ is the following block diagonal matrix
    \[
        \begin{pmatrix}
            A_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & A_{m}
        \end{pmatrix}
    \]

    where $A_{k}$ is the upper-triangular matrix of the form
    \[
        \begin{pmatrix}
            \lambda_{k} &        & *           \\
                        & \ddots &             \\
            0           &        & \lambda_{k}
        \end{pmatrix}.
    \]

    Let $D$ be the linear operator on $V$ whose matrix is the following block diagonal matrix
    \[
        \begin{pmatrix}
            D_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & D_{m}
        \end{pmatrix}
    \]

    where $D_{k}$ is the following diagonal matrix
    \[
        \begin{pmatrix}
            \lambda_{k} &        & 0           \\
                        & \ddots &             \\
            0           &        & \lambda_{k}
        \end{pmatrix}.
    \]

    Let $N$ be the linear operator on $V$ whose matrix is the following block diagonal matrix
    \[
        \begin{pmatrix}
            A_{1} - D_{1} &        & 0             \\
                          & \ddots &               \\
            0             &        & A_{m} - D_{m}
        \end{pmatrix}.
    \]

    $D$ is diagonalizable, $N$ is nilpotent, and because $D_{k}A_{k} = A_{k}D_{k}$, we obtain that
    \begin{align*}
        \begin{pmatrix}
            D_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & D_{m}
        \end{pmatrix}
        \begin{pmatrix}
            A_{1} - D_{1} &        & 0             \\
                          & \ddots &               \\
            0             &        & A_{m} - D_{m}
        \end{pmatrix}
         & =
        \begin{pmatrix}
            D_{1}(A_{1} - D_{1}) &        & 0                    \\
                                 & \ddots &                      \\
            0                    &        & D_{m}(A_{m} - D_{m})
        \end{pmatrix} \\
         & =
        \begin{pmatrix}
            (A_{1} - D_{1})D_{1} &        & 0                    \\
                                 & \ddots &                      \\
            0                    &        & (A_{m} - D_{m})D_{m}
        \end{pmatrix} \\
         & =
        \begin{pmatrix}
            A_{1} - D_{1} &        & 0             \\
                          & \ddots &               \\
            0             &        & A_{m} - D_{m}
        \end{pmatrix}
        \begin{pmatrix}
            D_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & D_{m}
        \end{pmatrix}.
    \end{align*}

    So $DN = ND$.

    Hence there exist a diagonalizable operator $D$ and a nilpotent operator $N$ on $V$ such that $DN = ND$ and $T = D + N$.
\end{proof}
\newpage

% chapter8:sectionB:exercise10
\begin{exercise}\label{chapter8:sectionB:exercise10}
    Suppose $V$ is a complex inner product space, $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$, and $T \in \lmap{V}$. Let $\lambda_{1}, \ldots, \lambda_{n}$ be the eigenvalues of $T$, each included as many times as its multiplicity. Prove that
    \[
        \abs{\lambda_{1}}^{2} + \cdots + \abs{\lambda_{n}}^{2} \leq \norm{Te_{1}}^{2} + \cdots + \norm{Te_{n}}^{2}.
    \]
\end{exercise}

\begin{quote}
    See the comment after Exercise~\ref{chapter7:sectionA:exercise5}.
\end{quote}

\begin{proof}
    According to Schur's theorem, there is an orthonormal basis $f_{1}, \ldots, f_{n}$ of $V$ to which $T$ has an upper-triangular matrix $A$. On the other hand, $\lambda_{k}$ appears $d_{k}$ times on the diagonal of $A$, where $d_{k}$ is the algebraic multiplicity of $\lambda_{k}$.
    \begin{align*}
        \abs{\lambda_{1}}^{2} + \cdots + \abs{\lambda_{n}}^{2} & = \abs{\innerprod{Tf_{1}, f_{1}}}^{2} + \cdots + \abs{\innerprod{Tf_{n}, f_{n}}}^{2} & (\lambda_{k} = A_{k,k} = \innerprod{Te_{k}, e_{k}})               \\
                                                               & \leq \norm{Tf_{1}}^{2}\norm{f_{1}}^{2} + \cdots + \norm{Tf_{n}}^{2}\norm{f_{n}}^{2}  & \text{(Cauchy-Schwarz's inequality)}                              \\
                                                               & = \norm{Tf_{1}}^{2} + \cdots + \norm{Tf_{n}}^{2}                                                                                                         \\
                                                               & = \norm{Te_{1}}^{2} + \cdots + \norm{Te_{n}}^{2}.                                    & \text{(comment after Exercise~\ref{chapter7:sectionA:exercise5})}
    \end{align*}
\end{proof}
\newpage

% chapter8:sectionB:exercise11
\begin{exercise}\label{chapter8:sectionB:exercise11}
    Give an example of an operator on $\mathbb{C}^{4}$ whose characteristic polynomial equal ${(z - 7)}^{2}{(z - 8)}^{2}$.
\end{exercise}

\begin{proof}
    Here is an example.

    $T\in\lmap{\mathbb{C}^{4}}$ and
    \[
        T(z_{1}, z_{2}, z_{3}, z_{4}) = (7z_{1}, 7z_{2}, 8z_{3}, 8z_{4}).
    \]

    $T$ is diagonalizable, $E(7, T) = \operatorname{span}((1, 0, 0, 0), (0, 1, 0, 0))$, and $E(8, T) = \operatorname{span}((0, 0, 1, 0), (0, 0, 0, 1))$ so the characteristic polynomial of $T$ is ${(z-7)}^{2}{(z-8)}^{2}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise12
\begin{exercise}\label{chapter8:sectionB:exercise12}
    Give an example of an operator on $\mathbb{C}^{4}$ whose characteristic polynomial equals $(z - 1){(z - 5)}^{3}$ and whose minimal polynomial equals $(z - 1){(z - 5)}^{2}$.
\end{exercise}

\begin{proof}
    Here is an example.

    $T\in\lmap{\mathbb{C}^{4}}$ and the matrix of $T$ with respect to the standard basis $e_{1}, e_{2}, e_{3}, e_{4}$ of $\mathbb{C}^{4}$ is
    \[
        \begin{pmatrix}
            5 & 0 & 1 & 0 \\
            0 & 5 & 0 & 0 \\
            0 & 0 & 5 & 0 \\
            0 & 0 & 0 & 1
        \end{pmatrix}.
    \]

    The generalized eigenspaces of $T$ are
    \[
        G(5, T) = \kernel{{(T - 5I)}^{2}}\qquad G(1, T) = \kernel{{(T - I)}^{1}}
    \]

    where $\dim G(5, T) = 3$ and $\dim G(1, T) = 1$, so the minimal polynomial of $T$ is $(z-1){(z-5)}^{2}$ and the characteristic polynomial of $T$ is $(z-1){(z-5)}^{3}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise13
\begin{exercise}\label{chapter8:sectionB:exercise13}
    Give an example of an operator on $\mathbb{C}^{4}$ whose characteristic and minimal polynomials both equal $z{(z - 1)}^{2}(z - 3)$.
\end{exercise}

\begin{proof}
    Here is an example.

    $T\in\lmap{\mathbb{C}^{4}}$ and the matrix of $T$ with respect to the standard basis $e_{1}, e_{2}, e_{3}, e_{4}$ of $\mathbb{C}^{4}$ is
    \[
        \begin{pmatrix}
            0 & 0 & 0 & 0 \\
            0 & 1 & 1 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 3
        \end{pmatrix}.
    \]

    The generalized eigenspaces of $T$ are
    \begin{align*}
        G(0, T) & = \kernel{(T - 0I)} = \operatorname{span}(e_{1}),              \\
        G(1, T) & = \kernel{{(T - 1I)}^{2}} = \operatorname{span}(e_{2}, e_{3}), \\
        G(3, T) & = \kernel{(T - 3I)} = \operatorname{span}(e_{4}).
    \end{align*}

    So the minimal polynomial and the characteristic polynomial of $T$ are equal to $z{(z-1)}^{2}{(z-3)}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise14
\begin{exercise}\label{chapter8:sectionB:exercise14}
    Give an example of an operator on $\mathbb{C}^{4}$ whose characteristic and minimal polynomials both equal $z{(z - 1)}^{2}(z - 3)$ and whose minimal polynomial equals $z(z - 1)(z - 3)$.
\end{exercise}

\begin{proof}
    Here is an example.

    $T\in\lmap{\mathbb{C}^{4}}$ and the matrix of $T$ with respect to the standard basis $e_{1}, e_{2}, e_{3}, e_{4}$ of $\mathbb{C}^{4}$ is
    \[
        \begin{pmatrix}
            0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 3
        \end{pmatrix}.
    \]

    The generalized eigenspaces of $T$ are
    \begin{align*}
        G(0, T) & = E(0, T) = \kernel{(T - 0I)} = \operatorname{span}(e_{1}),        \\
        G(1, T) & = E(1, T) = \kernel{(T - 1I)} = \operatorname{span}(e_{2}, e_{3}), \\
        G(3, T) & = E(3, T) = \kernel{(T - 3I)} = \operatorname{span}(e_{4}).
    \end{align*}

    So the characteristic polynomial of $T$ is $z{(z-1)}^{2}(z-3)$ and the minimal polynomial of $T$ is $z(z-1)(z-3)$.
\end{proof}
\newpage

% chapter8:sectionB:exercise15
\begin{exercise}\label{chapter8:sectionB:exercise15}
    Let $T$ be the operator on $\mathbb{C}^{4}$ defined by $T(z_{1}, z_{2}, z_{3}, z_{4}) = (0, z_{1}, z_{2}, z_{3})$. Find the characteristic polynomial and the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    \begin{align*}
        T^{4}(z_{1}, z_{2}, z_{3}, z_{4}) & = T^{3}(0, z_{1}, z_{2}, z_{3}) \\
                                          & = T^{2}(0, 0, z_{1}, z_{2})     \\
                                          & = T(0, 0, 0, z_{1})             \\
                                          & = (0, 0, 0, 0)
    \end{align*}

    so $T$ is nilpotent.

    $G(0, T) = \mathbb{C}^{4}$, so the characteristic polynomial of $T$ is $z^{4}$. $4$ is the smallest positive integer $k$ such that $T^{k} = 0$ so $z^{4}$ is also the minimal polynomial of $T$.
\end{proof}
\newpage

% chapter8:sectionB:exercise16
\begin{exercise}\label{chapter8:sectionB:exercise16}
    Let $T$ be the operator on $\mathbb{C}^{6}$ defined by
    \[
        T(z_{1}, z_{2}, z_{3}, z_{4}, z_{5}, z_{6}) = (0, z_{1}, z_{2}, 0, z_{4}, 0).
    \]

    Find the characteristic polynomial and the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    \begin{align*}
        T^{3}(z_{1}, z_{2}, z_{3}, z_{4}, z_{5}, z_{6}) & = T^{2}(0, z_{1}, z_{2}, 0, z_{4}, 0) \\
                                                        & = T(0, 0, z_{1}, 0, 0, 0)             \\
                                                        & = (0, 0, 0, 0, 0, 0)
    \end{align*}

    so $T$ is nilpotent.

    $G(0, T) = \mathbb{C}^{6}$, so the characteristic polynomial of $T$ is $z^{6}$. $3$ is the smallest positive integer $k$ such that $T^{k} = 0$ so $z^{3}$ is the minimal polynomial of $T$.
\end{proof}
\newpage

% chapter8:sectionB:exercise17
\begin{exercise}\label{chapter8:sectionB:exercise17}
    Suppose $\mathbb{F} = \mathbb{C}$ and $P\in\lmap{V}$ is such that $P^{2} = P$. Prove that the characteristic polynomial of $P$ is $z^{m}{(z - 1)}^{n}$, where $m = \dim\kernel{P}$ and $n = \dim\range{P}$.
\end{exercise}

\begin{proof}
    $P^{2} = P$ so $V = \kernel{P}\oplus\range{P}$. Moreover, $\kernel{P} = E(0, P)$ and $\range{P} = E(1, P)$. Therefore, if $m = \dim\kernel{P}$ and $n = \dim\range{P}$, then the characteristic polynomial of $P$ is $z^{m}{(z-1)}^{n}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise18
\begin{exercise}\label{chapter8:sectionB:exercise18}
    Suppose $T\in\lmap{V}$ and $\lambda$ is an eigenvalue of $T$. Explain why the following four numbers equal each other.
    \begin{enumerate}[label={(\alph*)}]
        \item The exponent of $z - \lambda$ in the factorization of the minimal polynomial of $T$.
        \item The smallest positive integer $m$ such that ${(T - \lambda I)}^{m}\vert_{G(\lambda, T)} = 0$.
        \item The smallest positive integer $m$ such that
              \[
                  \kernel{{(T - \lambda I)}^{m}} = \kernel{{(T - \lambda I)}^{m+1}}.
              \]
        \item The smallest positive integer $m$ such that
              \[
                  \range{{(T - \lambda I)}^{m}} = \range{{(T - \lambda I)}^{m+1}}.
              \]
    \end{enumerate}
\end{exercise}

\begin{proof}
    Denote the positive integers in (a), (b), (c), (d) by $m_{a}, m_{b}, m_{c}, m_{d}$, respectively. These positive integers don't exceed $\dim V$.

    Because
    \begin{align*}
        \{0\} & = \kernel{I} \subseteq \kernel{{(T - \lambda I)}} \subseteq \cdots \subseteq \kernel{{(T - \lambda I)}^{k}} \subseteq \kernel{{(T - \lambda I)}^{k+1}} \subseteq \cdots \\
        V     & = \range{I} \supseteq \range{{(T - \lambda I)}} \supseteq \cdots \supseteq \range{{(T - \lambda I)}^{k}} \supseteq \range{{(T - \lambda I)}^{k+1}} \subseteq \cdots
    \end{align*}

    and by the fundamental theorem of linear maps
    \begin{align*}
        \dim V & = \dim\kernel{(T - \lambda I)} + \dim\range{(T - \lambda I)}             \\
               & = \cdots                                                                 \\
               & = \dim\kernel{{(T - \lambda I)}^{k}} + \dim\range{{(T - \lambda I)}^{k}} \\
               & = \cdots
    \end{align*}

    and
    \begin{align*}
        \kernel{{(T - \lambda I)}^{k}} & = \kernel{{(T - \lambda I)}^{k+1}} \implies\kernel{{(T - \lambda I)}^{k}} = \kernel{{(T - \lambda I)}^{k+m}}\forall m\in\mathbb{Z}^{+}, \\
        \range{{(T - \lambda I)}^{k}}  & = \range{{(T - \lambda I)}^{k+1}} \implies\range{{(T - \lambda I)}^{k}} = \range{{(T - \lambda I)}^{k+m}}\forall m\in\mathbb{Z}^{+}
    \end{align*}

    and
    \begin{align*}
        \kernel{{(T - \lambda I)}^{\dim V}} & = \kernel{{(T - \lambda I)}^{\dim V+1}} = \cdots \\
        \range{{(T - \lambda I)}^{\dim V}}  & = \range{{(T - \lambda I)}^{\dim V+1}}  = \cdots
    \end{align*}

    so $m_{c} = m_{d}$.

    According to the definition of $m_{b}$, we conclude that $m_{b}$ is the smallest positive integer $m$ such that
    \[
        \kernel{{(T - \lambda I)}^{m}} = \kernel{{(T - \lambda I)}^{\dim V}}
    \]

    so $m_{b}$ is also the smallest positive integer such that $\kernel{{(T - \lambda I)}^{m}} = \kernel{{(T - \lambda I)}^{m+1}}$.

    Therefore $m_{b} = m_{c}$.

    Finally, $m_{a} = m_{b}$ due to Exercise~\ref{chapter8:sectionB:exercise6}.

    Thus $m_{a} = m_{b} = m_{c} = m_{d}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise19
\begin{exercise}\label{chapter8:sectionB:exercise19}
    Suppose $\mathbb{F} = \mathbb{C}$ and $S \in \lmap{V}$ is a unitary operator. Prove that the constant term in the characteristic polynomial of $S$ has absolute value $1$.
\end{exercise}

\begin{proof}
    $S$ is a unitary operator so $S$ is normal. According to the complex spectral theorem, $S$ is diagonalizable. Moreover, the eigenvalues of $S$ have absolute value $1$. Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $S$ and $d_{k} = \dim E(\lambda_{k}, S)$. The characteristic polynomial of $S$ is
    \[
        {(z - \lambda_{1})}^{d_{1}}\cdots {(z - \lambda_{m})}^{d_{m}}.
    \]

    The constant term of this polynomial is
    \[
        {(-\lambda_{1})}^{d_{1}}\cdots {(-\lambda_{m})}^{d_{m}}
    \]

    which has absolute value $1$.
\end{proof}
\newpage

% chapter8:sectionB:exercise20
\begin{exercise}\label{chapter8:sectionB:exercise20}
    Suppose that $\mathbb{F} = \mathbb{C}$ and $V_{1}, \ldots, V_{m}$ are nonzero subspaces of $V$ such that
    \[
        V = V_{1}\oplus \cdots \oplus V_{m}.
    \]

    Suppose $T\in\lmap{V}$ and each $V_{k}$ is invariant under $T$. For each $k$, let $p_{k}$ denote the characteristic polynomial of $T\vert_{V_{k}}$. Prove that the characteristic polynomial of $T$ equals $p_{1}\cdots p_{m}$.
\end{exercise}

\begin{proof}
    Let $\lambda_{1}, \ldots, \lambda_{n}$ be the distinct eigenvalues of $T$. By the generalized eigenspace decomposition theorem
    \[
        V = \bigoplus^{m}_{k=1}\bigoplus^{n}_{j=1}G(\lambda_{j}, T\vert_{V_{k}})
    \]

    The exponent of $z - \lambda_{i}$ in the factorization of $p_{k}$ is
    \[
        \dim G(\lambda_{i}, T\vert_{V_{k}}).
    \]

    For every $v_{i}\in G(\lambda_{i}, T)$, there exist unique vectors $v_{j,k}\in G(\lambda_{j}, T\vert_{v_{k}})$ such that
    \[
        v_{i} = \sum^{m}_{k=1}\sum^{n}_{j=1}v_{j,k}.
    \]

    If $v_{i} = 0$ then all vectors $v_{j,k}$ are $0$. Otherwise, assume there exists $v_{j,k}\ne 0$ such that $j\ne i$, then there are generalized eigenvectors of $\lambda_{j}$, $\lambda_{i}$ which are linearly dependent, which is a contradiction. Hence $v_{j,k} = 0$ for every $j\ne i$, which implies
    \[
        v_{i} = \sum^{m}_{k=1}v_{i,k}.
    \]

    Therefore
    \[
        G(\lambda_{i}, T) = \bigoplus^{m}_{k=1}G(\lambda_{i}, T\vert_{V_{k}}).
    \]

    Denote the characteristic polynomial of $T$ by $p$. The exponent of $z - \lambda_{i}$ in the factorization of $p(z)$ is
    \[
        \dim G(\lambda_{i}, T) = \sum^{m}_{k=1}\dim G(\lambda_{i}, T\vert_{V_{k}})
    \]

    which is the sum of the exponents of $z - \lambda_{i}$ in the factorizations of $p_{1}, \ldots, p_{m}$.

    Thus $p = p_{1}\cdots p_{m}$.
\end{proof}
\newpage

% chapter8:sectionB:exercise21
\begin{exercise}\label{chapter8:sectionB:exercise21}
    Suppose $p, q\in\mathscr{P}(\mathbb{C})$ are monic polynomials with the same zeros and $q$ is a polynomial multiple of $p$. Prove that there exists $T\in\lmap{\mathbb{C}^{\deg q}}$ such that the characteristic polynomial of $T$ is $q$ and the minimal polynomial of $T$ is $p$.
\end{exercise}

\begin{quote}
    This exercise implies that every monic polynomial is the characteristic polynomial of some operator.
\end{quote}

\begin{proof}
    Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$. Suppose that
    \begin{align*}
        p(z) & = {(z - \lambda_{1})}^{h_{1}}\cdots {(z - \lambda_{m})}^{h_{m}}, \\
        q(z) & = {(z - \lambda_{1})}^{k_{1}}\cdots {(z - \lambda_{m})}^{k_{m}},
    \end{align*}

    where $h_{j}, k_{j}$ are positive integers for $j\in\{1,\ldots,m\}$. This assumption is possible due to the fundamental theorem of algebra.

    Because $q$ is a polynomial multiple of $p$, it follows that $h_{j}\leq k_{j}$ for every $j\in\{1,\ldots,m\}$.

    Let $A_{j}$ be an upper-triangular matrix with $k_{j}$ columns such that the entries on its diagonal are all $\lambda_{j}$ and $A_{j} - \lambda_{j}I$ is a nilpotent matrix whose index is $h_{j}$. Let $A$ be the following block diagonal matrix
    \[
        \begin{pmatrix}
            A_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & A_{m}
        \end{pmatrix}
    \]

    and $T$ be the operator on $\mathbb{C}^{\deg q}$ whose matrix with respect to the standard basis of $\mathbb{C}^{\deg q}$ is $A$. By Exercise~\ref{chapter8:sectionB:exercise6}, $p$ is the minimal polynomial of $T$. By the definition of characteristic polynomial, $q$ is the characteristic polynomial of $T$.

    Thus there exists $T\in\lmap{\mathbb{C}^{\deg q}}$ such that the characteristic polynomial of $T$ is $q$ and the minimal polynomial of $T$ is $p$.
\end{proof}
\newpage

% chapter8:sectionB:exercise22
\begin{exercise}\label{chapter8:sectionB:exercise22}
    Suppose $A$ and $B$ are block diagonal matrices of the form
    \[
        A = \begin{pmatrix}
            A_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & A_{m}
        \end{pmatrix},
        \qquad
        B = \begin{pmatrix}
            B_{1} &        & 0     \\
                  & \ddots &       \\
            0     &        & B_{m}
        \end{pmatrix},
    \]

    where $A_{k}$ and $B_{k}$ are square matrices of the same size for each $k = 1, \ldots, m$. Show that $AB$ is a block diagonal matrix of the form
    \[
        AB = \begin{pmatrix}
            A_{1}B_{1} &        & 0          \\
                       & \ddots &            \\
            0          &        & A_{m}B_{m}
        \end{pmatrix}.
    \]
\end{exercise}

\begin{proof}
    Let $n_{i}$ be the number of columns of $A_{i}$ and $n_{0} = 0$.
    \begin{align*}
        {(AB)}_{j,k} = A_{j,\cdot}B_{\cdot,k}.
    \end{align*}

    If there exists $i\in\{0,1,\ldots,m-1\}$ such that
    \[
        n_{0} + \cdots + n_{i} + 1\leq j, k\leq n_{0} + \cdots + n_{i} + n_{i+1}
    \]

    then $A_{j,\cdot}B_{\cdot,k}$ is the multiplication of the row $j - (n_{0} + \cdots + n_{i})$ of $A_{i}$ and the column $k - (n_{0} + \cdots + n_{i})$ of $B_{i}$.

    Otherwise, $A_{j,\cdot}B_{\cdot,k} = 0$.

    Thus
    \[
        AB = \begin{pmatrix}
            A_{1}B_{1} &        & 0          \\
                       & \ddots &            \\
            0          &        & A_{m}B_{m}
        \end{pmatrix}.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionB:exercise23
\begin{exercise}\label{chapter8:sectionB:exercise23}
    Suppose $\mathbb{F} = \mathbb{R}$, $T\in\lmap{V}$, and $\lambda\in\mathbb{C}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $u + \iota v\in G(\lambda, T_{\mathbb{C}})$ if and only if $u - \iota v\in G(\conj{\lambda}, T_{\mathbb{C}})$.
        \item Show that the multiplicity of $\lambda$ as an eigenvalue of $T_{\mathbb{C}}$ equals the multiplicity of $\conj{\lambda}$ as an eigenvalue of $T_{\mathbb{C}}$.
        \item Use (b) and the result about the sum of the multiplicities (8.25) to show that if $\dim V$ is an odd number, then $T_{\mathbb{C}}$ has a real eigenvalue.
        \item Use (c) and the result about real eigenvalues of $T_{\mathbb{C}}$ (Exercise~\ref{chapter5:sectionA:exercise17}) to show that if $\dim V$ is an odd number, then $T$ has an eigenvalue (thus giving an alternative proof of 5.34).
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \begin{align*}
                  u + \iota v\in G(\lambda, T_{\mathbb{C}}) & \Longleftrightarrow {(T - \lambda I)}^{\dim V}u + \iota {(T - \lambda I)}^{\dim V}v = 0               \\
                                                            & \Longleftrightarrow {(T - \lambda I)}^{\dim V}u = {(T - \lambda I)}^{\dim V}v = 0                     \\
                                                            & \Longleftrightarrow {(T - \conj{\lambda} I)}^{\dim V}u = {(T - \conj{\lambda} I)}^{\dim V}v = 0       \\
                                                            & \Longleftrightarrow {(T - \conj{\lambda} I)}^{\dim V}u - \iota {(T - \conj{\lambda} I)}^{\dim V}v = 0 \\
                                                            & \Longleftrightarrow u - \iota v\in G(\conj{\lambda}, T_{\mathbb{C}}).
              \end{align*}
        \item The linear map $F: G(\lambda, T_{\mathbb{C}})\to G(\conj{\lambda}, T_{\mathbb{C}})$ such that $F(u + \iota v) = u - \iota v$ is an isomorphism. Hence
              \[
                  \dim G(\lambda, T_{\mathbb{C}}) = \dim G(\conj{\lambda}, T_{\mathbb{C}}).
              \]

              Therefore the multiplicity of $\lambda$ as an eigenvalue of $T_{\mathbb{C}}$ and the multiplicity of $\conj{\lambda}$ as an eigenvalue of $T_{\mathbb{C}}$ are equal.
        \item If $T_{\mathbb{C}}$ has no real eigenvalue, then all eigenvalues of $T_{\mathbb{C}}$ are complex but non-real numbers. By (b), it follows that the dimension of $V$ is an even number, because $\dim V$ equals to the sum of multiplicity of the distinct eigenvalues of $T_{\mathbb{C}}$. This is a contradiction because $\dim V$ is an odd number. Thus $T_{\mathbb{C}}$ has a real eigenvalue.
        \item By (c), $T_{\mathbb{C}}$ has a real eigenvalue $\lambda$, so there exist vectors $u, v\in V$ which are not both zero such that
              \[
                  Tu + \iota Tv = T_{\mathbb{C}}(u + \iota v) = \lambda u + \iota \lambda v.
              \]

              Therefore $Tu = \lambda u$, $Tv = \lambda v$. Since $u, v$ are not both zero, we conclude that $\lambda$ is an eigenvalue of $T$. Thus $T$ has an eigenvalue.
    \end{enumerate}
\end{proof}
\newpage

\section{Consequences of Generalized Eigenspace Decomposition}

% chapter8:sectionC:exercise1
\begin{exercise}\label{chapter8:sectionC:exercise1}
    Suppose $T\in\lmap{\mathbb{C}^{3}}$ is the operator defined by $T(z_{1}, z_{2}, z_{3}) = (z_{2}, z_{3}, 0)$. Prove that $T$ does not have a square root.
\end{exercise}

\begin{proof}
    \begin{align*}
        T^{3}(z_{1}, z_{2}, z_{3}) & = T^{2}(z_{2}, z_{3}, 0) \\
                                   & = T(z_{3}, 0, 0)         \\
                                   & = (0, 0, 0)
    \end{align*}

    so $T$ is nilpotent and it has index $3$.

    Assume $T$ has a square root $S$, then $T = S^{2}$. Because $T^{3} = 0$, $S^{6} = 0$, so $S$ is nilpotent, and $S^{3} = 0$. Moreover, $ST = S^{3} = 0, TS = S^{3} = 0$.
    \begin{align*}
        \kernel{S} & \supseteq \range{T},  \\
        \range{S}  & \subseteq \kernel{T}.
    \end{align*}

    So $\dim\kernel{S} \geq \dim\range{T} = 2$ and $\dim\range{S} \leq \dim\kernel{T} = 1$.

    There are two possible cases.
    \begin{itemize}
        \item $\dim\kernel{S} = 3$. Then $S = 0$, which implies $T = 0$, which is a contradiction.
        \item $\dim\kernel{S} = 2$. Then $\kernel{S} = \range{T}$ and $\range{S} = \kernel{T}$.

              $S(0, 1, 0) = S(0, 0, 1) = (0, 0, 0)$ because $(0, 1, 0), (0, 0, 1)\in \range{T} = \kernel{S}$.

              $S^{2}(1, 0, 0) = T(1, 0, 0) = 0$.

              Hence $S^{2} = 0$, which implies $T = 0$, and this is a contradiction.
    \end{itemize}

    Hence the assumption is false. Thus $T$ does not have a square root.
\end{proof}
\newpage

% chapter8:sectionC:exercise2
\begin{exercise}\label{chapter8:sectionC:exercise2}
    Define $T\in\lmap{\mathbb{F}^{5}}$ by $T(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) = (2x_{2}, 3x_{3}, -x_{4}, 4x_{5}, 0)$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $T$ is nilpotent.
        \item Find a square root of $I + T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \begin{align*}
                  T^{5}(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) & = T^{4}(2x_{2}, 3x_{3}, -x_{4}, 4x_{5}, 0) \\
                                                           & = T^{3}(6x_{3}, -3x_{4}, -4x_{5}, 0, 0)    \\
                                                           & = T^{2}(-6x_{4}, -12x_{5}, 0, 0, 0)        \\
                                                           & = T(-24x_{5}, 0, 0, 0, 0)                  \\
                                                           & = (0, 0, 0, 0, 0).
              \end{align*}

              Hence $T$ is nilpotent.
        \item The Taylor expansion of $\sqrt{1 + x}$ at $0$ is
              \[
                  1 + \frac{1}{2}x + \binom{1/2}{2}x^{2} + \binom{1/2}{3}x^{3} + \cdots.
              \]

              A square root of $I + T$ is
              \[
                  S = 1 + \frac{1}{2}T + \binom{1/2}{2}T^{2} + \binom{1/2}{3}T^{3} + \binom{1/2}{4}T^{4}.
              \]

              With the help of SymPy, I obtain
              \begin{multline*}
                  S(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) = \\
                  \left(x_{1}, x_{1} + x_{2}, \frac{-3}{4}x_{1} + \frac{3}{2}x_{2} + x_{3}, \frac{-3}{8}x_{1} + \frac{3}{8}x_{2} + \frac{-1}{2}x_{3} + x_{4}, \frac{15}{16}x_{1} + \frac{-3}{4}x_{2} + \frac{1}{2}x_{3} + 2x_{4} + x_{5}\right).
              \end{multline*}
    \end{enumerate}
\end{proof}
\newpage

% chapter8:sectionC:exercise3
\begin{exercise}\label{chapter8:sectionC:exercise3}
    Suppose $V$ is a complex vector space. Prove that every invertible operator on $V$ has a cube root.
\end{exercise}

\begin{proof}
    First, I prove that $I + F$ has a $k$th root if $F$ is nilpotent.

    The Taylor expansion of ${(1 + x)}^{1/k}$ at $x = 0$ is
    \[
        \sum^{\infty}_{n=0}\binom{1/k}{n}\frac{d^{n}}{dx^{n}}{(1 + x)}^{1/k}\vert_{x=0}.
    \]

    Let the index of the nilpotent operator $F$ be $m$, then
    \[
        R = I + \frac{1}{k}T + \binom{1/k}{2}T^{2} + \cdots + \binom{1/k}{m-1}T^{m-1}
    \]

    is a $k$th root of $F$.

    Let $\lambda_{1}, \ldots, \lambda_{n}$ be the distinct eigenvalues of $T$. These eigenvalues are not equal to $0$ because $T$ is invertible.
    \[
        T\vert_{G(\lambda_{i}, T)} - \lambda_{i}I
    \]

    is nilpotent. By the previously proved result,
    \[
        I + \frac{1}{\lambda_{i}}(T\vert_{G(\lambda_{i}, T)} - \lambda_{i}I)
    \]

    has a $k$th root. So $T\vert_{G(\lambda_{i}, T)}$ has a $k$th root $R_{i}$ for every $i\in\{1,\ldots,n\}$. On the other hand,
    \[
        V = \bigoplus^{n}_{i=1}G(\lambda_{i}, T).
    \]

    Let $R\in\lmap{V}$ defined by
    \[
        Rv = \sum^{n}_{i=1}R_{i}v_{i}
    \]

    where $v = \sum^{n}_{i=1}v_{i}$ and $v_{i}\in G(\lambda_{i}, T)$, so $R$ is a $k$th root of $T$. Therefore $T$ has a $k$th root. Thus $T$ has a cube root.
\end{proof}
\newpage

% chapter8:sectionC:exercise4
\begin{exercise}\label{chapter8:sectionC:exercise4}
    Suppose $V$ is a real vector space. Prove that the operator $-I$ on $V$ has a square root if and only if $\dim V$ is an even number.
\end{exercise}

\begin{quote}[Additional notes]
    $\dim V > 0$.
\end{quote}

\begin{proof}
    Suppose $-I$ has a square root $R$, then $R^{2} + I = 0$. So $x^{2} + 1$ is the minimal polynomial of $R$, since $V$ is a real vector space (the polynomial $x^{2} + 1$ is irreducible on $\mathbb{R}$). Therefore $R$ has no eigenvalue. If $\dim V$ is an odd number, then $R$ has an eigenvalue, according to Exercise~\ref{chapter8:sectionB:exercise23} (d), which is a contradiction. Hence $\dim V$ is an even number.

    Suppose $\dim V$ is an even number. Let $\dim V = 2n$ and $v_{1}, \ldots, v_{2n}$ be a basis of $V$, let $T\in\lmap{V}$ defined by
    \[
        Tv_{k} = \begin{cases}
            v_{k+1}  & \text{if $k$ is odd}  \\
            -v_{k-1} & \text{if $k$ is even}
        \end{cases}
    \]

    then $\operatorname{span}(v_{1}, v_{2}), \ldots, \operatorname{span}(v_{2n-1}, v_{2n})$ are invariant subspaces under $T$. Moreover,
    \[
        V = \operatorname{span}(v_{1}, v_{2})\oplus\cdots\oplus\operatorname{span}(v_{2n-1}, v_{2n})
    \]

    and
    \[
        {(T\vert_{\operatorname{span}(v_{2k-1}, v_{2k})})}^{2} = -I\vert_{\operatorname{span}(v_{2k-1}, v_{2k})}
    \]

    so $T$ is a square root of $-I$. Thus $-I$ has a square root.
\end{proof}
\newpage

% chapter8:sectionC:exercise5
\begin{exercise}\label{chapter8:sectionC:exercise5}
    Suppose $T\in\lmap{\mathbb{C}^{2}}$ is the operator defined by $T(w, z) = (-w-z, 9w+5z)$. Find a Jordan basis for $T$.
\end{exercise}

\begin{proof}
    The minimal polynomial of $T$ is ${(z - 2)}^{2}$.

    The matrix of $T - 2I$ with respect to the basis $(1, 0), (-3, 9)$ is
    \[
        \begin{pmatrix}
            0 & 1 \\
            0 & 0
        \end{pmatrix}
    \]

    so the matrix of $T$ with respect to the basis $(1, 0), (-3, 9)$ is
    \[
        \begin{pmatrix}
            2 & 1 \\
            0 & 2
        \end{pmatrix}.
    \]

    Thus $(1, 0), (-3, 9)$ is a Jordan basis of $T$.
\end{proof}
\newpage

% chapter8:sectionC:exercise6
\begin{exercise}\label{chapter8:sectionC:exercise6}
    Find a basis of $\mathscr{P}_{4}(\mathbb{R})$ that is a Jordan basis for the differentiation operator $D$ on $\mathscr{P}_{4}(\mathbb{R})$ defined by $Dp = p'$.
\end{exercise}

\begin{proof}
    The matrix of $D$ with respect to the basis $1, x, x^{2}, x^{3}, x^{4}$ is
    \[
        \begin{pmatrix}
            0 & 1 & 0 & 0 & 0 \\
            0 & 0 & 2 & 0 & 0 \\
            0 & 0 & 0 & 3 & 0 \\
            0 & 0 & 0 & 0 & 4 \\
            0 & 0 & 0 & 0 & 0
        \end{pmatrix}
    \]

    so the matrix of $D$ with respect to the basis $1/0{!}, x/1{!}, x^{2}/2{!}, x^{3}/3{!}, x^{4}/4{!}$ is
    \[
        \begin{pmatrix}
            0 & 1 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 0 & 1 \\
            0 & 0 & 0 & 0 & 0
        \end{pmatrix}.
    \]

    Thus $1/0{!}, x/1{!}, x^{2}/2{!}, x^{3}/3{!}, x^{4}/4{!}$ is a Jordan basis of $D$.
\end{proof}
\newpage

% chapter8:sectionC:exercise7
\begin{exercise}\label{chapter8:sectionC:exercise7}
    Suppose $T \in \lmap{V}$ is nilpotent and $v_{1}, \ldots, v_{n}$ is a Jordan basis for $T$. Prove that the minimal polynomial of $T$ is $z^{m + 1}$, where $m$ is the length of the longest consecutive string of $1$'s that appears on the line directly above the diagonal in the matrix of $T$ with respect to $v_{1}, \ldots, v_{n}$.
\end{exercise}

\begin{proof}
    Let the matrix of $T$ with respect to $v_{1}, \ldots, v_{n}$ be $A$, then $A$ is a block diagonal matrix where all blocks are of the form
    \[
        \begin{pmatrix}
            0      & 1      & \cdots & 0      \\
            0      & 0      & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & 0
        \end{pmatrix}.
    \]

    Let the $A_{1}, \ldots, A_{m}$ be the block of $A$, then by Exercise~\ref{chapter8:sectionB:exercise22}
    \[
        A^{k} = \begin{pmatrix}
            {A_{1}}^{k} &        & 0           \\
                        & \ddots &             \\
            0           &        & {A_{m}}^{k}
        \end{pmatrix}.
    \]

    The index of $A$ is also the largest among the indices of $A_{1}, \ldots, A_{m}$, which is also the length of the longest consecutive string of $1$'s plus $1$.
\end{proof}
\newpage

% chapter8:sectionC:exercise8
\begin{exercise}\label{chapter8:sectionC:exercise8}
    Suppose $T\in\lmap{V}$ and $v_{1}, \ldots, v_{n}$ is a basis of $V$ that is a Jordan basis for $T$. Describe the matrix of $T^{2}$ with respect to this basis.
\end{exercise}

\begin{proof}
    The square of a Jordan block is
    \[
        {\begin{pmatrix}
                    \lambda & 1       & 0       & \cdots & 0       \\
                    0       & \lambda & 1       & \cdots & 0       \\
                    0       & 0       & \lambda & \cdots & 0       \\
                    \vdots  & \vdots  & \vdots  &        & \vdots  \\
                    0       & 0       & 0       & \cdots & \lambda
                \end{pmatrix}}^{2}
        = \begin{pmatrix}
            \lambda^{2} & 2\lambda    & 1           & \cdots & 0           \\
            0           & \lambda^{2} & 2\lambda    & \cdots & 0           \\
            0           & 0           & \lambda^{2} & \cdots & 0           \\
            \vdots      & \vdots      & \vdots      &        & \vdots      \\
            0           & 0           & 0           & \cdots & \lambda^{2}
        \end{pmatrix}.
    \]

    So the matrix of $T^{2}$ with respect to $v_{1}, \ldots, v_{n}$ is still a block diagonal matrix like the matrix of $T$ with respect to the same Jordan basis, and each block is of the form
    \[
        \begin{pmatrix}
            \lambda^{2} & 2\lambda    & 1           & \cdots & 0           \\
            0           & \lambda^{2} & 2\lambda    & \cdots & 0           \\
            0           & 0           & \lambda^{2} & \cdots & 0           \\
            \vdots      & \vdots      & \vdots      &        & \vdots      \\
            0           & 0           & 0           & \cdots & \lambda^{2}
        \end{pmatrix}.
    \]
\end{proof}
\newpage

% chapter8:sectionC:exercise9
\begin{exercise}\label{chapter8:sectionC:exercise9}
    Suppose $T \in \lmap{V}$ is nilpotent. Explain why there exist $v_{1}, \ldots, v_{n} \in V$ and nonnegative integers $m_{1}, \ldots, m_{n}$ such that (a) and (b) below both hold.
    \begin{enumerate}[label={(\alph*)}]
        \item $T^{m_{1}}v_{1}, \ldots, Tv_{1}, v_{1}; \ldots; T^{m_{n}}v_{n}, \ldots, Tv_{n}, v_{n}$ is a basis of $V$.
        \item $T^{m_{1}+1}v_{1} = \cdots = T^{m_{n}+1}v_{n} = 0$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Because $T$ is nilpotent, there exists a Jordan basis $u_{1}, \ldots, u_{\dim V}$ of $V$ to which $T$ has a block diagonal matrix $A$. Let's the matrices lying along the diagonal of $A$ be $A_{1}, \ldots, A_{n}$.

    Let $c_{0} = 0$ and $c_{1}, \ldots, c_{n}$ be the number of columns of $A_{1}, \ldots, A_{n}$. The labels of the columns of $A_{k}$ are
    \[
        c_{0} + c_{1} + \cdots + c_{k-1} + 1, \ldots, c_{0} + c_{1} + \cdots + c_{k-1} + c_{k}.
    \]

    For each $k\in\{1,\ldots,n\}$, $A_{k}$ is the matrix of the restriction of $T$ to
    \[
        \operatorname{span}(u_{c_{0} + \cdots + c_{k-1}+1}, \ldots, u_{c_{0} + c_{1} + \cdots + c_{k}}).
    \]

    Denote this restriction by $T_{k}$. Because $T$ is nilpotent, then so is $T_{k}$. $A_{k}$ is a Jordan block, so
    \begin{align*}
        T(u_{c_{0} + c_{1} + \cdots + c_{k-1}})         & = 0                                       \\
        T(u_{c_{0} + c_{1} + \cdots + c_{k-1} + 1})     & = u_{c_{0} + c_{1} + \cdots + c_{k-1}}    \\
                                                        & \cdots                                    \\
        T(u_{c_{0} + c_{1} + \cdots + c_{k-1} + c_{k}}) & = u_{c_{0} + c_{1} + \cdots + c_{k} - 1}.
    \end{align*}

    For each $k\in\{1,\ldots,n\}$, let $v_{k} = u_{c_{0} + c_{1} + \cdots + c_{k}}$ and $m_{k} = c_{k} - 1$, then
    \begin{enumerate}[label={(\alph*)}]
        \item $T^{m_{1}}v_{1}, \ldots, Tv_{1}, v_{1}; \ldots; T^{m_{n}}v_{n}, \ldots, Tv_{n}, v_{n}$ is a basis of $V$, because it is the same as the basis $u_{1}, \ldots, u_{\dim V}$.
        \item For every $k\in\{1,\ldots,n\}$, $m_{k}$ is a nonnegative integer, since $c_{k}$ is a positive integer. Moreover
              \[
                  T^{m_{k}+1}v_{k} = T^{c_{k}}u_{c_{0} + c_{1} + \cdots + c_{k}} = 0
              \]

              because the restriction of $T$ to $\operatorname{span}(u_{c_{0} + \cdots + c_{k-1}+1}, \ldots, u_{c_{0} + c_{1} + \cdots + c_{k}})$ is nilpotent and has index $c_{k}$.
    \end{enumerate}
\end{proof}
\newpage

% chapter8:sectionC:exercise10
\begin{exercise}\label{chapter8:sectionC:exercise10}
    Suppose $T \in \lmap{V}$ and $v_{1}, \ldots, v_{n}$ is a basis of $V$ that is a Jordan basis for $T$. Describe the matrix of $T$ with respect to the basis $v_{n}, \ldots, v_{1}$ obtained by reversing the order of the $v$'s.
\end{exercise}

\begin{proof}
    Let $A_{1}, \ldots, A_{m}$ be the Jordan blocks of the matrix of $T$ with respect to the Jordan basis $v_{1}, \ldots, v_{n}$.

    Then the matrix of $T$ with respect to the basis $v_{n}, \ldots, v_{1}$ is also a block diagonal matrix, however the blocks lying along the diagonal are ${A_{m}^{\top}}, \ldots, {A_{1}^{\top}}$ (the transposes of $A_{m}, \ldots, A_{1}$).
\end{proof}
\newpage

% chapter8:sectionC:exercise11
\begin{exercise}\label{chapter8:sectionC:exercise11}
    Suppose $T \in \lmap{V}$. Explain why every vector in each Jordan basis for $T$ is a generalized eigenvector of $T$.
\end{exercise}

\begin{proof}
    Let $A$ be the matrix of $T$ with respect to a Jordan basis $v_{1}, \ldots, v_{\dim V}$ for $T$, then $A$ is a block diagonal matrix where each block along the diagonal are of the form
    \[
        \begin{pmatrix}
            \lambda & 1       & \cdots & 0       \\
            0       & \lambda & \cdots & 0       \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & 0       & \cdots & \lambda
        \end{pmatrix}
    \]

    where $\lambda$ is an eigenvalue of $T$. Let the blocks of $A$ be $A_{1}, \ldots, A_{n}$. Let $c_{1}, \ldots, c_{n}$ be the numbers of columns of $A_{1}, \ldots, A_{n}$. For every $k\in\{1,\ldots,n\}$, $A_{j}$ is the matrix of the restriction of $T$ to
    \[
        \operatorname{span}(v_{c_{1} + \cdots + c_{k-1}+1}, \ldots, v_{c_{1} + \cdots + c_{k-1} + c_{k}}).
    \]

    Let $\lambda_{k}$ be the number on the diagonal entries of $A_{k}$, then $\lambda_{k}$ is an eigenvalue of $T$, because $\lambda_{k}$ lies on the diagonal of the upper-triangular matrix $A$.

    Moreover, for every $k\in\{1,\ldots,n\}$, each vector in the list of the following $c_{k}$ vectors
    \[
        v_{c_{1} + \cdots + c_{k-1}+1}, \ldots, v_{c_{1} + \cdots + c_{k-1} + c_{k}}
    \]

    is a generalized eigenvector of $T$ corresponding to $\lambda_{k}$, because
    \[
        {(T - \lambda_{k}I)}^{c_{k}}u = 0
    \]

    for every vector $u$ in the list
    \[
        v_{c_{1} + \cdots + c_{k-1}+1}, \ldots, v_{c_{1} + \cdots + c_{k-1} + c_{k}}.
    \]

    Thus every vector in each Jordan basis for $T$ is a generalized eigenvector of $T$.
\end{proof}
\newpage

% chapter8:sectionC:exercise12
\begin{exercise}\label{chapter8:sectionC:exercise12}
    Suppose $T \in \lmap{V}$ is diagonalizable. Show that $\mathcal{M}(T)$ is a diagonal matrix with respect to every Jordan basis for $T$.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{\dim V}$ be a Jordan basis for $T$ of the vector space $V$. According to Exercise~\ref{chapter8:sectionC:exercise11}, $v_{1}, \ldots, v_{\dim V}$ are generalized eigenvectors of $T$. By Exercise~\ref{chapter8:sectionA:exercise15}, because $T$ is diagonalizable, every generalized eigenvector of $T$ is an eigenvector of $T$. Therefore every vector in the Jordan basis $v_{1}, \ldots, v_{\dim V}$ for $T$ is an eigenvector of $T$. So the matrix of $T$ with respect to this Jordan basis is a diagonal matrix.

    Because the Jordan basis which we are working with is arbitrary, so we conclude that $\mathcal{M}(T)$ is a diagonal matrix with respect to every Jordan basis for $T$.
\end{proof}
\newpage

% chapter8:sectionC:exercise13
\begin{exercise}\label{chapter8:sectionC:exercise13}
    Suppose $T\in\lmap{V}$ is nilpotent. Prove that if $v_{1}, \ldots, v_{n}$ are vectors in $V$ and $m_{1}, \ldots, m_{n}$ are nonnegative integers such that
    \[
        T^{m_{1}}v_{1}, \ldots, Tv_{1}, v_{1}, \ldots, T^{m_{n}}v_{n}, \ldots, Tv_{n}, v_{n} \text{ is a basis of $V$}
    \]

    and
    \[
        T^{m_{1}+1}v_{1} = \cdots = T^{m_{n}+1}v_{n} = 0,
    \]

    then $T^{m_{1}}v_{1}, \ldots, T^{m_{n}}v_{n}$ is a basis of $\kernel{T}$.
\end{exercise}

\begin{quote}
    This exercise shows that $n = \dim \kernel{T}$. Thus the positive integer $n$ that appears above depends only on $T$ and not on the specific Jordan basis chosen for $T$.
\end{quote}

\begin{proof}
    Let $v$ be an arbitrary vector in $\kernel{T}$. $v$ is a linear combination of
    \[
        T^{m_{1}}v_{1}, \ldots, Tv_{1}, v_{1}, \ldots, T^{m_{n}}v_{n}, \ldots, Tv_{n}, v_{n}.
    \]

    So there exist scalars
    \[
        a_{1,m_{1}}, \ldots, a_{1,1}, a_{1,0}; \ldots; a_{n,m_{n}}, \ldots, a_{n,1}, a_{n,0}
    \]

    such that
    \[
        v = a_{1,m_{1}}T^{m_{1}}v_{1} + \cdots + a_{1,1}Tv_{1} + a_{1,0}v_{1} + \cdots + a_{n,m_{n}}T^{m_{n}}v_{n} + \cdots + a_{n,1}Tv_{n} + a_{n,0}v_{n}.
    \]

    Apply $T$ to both sides, we obtain
    \[
        0 = a_{1,m_{1}-1}T^{m_{1}}v_{1} + \cdots + a_{1,1}T^{2}v_{1} + a_{1,0}Tv_{1} + \cdots + a_{n,m_{n}-1}T^{m_{n}}v_{n} + \cdots + a_{n,1}T^{2}v_{n} + a_{n,0}Tv_{n}.
    \]

    Because the list $T^{m_{1}}v_{1}, \ldots, Tv_{1}, \ldots, T^{m_{n}}v_{n}, \ldots, Tv_{n}$ is linearly independent, it follows that
    \[
        a_{1,m_{1}-1} = \cdots = a_{1,1} = a_{1,0} = \cdots = a_{n,m_{n}-1} = \cdots = a_{n,1} = a_{n,0} = 0.
    \]

    Hence
    \[
        v = a_{1,m_{1}}T^{m_{1}}v_{1} + \cdots + a_{n,m_{n}}T^{m_{n}}v_{n}
    \]

    which means the list $T^{m_{1}}v_{1}, \ldots, T^{m_{n}}v_{n}$ spans $\kernel{T}$. Moreover, this list is linearly independent, so it is a basis of $\kernel{T}$.

    Thus $T^{m_{1}}v_{1}, \ldots, T^{m_{n}}v_{n}$ is a basis of $\kernel{T}$.
\end{proof}
\newpage

% chapter8:sectionC:exercise14
\begin{exercise}\label{chapter8:sectionC:exercise14}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T \in \lmap{V}$. Prove that there does not exist a direct sum decomposition of $V$ into two nonzero subspaces invariant under $T$ if and only if the minimal polynomial of $T$ is of the form ${(z - \lambda)}^{\dim V}$ for some $\lambda\in\mathbb{C}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There does not exist a direct sum decomposition of $V$ into two nonzero subspaces invariant under $T$.

    Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$, then $V$ has a generalized eigendecomposition
    \[
        V = \bigoplus^{m}_{j=1}G(\lambda_{j}, T)
    \]

    and every $G(\lambda_{j}, T)$ is invariant under $T$. Therefore $m = 1$, so $V = G(\lambda_{1}, T)$. Let $v_{1}, \ldots, v_{\dim V}$ be Jordan basis for $T$, then the matrix of $T$ with respect to this basis is a block diagonal matrix. Let the blocks be $A_{1}, \ldots, A_{n}$, then each $A_{j}$ is of the form
    \[
        \begin{pmatrix}
            \lambda_{1} & 1           & \cdots & 0           \\
            0           & \lambda_{1} & \cdots & 0           \\
            \vdots      & \vdots      &        & \vdots      \\
            0           & 0           & \cdots & \lambda_{1}
        \end{pmatrix}.
    \]

    Let $c_{1}, \ldots, c_{n}$ be the numbers of columns of $A_{1}, \ldots, A_{n}$ and let
    \[
        V_{j} = \operatorname{span}(v_{c_{1} + \cdots + c_{j-1}+1}, \ldots, v_{c_{1} + \cdots + c_{j-1} + c_{j}}).
    \]

    Then $V_{j}$ is invariant under $T$ and the matrix of $T\vert_{V_{j}}$ with respect to
    \[
        v_{c_{1} + \cdots + c_{j-1}+1}, \ldots, v_{c_{1} + \cdots + c_{j-1} + c_{j}}
    \]

    is $A_{j}$. Moreover,
    \[
        V = \bigoplus^{n}_{j=1}V_{j}
    \]

    so $n = 1$. Hence $A$ contains a single Jordan block, which means the minimal polynomial of $T$ is ${(z - \lambda_{1})}^{\dim V}$.

    \bigskip
    $(\Leftarrow)$ The minimal polynomial of $T$ is of the form ${(z - \lambda)}^{\dim V}$ for some $\lambda\in\mathbb{C}$.

    Assume $V$ admits a direct sum decomposition $V = U\oplus W$ where $U, W$ are nonzero subspaces and are invariant under $T$.

    The minimal polynomials of $T\vert_{U}, T\vert_{W}$ are polynomial divisors of ${(z - \lambda)}^{\dim V}$. Let the minimal polynomials of $T\vert_{U}, T\vert_{W}$ be ${(z - \lambda)}^{m}, {(z - \lambda)}^{n}$, respectively. Because the degree of the minimal polynomial of an operator on a finite-dimensional vector space does not exceed the dimension of the vector space, so $m\leq \dim U < \dim V$ and $n\leq \dim W < \dim V$.

    Let $p = \max\{m, n\}$, then ${(T - \lambda I)}^{p} = 0$, which contradicts ${(z - \lambda)}^{\dim V}$ being the minimal polynomial of $T$ because $p < \dim V$. Hence the assumption is false.

    Thus $V$ cannot be decomposed into a direct sum of two nonzero subspaces invariant under $T$.
\end{proof}
\newpage

\section{Trace: A Connection Between Matrices and Operators}

% chapter8:sectionD:exercise1
\begin{exercise}\label{chapter8:sectionD:exercise1}
    Suppose $V$ is an inner product space and $v, w\in V$. Define an operator $T\in\lmap{V}$ by $Tu = \innerprod{u, v}w$. Find a formula for $\operatorname{tr}T$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be an orthonormal basis of $T$.
    \begin{align*}
        \operatorname{tr}T & = \sum^{\dim V}_{j=1}\innerprod{Te_{j}, e_{j}} = \sum^{\dim V}_{j=1}\innerprod{\innerprod{e_{j}, v}w, e_{j}} = \sum^{\dim V}_{j=1}\innerprod{e_{j}, v}\innerprod{w, e_{j}} \\
                           & = \sum^{\dim V}_{j=1}\innerprod{w, e_{j}}\conj{\innerprod{v, e_{j}}}                                                                                                       \\
                           & = \innerprod{w, v}.
    \end{align*}

    Hence $\operatorname{tr}T = \innerprod{w, v}$.
\end{proof}
\newpage

% chapter8:sectionD:exercise2
\begin{exercise}\label{chapter8:sectionD:exercise2}
    Suppose $P\in\lmap{V}$ satisfies $P^{2} = P$. Prove that
    \[
        \operatorname{tr} P = \dim\range{P}.
    \]
\end{exercise}

\begin{proof}
    $P^{2} = P$ so $V = \kernel{P}\oplus \range{P}$, where $\kernel{P} = E(0, P)$ and $\range{P} = E(1, P)$. Hence $P$ is diagonalizable and there exists a basis of $V$ to which the matrix of $P$ is a diagonal matrix. The number of $0$'s on the diagonal is $\dim\kernel{P}$ and the number of $1$'s on the diagonal is $\dim\range{P}$. By the definition of trace of an operator, we conclude $\operatorname{tr}T = \dim\range{T}$.
\end{proof}
\newpage

% chapter8:sectionD:exercise3
\begin{exercise}\label{chapter8:sectionD:exercise3}
    Suppose $T\in\lmap{V}$ and $T^{5} = T$. Prove that the real and imaginary parts of $\operatorname{tr} T$ are both integers.
\end{exercise}

\begin{proof}
    Because $T^{5} = T$, $z^{5} - z$ is a polynomial multiple of the minimal polynomial of $T$.
    \[
        z^{5} - z = z(z - 1)(z + 1)(z - \iota)(z + \iota)
    \]

    so the characteristic polynomial of $T$ is
    \[
        z^{s_{1}}{(z - 1)}^{s_{2}}{(z + 1)}^{s_{3}}{(z - \iota)}^{s_{4}}{(z + \iota)}^{s_{5}}
    \]

    where $s_{1}, s_{2}, s_{3}, s_{4}, s_{5}$ are \textbf{nonnegative} integers (these are assumed to be negative since we don't know the minimal polynomial of $T$).

    $\operatorname{tr}T$ is the negative of the coefficient of $z^{s_{1} + s_{2} + s_{3} + s_{4} + s_{5} - 1}$ in the characteristic polynomial of $T$, so the real and imaginary parts of it are integers.
\end{proof}
\newpage

% chapter8:sectionD:exercise4
\begin{exercise}\label{chapter8:sectionD:exercise4}
    Suppose $V$ is an inner product space and $T\in\lmap{V}$. Prove that
    \[
        \operatorname{tr}T^{*} = \conj{\operatorname{tr} T}.
    \]
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be an orthonormal basis of $V$, then
    \[
        \operatorname{tr}T^{*} = \sum^{\dim V}_{j=1}\innerprod{T^{*}e_{j},e_{j}} = \sum^{\dim V}_{j=1}\conj{\innerprod{Te_{j}, e_{j}}} = \conj{\left(\sum^{\dim V}_{j=1}\innerprod{Te_{j}, e_{j}}\right)} = \conj{\operatorname{tr}T}.\qedhere
    \]
\end{proof}
\newpage

% chapter8:sectionD:exercise5
\begin{exercise}\label{chapter8:sectionD:exercise5}
    Suppose $V$ is an inner product space. Suppose $T\in\lmap{V}$ is a positive operator and $\operatorname{tr} T = 0$. Prove that $T = 0$.
\end{exercise}

\begin{proof}
    $T$ is a positive operator so $T$ is self-adjoint. By the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{\dim V}$ to which the matrix of $T$ is a diagonal matrix $A$. The trace of $T$ is the trace of $A$, which is equal to the sum of the eigenvalues of $T$ (each is included as many as its multiplicity). However, $T$ is a positive operator so all of its eigenvalues are nonnegative. Together with $\operatorname{tr}T = 0$, we conclude that $T = 0$.
\end{proof}
\newpage

% chapter8:sectionD:exercise6
\begin{exercise}\label{chapter8:sectionD:exercise6}
    Suppose $V$ is an inner product space and $P, Q\in\lmap{V}$ are orthogonal projections. Prove that $\operatorname{tr}(PQ) \geq 0$.
\end{exercise}

\begin{proof}
    Because $P, Q$ are orthogonal projections, then there exist subspaces $U, W$ of $V$ such that $P = P_{U}$ and $Q = P_{W}$. For every $v\in V$, $P_{U}P_{W}v\in U\cap W$, so ${(P_{U}P_{W})}^{2}v = P_{U}P_{W}v$. Hence ${(P_{U}P_{W})}^{2} = P_{U}P_{W}$, which means ${(PQ)}^{2} = PQ$. By Exercise~\ref{chapter8:sectionD:exercise2}, $\operatorname{tr}(PQ) = \dim\range{PQ}\geq 0$.
\end{proof}
\newpage

% chapter8:sectionD:exercise7
\begin{exercise}\label{chapter8:sectionD:exercise7}
    Suppose $T\in\lmap{\mathbb{C}^{3}}$ is the operator whose matrix is
    \[
        \begin{pmatrix}
            51 & -12 & -21 \\
            60 & -40 & -28 \\
            57 & -68 & 1
        \end{pmatrix}.
    \]

    Someone tells you (accurately) that $-48$ and $24$ are eigenvalues of $T$. Without using a computer or writing anything down, find the third eigenvalue of $T$.
\end{exercise}

\begin{proof}
    The third eigenvalue of $T$ is $51 - 40 + 1 - ((-48) + 24) = 36$.
\end{proof}
\newpage

% chapter8:sectionD:exercise8
\begin{exercise}\label{chapter8:sectionD:exercise8}
    Prove or give a counterexample: If $S, T \in \lmap{V}$, then $\operatorname{tr}(ST) = (\operatorname{tr} S)(\operatorname{tr} T)$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V = \mathbb{C}^{2}$, $S, T\in\lmap{V}$ and $S(w, z) = T(w, z) = (z, w)$. The matrix of $S, T$ with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}$. Therefore
    \[
        \operatorname{tr}(ST) = \operatorname{tr}(I) = 2 \ne 0 = 0\cdot 0 = (\operatorname{tr} S)(\operatorname{tr} T).
    \]
\end{proof}
\newpage

% chapter8:sectionD:exercise9
\begin{exercise}\label{chapter8:sectionD:exercise9}
    Suppose $T\in\lmap{V}$ is such that $\operatorname{tr}(ST) = 0$ for all $S\in\lmap{V}$. Prove that $T = 0$.
\end{exercise}

\begin{proof}
    Choose $S = T^{*}$, then $\operatorname{tr}(T^{*}T) = 0$. On the other hand, $T^{*}T$ is a positive operator, so according to Exercise~\ref{chapter8:sectionD:exercise5}, $T^{*}T = 0$. Thus $T = 0$.
\end{proof}
\newpage

% chapter8:sectionD:exercise10
\begin{exercise}\label{chapter8:sectionD:exercise10}
    Prove that the trace is the only linear functional $\tau: \lmap{V}\to\mathbb{F}$ such that
    \[
        \tau(ST) = \tau(TS)
    \]

    for all $S, T\in\lmap{V}$ and $\tau(I) = \dim V$.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$. For each pair of $j,k\in\{1,\ldots,n\}$, define $P_{j,k}\in\lmap{V}$ by
    \[
        P_{j,k}(a_{1}v_{1} + \cdots + a_{n}v_{n}) = a_{k}v_{j}.
    \]

    Suppose $j\ne k$.
    \begin{align*}
        P_{j,j}P_{j,k}(a_{1}v_{1} + \cdots + a_{n}v_{n}) & = P_{j,j}(a_{k}v_{j}) = a_{k}v_{j} = P_{j,k}(a_{1}v_{1} + \cdots + a_{n}v_{n})  \\
        P_{j,k}P_{j,j}(a_{1}v_{1} + \cdots + a_{n}v_{n}) & = P_{j,k}(a_{j}v_{j}) = 0                                                       \\
        P_{k,k}P_{j,k}(a_{1}v_{1} + \cdots + a_{n}v_{n}) & = P_{k,k}(a_{k}v_{j}) = 0                                                       \\
        P_{j,k}P_{k,k}(a_{1}v_{1} + \cdots + a_{n}v_{n}) & = P_{j,k}(a_{k}v_{k}) = a_{k}v_{j} = P_{j,k}(a_{1}v_{1} + \cdots + a_{n}v_{n}).
    \end{align*}

    Therefore $P_{j,j}P_{j,k} = P_{j,k}P_{k,k} = P_{j,k}$ and $P_{j,k}P_{j,j} = P_{k,k}P_{j,k} = 0$, so
    \begin{align*}
        \tau(P_{j,k}) = \tau(P_{j,j}P_{j,k}) = \tau(P_{j,k}P_{j,j}) = 0.
    \end{align*}

    Let $C_{j,k} = C_{k,j}\in\lmap{V}$ defined by
    \[
        C_{j,k}v_{i} = \begin{cases}
            v_{k} & i = j            \\
            v_{j} & i = k            \\
            v_{i} & \text{otherwise}
        \end{cases}
    \]

    then $C_{j,k}^{2} = I$, which means $C_{j,k}^{-1} = C_{j,k}$.
    \begin{align*}
        (C_{j,k}^{-1}P_{j,j}C_{j,k})(a_{1}v_{1} + \cdots + a_{n}v_{n}) & = (C_{j,k}^{-1}P_{j,j})(\cdots + a_{k}v_{j} + \cdots + a_{j}v_{k} + \cdots) \\
                                                                       & = C_{j,k}^{-1}(a_{k}v_{j})                                                  \\
                                                                       & = a_{k}v_{k}                                                                \\
                                                                       & = P_{k,k}(a_{1}v_{1} + \cdots + a_{n}v_{n}).
    \end{align*}

    Hence
    \begin{align*}
        \tau(P_{k,k}) & = \tau(C_{j,k}^{-1}(P_{j,j}C_{j,k}))                                          \\
                      & = \tau(P_{j,j}C_{j,k}C_{j,k}^{-1})   & \text{(because $\tau(ST) = \tau(TS)$)} \\
                      & = \tau(P_{j,j})
    \end{align*}

    which means $\tau(P_{1,1}) = \cdots = \tau(P_{n,n})$. On the other hand,
    \[
        \tau(P_{1,1}) + \cdots + \tau(P_{n,n}) = \tau(I) = \dim V = n
    \]

    so $\tau(P_{1,1}) = \cdots = \tau(P_{n,n}) = 1$. Therefore
    \[
        \tau(P_{j,k}) = \begin{cases}
            1 & \text{if $j = k$}  \\
            0 & \text{if $j\ne k$}
        \end{cases}.
    \]

    Finally, for every $T\in\lmap{V}$
    \begin{align*}
        \tau(T) & = \tau\left(\sum^{n}_{k=1}\sum^{n}_{j=1}{(\mathcal{M}(T))}_{j,k}P_{j,k} \right) \\
                & = \sum^{n}_{k=1}\sum^{n}_{j=1}{(\mathcal{M}(T))}_{j,k}\tau(P_{j,k})             \\
                & = \sum^{n}_{k=1}{(\mathcal{M}(T))}_{k,k}                                        \\
                & = \operatorname{tr}(T).
    \end{align*}

    Hence $\tau = \operatorname{tr}$. Thus $\operatorname{tr}$ is the only linear functional on $V$ such that $\tau(ST) = \tau(TS)$ for all $S, T\in\lmap{V}$ and $\tau(I) = \dim V$.
\end{proof}
\newpage

% chapter8:sectionD:exercise11
\begin{exercise}\label{chapter8:sectionD:exercise11}
    Suppose $V$ and $W$ are inner product spaces and $T\in\lmap{V, W}$. Prove that if $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$ and $f_{1}, \ldots, f_{m}$ is an orthonormal basis of $W$, then
    \[
        \operatorname{tr}(T^{*}T) = \sum^{n}_{k=1}\sum^{m}_{j=1}\abs{\innerprod{Te_{k}, f_{j}}}^{2}.
    \]
\end{exercise}

\begin{quote}
    The numbers $\innerprod{Te_{k}, f_{j}}$ are the entries of the matrix of $T$ with respect to the orthonormal bases $e_{1}, \ldots, e_{n}$ and $f_{1}, \ldots, f_{m}$. These numbers depend on the bases, but $\operatorname{tr}(T^{*}T)$ does not depend on a choice of bases. Thus this exercise shows that the sum of the squares of the absolute values of the matrix entries does not depend on which orthonormal bases are used.
\end{quote}

\begin{proof}
    Let $A$ be the matrix of $T$ with respect to the bases $e_{1}, \ldots, e_{n}$ and $f_{1}, \ldots, f_{m}$, then $A^{*}$ is the matrix of $T^{*}$ with respect to the bases $f_{1}, \ldots, f_{m}$ and $e_{1}, \ldots, e_{n}$.
    \begin{align*}
        \operatorname{tr}(T^{*}T) & = \operatorname{tr}(A^{*}A)                                        \\
                                  & = \sum^{n}_{k=1}{(A^{*}A)}_{k,k}                                   \\
                                  & = \sum^{n}_{k=1}\sum^{m}_{j=1}{(A^{*})}_{k,j}A_{j,k}               \\
                                  & = \sum^{n}_{k=1}\sum^{m}_{j=1}\conj{A_{j,k}}A_{j,k}                \\
                                  & = \sum^{n}_{k=1}\sum^{m}_{j=1}\abs{A_{j,k}}^{2}                    \\
                                  & = \sum^{n}_{k=1}\sum^{m}_{j=1}\abs{\innerprod{Te_{k}, f_{j}}}^{2}.
    \end{align*}
\end{proof}
\newpage

% chapter8:sectionD:exercise12
\begin{exercise}\label{chapter8:sectionD:exercise12}
    Suppose $V$ and $W$ are finite-dimensional inner product spaces.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $\innerprod{S, T} = \operatorname{tr}(T^{*}S)$ defines an inner product on $\lmap{V, W}$.
        \item Suppose $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$ and $f_{1}, \ldots, f_{m}$ is an orthonormal basis of $W$. Show that the inner product on $\lmap{V, W}$ from (a) is the same as the standard inner product on $\mathbb{F}^{mn}$, where we identify each element of $\lmap{V, W}$ with its matrix (with respect to the bases just mentions) and then with an element of $\mathbb{F}^{mn}$.
    \end{enumerate}
\end{exercise}

\begin{quote}
    Caution: The norm of a linear map $T\in\lmap{V, W}$ as defined by 7.86 is not the same as the norm that comes from the inner product in (a) above. Unless explicitly stated otherwise, always assume that $\norm{T}$ refers to the norm as defined by 7.86. The norm that comes from the inner product in (a) is called the \textbf{Frobenius norm} or the \textbf{Hilbert-Schmidt norm}.
\end{quote}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $\innerprod{S_{1} + S_{2}, T} = \operatorname{tr}(T^{*}(S_{1} + S_{2})) = \operatorname{tr}(T^{*}S_{1}) + \operatorname{tr}(T^{*}S_{2}) = \innerprod{S_{1}, T} + \innerprod{S_{2}, T}$. So the first slot is additive.

              $\innerprod{\lambda S, T} = \operatorname{tr}(T^{*}(\lambda S)) = \operatorname{tr}(\lambda T^{*}S) = \lambda\operatorname{tr}(T^{*}S) = \lambda\innerprod{S, T}$. So the first slot is homogeneitive.

              By Exercise~\ref{chapter8:sectionC:exercise11}, $\innerprod{T, T} = \operatorname{tr}(T^{*}T)\geq 0$. So $\innerprod{\cdot,\cdot}$ is positive.

              $T^{*}T$ is a positive operator, so $\operatorname{tr}(T^{*}T) = 0$ implies $T^{*}T = 0$ (according to Exercise~\ref{chapter8:sectionD:exercise5}), and $T^{*}T = 0$ implies $T = 0$. So $\innerprod{\cdot,\cdot}$ is positive definite.

              By Exercise~\ref{chapter8:sectionD:exercise4}
              \[
                  \innerprod{S, T} = \operatorname{tr}(T^{*}S) = \conj{\operatorname{tr}(S^{*}T)} = \conj{\innerprod{T, S}}.
              \]

              So $\innerprod{\cdot,\cdot}$ is conjugate symmetric.

              Hence $\innerprod{\cdot,\cdot}$ is an inner product on $\lmap{V, W}$.
        \item Let $S, T\in\lmap{V, W}$ and $A, B$ are the matrices of $S, T$ with respect to the orthonormal bases $e_{1}, \ldots, e_{n}$ of $V$ and $f_{1}, \ldots, f_{m}$ of $W$.
              \begin{align*}
                  \innerprod{S, T} & = \operatorname{tr}(T^{*}S) = \operatorname{tr}(B^{*}A)          \\
                                   & = \sum^{n}_{k=1}{(B^{*}A)}_{k,k}                                 \\
                                   & = \sum^{n}_{k=1}\sum^{m}_{j=1}{(B^{*})}_{k,j}A_{j,k}             \\
                                   & = \sum^{n}_{k=1}\sum^{m}_{j=1}A_{j,k}\conj{B_{j,k}}              \\
                                   & = \innerprod{A, B} = \innerprod{\mathcal{M}(S), \mathcal{M}(T)}.
              \end{align*}

              Hence the inner product on $\lmap{V, W}$ from (a) is the same as the standard inner product on $\mathbb{F}^{mn}$.
    \end{enumerate}
\end{proof}
\newpage

% chapter8:sectionD:exercise13
\begin{exercise}\label{chapter8:sectionD:exercise13}
    Find $S, T\in\lmap{\mathscr{P}(\mathbb{F})}$ such that $ST - TS = I$.
\end{exercise}

\begin{proof}
    Choose $S = D$ (the differentiation operator on $\mathscr{P}(\mathbb{F})$) and $T$ defined by $T(p(x)) = xp(x)$.
    \begin{align*}
        (ST - TS)(p(x)) & = D(xp(x)) - xp'(x)      \\
                        & = p(x) + xp'(x) - xp'(x) \\
                        & = p(x)                   \\
                        & = I(p(x)).
    \end{align*}

    Thus $ST - TS = I$.
\end{proof}
\newpage
