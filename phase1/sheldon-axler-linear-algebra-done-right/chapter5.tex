\chapter{Eigenvalues and Eigenvectors}

\section{Invariant Subspaces}

% chapter5:sectionA:exercise1
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $U$ is a subspace of $V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that if $U\subseteq \kernel{T}$, then $U$ is invariant under $T$.
        \item Prove that if $\range{T}\subseteq U$, then $U$ is invariant under $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter5:sectionA:exercise2
\begin{exercise}
    Suppose that $T\in\lmap{V}$ and $V_{1}, \ldots, V_{m}$ are subspaces of $V$ invariant under $T$. Prove that $V_{1} + \cdots + V_{m}$ is invariant under $T$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter5:sectionA:exercise3
\begin{exercise}
    Suppose $T\in\lmap{V}$. Prove that the intersection of every collection of subspaces of $V$ invariant under $T$ is invariant under $T$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter5:sectionA:exercise4
\begin{exercise}
    Prove or give a counterexample: If $V$ is finite-dimensional and $U$ is a subspace of $V$ that is invariant under every operator on $V$, then $U = \{0\}$ or $U = V$.
\end{exercise}

\begin{proof}
    The answer is affirmative.

    If $U = \{0\}$ or $U = V$, then $U$ is invariant under every operator on $V$.

    Let $U$ be a subspace of $V$ such that $U\ne \{0\}$ and $U\ne V$ and invariant under $T$.

    Let $u_{1}, \ldots, u_{m}$ be a basis of $U$. Extend this list to obtain a basis of $V$ and let it be
    \[
        u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{n}.
    \]

    I define the operator $T$ on $V$ as follows: $Tu_{1} = v_{1}$, $Tu_{k} = u_{k}$ for $k = 2,\ldots, m$, and $Tv_{i} = v_{i}$ for $i = 1,\ldots, n$. Then $U$ is not invariant under $T$.

    Hence if $V$ is finite-dimensional and $U$ is a subspace of $V$ that is invariant under every operator on $V$, then $U = \{0\}$ or $U = V$.
\end{proof}
\newpage

% chapter5:sectionA:exercise5
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{R}^{2}}$ is defined by $T(x, y) = (-3y, x)$. Find the eigenvalues of $T$.
\end{exercise}

\begin{proof}
    Suppose $\lambda$ is an eigenvalue of $T$, then there exists $(x, y)$ such that $(-3y, x) = (\lambda x, \lambda y)$.
    \[
        -3x = -3\lambda y = \lambda^{2}x
    \]

    it follows that $x(\lambda^{2} + 3) = 0$ so $x = 0$. Because $x = \lambda y$, we deduce that $\lambda y = 0$. However $y\ne 0$ because $(x, y)$ is an eigenvector, so $\lambda = 0$. Therefore $x = y = 0$.

    Hence $T$ has no (real) eigenvalues.
\end{proof}
\newpage

% chapter5:sectionA:exercise6
\begin{exercise}
    Define $T\in\lmap{\mathbb{F}^{2}}$ by $T(w, z) = (z, w)$. Find all eigenvalues and eigenvectors of $T$.
\end{exercise}

\begin{proof}
    The eigenvalues of $T$ are $1$ and $-1$.

    The eigenvectors of $T$ with respect to $1$ are of the form $(z, z)$.

    The eigenvectors of $T$ with respect to $-1$ are of the form $(z, -z)$.
\end{proof}
\newpage

% chapter5:sectionA:exercise7
\begin{exercise}
    Define $T\in\lmap{\mathbb{F}^{3}}$ by $T(z_{1}, z_{2}, z_{3}) = (2z_{2}, 0, 5z_{3})$. Find all eigenvalues and eigenvectors of $T$.
\end{exercise}

\begin{proof}
    The only eigenvalues of $T$ are $0$ and $5$.

    The eigenvectors of $T$ with respect to $0$ are of the form $(z, 0, 0)$.

    The eigenvectors of $T$ with respect to $0$ are of the form $(0, 0, z)$.
\end{proof}
\newpage

% chapter5:sectionA:exercise8
\begin{exercise}
    Suppose $P\in\lmap{V}$ is such that $P^{2} = P$. Prove that if $\lambda$ is an eigenvalue of $P$, then $\lambda = 0$ or $\lambda = 1$.
\end{exercise}

\begin{proof}
    If $\lambda$ is an eigenvalue of $P$, then for every eigenvector $v$ of $P$ with respect to $\lambda$, we have
    \[
        \lambda v = Pv = P^{2}v = \lambda^{2}v
    \]

    Hence $\lambda = \lambda^{2}$, which implies $\lambda = 0$ or $\lambda = 1$.
\end{proof}
\newpage

% chapter5:sectionA:exercise9
\begin{exercise}
    Define $T: \mathscr{P}(\mathbb{R})\to \mathscr{P}(\mathbb{R})$ by $Tp = p'$. Find all eigenvalues and eigenvectors of $T$.
\end{exercise}

\begin{proof}
    If $\deg P > 0$, then $\deg p' = (\deg p) - 1$. So every nonconstant polynomial is not an eigenvector of $T$.

    If $\deg P = 0$, then $p' = 0 = 0p$. So every nonzero constant polynomial is an eigenvector of $T$ with respect to the eigenvalue $0$.
\end{proof}
\newpage

% chapter5:sectionA:exercise10
\begin{exercise}
    Define $T\in\lmap{\mathscr{P}(\mathbb{R})}$ by $(Tp)(x) = xp'(x)$ for all $x\in \mathbb{R}$. Find all eigenvalues and eigenvectors of $T$.
\end{exercise}

The original problem is $T\in\lmap{\mathscr{P}_{4}(\mathbb{R})}$

\begin{proof}
    Let $\lambda$ be an eigenvalue of $T$ and $p(x) = a_{0} + a_{1}x + \cdots + a_{n}x^{n}$ an eigenvector corresponding to $\lambda$.
    \[
        \begin{split}
            xp'(x) = a_{1}x + 2a_{2}x^{2} + \cdots + na_{n}x^{n} \\
            \lambda p(x) = \lambda a_{0} + \lambda a_{1}x + \cdots + \lambda a_{n}x^{n}.
        \end{split}
    \]

    By comparing the coefficients, we obtain $0 = \lambda a_{0}$, $a_{1} = \lambda a_{1}$, \ldots, $na_{n} = \lambda a_{n}$.

    So $\lambda = 0$ and $p$ is a nonzero constant polynomial, or $\lambda = n$ and $p(x) = x^{n}$.

    Hence the eigenvalues of $T$ are nonnegative integer $n$ and the corresponding eigenvectors are of the form $kx^{n}$ (where $k$ is a nonzero constant in $\mathbb{F}$).
\end{proof}
\newpage

% chapter5:sectionA:exercise11
\begin{exercise}\label{chapter5:sectionA:exercise11}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $\alpha\in\mathbb{F}$. Prove that there exists $\delta > 0$ such that $T - \lambda I$ is invertible for all $\lambda\in\mathbb{F}$ such that $0 < \abs{\alpha - \lambda} < \delta$.
\end{exercise}

\begin{proof}
    $T$ has at most $(\dim V)$ eigenvalues. Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$.

    If $\alpha$ is not an eigenvalue of $T$, I choose
    \[
        \delta = \min\left\{ \abs{\alpha - \lambda_{i}}: 1\leq i\leq m \right\}.
    \]

    Then for all $\lambda$ such that $0 < \abs{\alpha - \lambda} < \delta$, $\lambda$ is not an eigenvalue of $T$.

    If $\alpha = \lambda_{k}$ for some $k$ in $1, \ldots, m$, I choose
    \[
        \delta = \min\left\{ \abs{\alpha - \lambda_{i}}: 1\leq i\leq m\wedge i\ne k \right\}.
    \]

    Then for all $\lambda$ such that $0 < \abs{\alpha - \lambda} < \delta$, $\lambda$ is not an eigenvalue of $T$.

    On the other hand, $\lambda\in\mathbb{F}$ is not an eigenvalue of $T$ if and only if $T - \lambda I$ is invertible.

    Hence there exists $\delta > 0$ such that $T - \lambda I$ is invertible for all $\lambda\in\mathbb{F}$ such that $0 < \abs{\alpha - \lambda} < \delta$.
\end{proof}
\newpage

% chapter5:sectionA:exercise12
\begin{exercise}
    Suppose $V = U\oplus W$, where $U$ and $W$ are nonzero subspaces of $V$. Define $P\in\lmap{V}$ by $P(u + w) = u$ for each $u\in U$ and each $w\in W$. Find all eigenvalues and eigenvectors of $P$.
\end{exercise}

\begin{proof}
    Let $\lambda$ be an eigenvalue of $P$ and $u + w$ be an eigenvector corresponding to $\lambda$ (where $u\in U$, $w\in W$), then $P(u + w) = \lambda(u + w)$. According to the definition of $P$, $\lambda (u + w) = u$, so $(\lambda - 1)u + \lambda w = 0$.

    Because $U\cap W = \{0\}$, it follows that $(\lambda - 1)u = 0$ and $\lambda w = 0$. Therefore $\lambda = 1$ or $\lambda = 0$.

    Hence the eigenvalues of $P$ are $1$ and $0$. The eigenvectors corresponding to $1$ is $ku$ (where $k\in\mathbb{F}$ and $k\ne 0$). The eigenvectors corresponding to $0$ is $kw$ (where $k\in\mathbb{F}$ and $k\ne 0$).
\end{proof}
\newpage

% chapter5:sectionA:exercise13
\begin{exercise}
    Suppose $T\in\lmap{V}$. Suppose $S\in\lmap{V}$ is invertible.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $T$ and $S^{-1}TS$ have the same eigenvalues.
        \item What is the relationship between the eigenvectors of $T$ and the eigenvectors of $S^{-1}TS$?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item If $\lambda$ is an eigenvalue of $T$.

              Let $v$ be an eigenvector corresponding to $\lambda$. Since $S$ is invertible, there exists $w\in V$ such that $Sw = v$. Therefore
              \[
                  (S^{-1}TS)(w) = (S^{-1}T)(v) = S^{-1}(\lambda v) = \lambda w.
              \]

              Hence $\lambda$ is also an eigenvalue of $S^{-1}TS$.

              If $\lambda$ is an eigenvalue of $S^{-1}TS$.

              Let $w$ be an eigenvector corresponding to $\lambda$, then $(S^{-1}TS)(w) = \lambda w$. Let $v = Sw$. Then apply $S$ to $(S^{-1}TS)(w)$ and $\lambda w$, we obtain that $(TS)(w) = S(\lambda w)$ and
              \[
                  Tv = T(Sw) = S(\lambda w) = \lambda Sw = \lambda v.
              \]

              Hence $\lambda$ is also an eigenvalue of $T$.

              Thus $T$ and $S^{-1}TS$ have the same eigenvalues.
        \item Let $A$ be the set of eigenvectors of $T$ with respect to an eigenvalue $\lambda$. Let $B$ be the set of eigenvectors of $S^{-1}TS$ with respect to the eigenvalue $\lambda$.

              There is a bijection from $A$ onto $B$ defined by $v\mapsto S^{-1}v$. In other word, $v$ is an eigenvector of $T$ corresponding to $\lambda$ if and only if $S^{-1}v$ is an eigenvector of $S^{-1}TS$ corresponding to $\lambda$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise14
\begin{exercise}
    Give an example of an operator on $\mathbb{R}^{4}$ that has no (real) eigenvalues.
\end{exercise}

\begin{proof}
    I define $T\in\lmap{\mathbb{R}^{4}}$ as follows:
    \[
        T(x_{1}, x_{2}, x_{3}, x_{4}) = (-x_{2}, x_{1}, -x_{4}, x_{3}).
    \]

    If $\lambda$ is an eigenvalue of $T$ and $(x_{1}, x_{2}, x_{3}, x_{4})$ is an eigenvector corresponding to $\lambda$, then
    \[
        (-x_{2}, x_{1}, -x_{4}, x_{3}) = \lambda (x_{1}, x_{2}, x_{3}, x_{4}).
    \]

    Therefore $x_{2} = -\lambda x_{1} = -\lambda^{2}x_{2}$ and $x_{4} = -\lambda x_{3} = -\lambda^{2}x_{4}$. Because $\lambda^{2} + 1\ne 0$, it follows that $x_{2} = 0$ and $x_{4} = 0$. So $x_{1} = 0$ and $x_{3} = 0$. This contradicts $(x_{1}, x_{2}, x_{3}, x_{4})$ being an eigenvector.

    Hence $T$ has no (real) eigenvalues.
\end{proof}
\newpage

% chapter5:sectionA:exercise15
\begin{exercise}\label{chapter5:sectionA:exercise15}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $\lambda\in\mathbb{F}$. Show that $\lambda$ is an eigenvalue of $T$ if and only if $\lambda$ is an eigenvalue of the dual operator $T'\in\lmap{V'}$.
\end{exercise}

\begin{proof}
    The range of a linear map and the range of its dual map have the same dimension. $T' - \lambda I'$ is the dual map of $T - \lambda I$. According to the fundamental theorem of linear maps
    \begin{align*}
        \dim\kernel{(T - \lambda I)} & = \dim V - \dim\range{(T - \lambda I)}    \\
                                     & = \dim V' - \dim\range{(T' - \lambda I')} \\
                                     & = \dim\kernel{(T' - \lambda I')}.
    \end{align*}

    $\lambda$ is an eigenvalue of $T$ if and only if $\kernel{(T - \lambda I)}\ne \{0\}$. $\lambda$ is an eigenvalue of $T'$ if and only if $\kernel{(T' - \lambda I')}\ne \{0\}$. Therefore $\lambda$ is an eigenvalue of $T$ if and only if $\lambda$ is an eigenvalue of $T'$.
\end{proof}
\newpage

% chapter5:sectionA:exercise16
\begin{exercise}
    Suppose $v_{1}, \ldots, v_{n}$ is a basis of $V$ and $T\in\lmap{V}$. Prove that if $\lambda$ is an eigenvalue of $T$, then
    \[
        \abs{\lambda}\leq n\max\left\{ \abs{{\mathcal{M}(T)}_{j,k}}: 1\leq j, k\leq n \right\},
    \]

    where ${\mathcal{M}(T)}_{j,k}$ denotes the entry in row $j$, column $k$ of the matrix of $T$ with respect to the basis $v_{1}, \ldots, v_{n}$.
\end{exercise}

\begin{proof}
    Let $A = \mathcal{M}(T)$ and $v = x_{1}v_{1} + \cdots + x_{n}v_{n}$ be an eigenvector of $T$ with respect to $\lambda$, then
    \[
        A\begin{pmatrix}x_{1} \\ \vdots \\ x_{n}\end{pmatrix} = \begin{pmatrix}
            A_{1,1}x_{1} + \cdots + A_{1,n}x_{n} \\
            \vdots                               \\
            A_{n,1}x_{1} + \cdots + A_{n,n}x_{n}
        \end{pmatrix} = \begin{pmatrix}
            \lambda x_{1} \\ \vdots \\ \lambda x_{n}
        \end{pmatrix}
    \]

    By Cauchy-Schwarz inequality
    \begin{align*}
        \abs{\lambda}^{2}\abs{x_{1}}^{2} + \cdots + \abs{\lambda}^{2}\abs{x_{n}}^{2} & = \sum^{n}_{i=1}{\abs{A_{i,1}x_{1} + \cdots + A_{i,n}x_{n}}}^{2}                                                \\
                                                                                     & \leq \sum^{n}_{i=1}(\abs{x_{1}}^{2} + \cdots + \abs{x_{n}}^{2})(\abs{A_{i,1}}^{2} + \cdots + \abs{A_{i,n}}^{2}) \\
                                                                                     & = (\abs{x_{1}}^{2} + \cdots + \abs{x_{n}}^{2})\sum_{j,k}\abs{A_{j,k}}^{2}                                       \\
                                                                                     & \leq n^{2}(\abs{x_{1}}^{2} + \cdots + \abs{x_{n}}^{2})\max\left\{\abs{A_{j,k}}^{2}: 1\leq j, k\leq n\right\}
    \end{align*}

    Because $\abs{x_{1}}^{2} + \cdots + \abs{x_{n}}^{2}\ne 0$, we deduce that
    \[
        \lambda^{2}\leq n^{2}\max\left\{\abs{A_{j,k}}^{2}: 1\leq j, k\leq n\right\}.
    \]

    Hence
    \[
        \abs{\lambda}\leq n\max\left\{\abs{A_{j,k}}: 1\leq j, k\leq n\right\}.\qedhere
    \]
\end{proof}
\newpage

% chapter5:sectionA:exercise17
\begin{exercise}\label{chapter5:sectionA:exercise17}
    Suppose $\mathbb{F} = \mathbb{R}$, $T\in\lmap{V}$, and $\lambda\in\mathbb{R}$. Prove that $\lambda$ is an eigenvalue of $T$ if and only if $\lambda$ is an eigenvalue of the complexification of $T_{\mathbb{C}}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $\lambda$ is an eigenvalue of $T$.

    Let $v$ be an eigenvector of $T$ corresponding to $\lambda$. Then according to the definition of $T_{\mathbb{C}}$
    \[
        T_{\mathbb{C}}(v + \iota v) = Tv + \iota Tv = \lambda v + \iota \lambda v.
    \]

    Hence $\lambda$ is also an eigenvalue of $T_{\mathbb{C}}$.

    $(\Rightarrow)$ $\lambda$ is an eigenvalue of $T_{\mathbb{C}}$.

    Let $u + \iota v$ be an eigenvector of $T_{\mathbb{C}}$ corresponding to $\lambda$. Because $\lambda\in\mathbb{R}$ and due to the definition of eigenvalue and eigenvector
    \[
        \begin{split}
            T_{\mathbb{C}}(u + \iota v) = Tu + \iota Tv, \\
            T_{\mathbb{C}}(u + \iota v) = \lambda u + \iota \lambda v.
        \end{split}
    \]

    Hence $Tu = \lambda u$ and $Tv = \lambda v$. Because $u + \iota v\ne 0 + \iota 0$, it follows that at least one vector in the list $u, v$ is nonzero. Hence $\lambda$ is also an eigenvalue of $T$.
\end{proof}
\newpage

% chapter5:sectionA:exercise18
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{R}$, $T\in\lmap{V}$, and $\lambda\in\mathbb{C}$. Prove that $\lambda$ is an eigenvalue of the complexification $T_{\mathbb{C}}$ if and only if $\conj{\lambda}$ is an eigenvalue of $T_{\mathbb{C}}$.
\end{exercise}

\begin{proof}
    Let $a = \operatorname{Re}\lambda$ and $b = \operatorname{Im}\lambda$.

    $(\Rightarrow)$ $\lambda$ is an eigenvalue of $T_{\mathbb{C}}$.

    Let $u + \iota v$ be an eigenvector of $T_{\mathbb{C}}$ corresponding to $\lambda$, then
    \[
        Tu + \iota Tv = T_{\mathbb{C}}(u + \iota v) = \lambda (u + \iota v) = (a + b\iota)(u + \iota v) = (au - bv) + \iota(av + bu).
    \]

    So $Tu = au - bv$ and $Tv = av + bu$. Then
    \[
        T_{\mathbb{C}}(u - \iota v) = Tu - \iota Tv = (au - bv) - \iota (av + bu) = (a - b\iota)(u - \iota v) = \conj{\lambda}(u - \iota v).
    \]

    Moreover, $u + \iota v\ne 0 + \iota 0$ so $u - \iota v\ne 0 + \iota 0$. Hence $\conj{\lambda}$ is an eigenvalue of $T_{\mathbb{C}}$.

    $(\Leftarrow)$ $\conj{\lambda}$ is an eigenvalue of $T_{\mathbb{C}}$.

    According to the previous part, $\conj{\conj{\lambda}}$ is an eigenvalue of $T_{\mathbb{C}}$. Hence $\lambda$ is an eigenvalue of $T_{\mathbb{C}}$.

    \bigskip

    Thus $\lambda$ is an eigenvalue of $T_{\mathbb{C}}$ if and only if $\conj{\lambda}$ is an eigenvalue of $T_{\mathbb{C}}$.
\end{proof}
\newpage

% chapter5:sectionA:exercise19
\begin{exercise}
    Show that the forward shift operator $T\in\lmap{\mathbb{F}^{\infty}}$ defined by
    \[
        T(z_{1}, z_{2}, \ldots) = (0, z_{1}, z_{2}, \ldots)
    \]

    has no eigenvalues.
\end{exercise}

\begin{proof}
    Assume $\lambda$ is an eigenvalue of $T$. Let $(z_{1}, z_{2}, \ldots)$ be an eigenvector of $T$ corresponding to $\lambda$.
    \[
        T(z_{1}, z_{2}, \ldots) = (\lambda z_{1}, \lambda z_{2}, \ldots).
    \]

    Identify $(\lambda z_{1}, \lambda z_{2}, \ldots)$ and $(0, z_{1}, z_{2}, \ldots)$, we obtain $\lambda z_{1} = 0$ and $\lambda z_{n+1} = z_{n}$. If $\lambda\ne 0$, then $z_{n} = 0$ for every positive integer $n$. If $\lambda = 0$, then $z_{n} = 0$ for every positive integer $n$. This contradicts $(z_{1}, z_{2}, \ldots)$ being an eigenvector.

    Thus $T$ has no eigenvalues.
\end{proof}
\newpage

% chapter5:sectionA:exercise20
\begin{exercise}
    Define the backward shift operator $S\in\lmap{\mathbb{F}^{\infty}}$ by
    \[
        S(z_{1}, z_{2}, z_{3}, \ldots) = (z_{2}, z_{3}, \ldots)
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that every element of $\mathbb{F}$ is an eigenvalue of $S$.
        \item Find all eigenvectors of $S$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $\lambda$ be an element of $\mathbb{F}$, then
              \[
                  S(1, \lambda, \lambda^{2}, \ldots) = (\lambda, \lambda^{2}, \ldots) = \lambda (1, \lambda, \lambda^{2}, \ldots)
              \]

              So $\lambda$ is an eigenvalue of $S$.
        \item Let $\lambda$ be an eigenvalue of $S$. Let $(z_{1}, z_{2}, z_{3}, \ldots)$ be an eigenvector of $S$ corresponding to $\lambda$. Then
              \[
                  (z_{2}, z_{3}, \ldots) = (\lambda z_{1}, \lambda z_{2}, \ldots)
              \]

              So $z_{n} = \lambda^{n-1}z_{1}$ for every positive integer $n > 1$. Because $(z_{1}, z_{2}, z_{3}, \ldots)$ is nonzero, it follows that $z_{1}\ne 0$. Hence the eigenvectors of $S$ corresponding to the eigenvalue $\lambda$ are of the form
              \[
                  (z_{1}, \lambda z_{1}, \lambda^{2}z_{1}, \ldots)
              \]

              where $z_{1}\ne 0$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise21
\begin{exercise}
    Suppose $T\in\lmap{V}$ is invertible.
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $\lambda\in\mathbb{F}$ with $\lambda \ne 0$. Prove that $\lambda$ is an eigenvalue of $T$ if and only if $\frac{1}{\lambda}$ is an eigenvalue of $T^{-1}$.
        \item Prove that $T$ and $T^{-1}$ have the same eigenvectors.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item If $\lambda$ is an eigenvalue of $T$, then there exists a nonzero vector $v$ such that $Tv = \lambda v$. So $v = T^{-1}(\lambda v)$, and it follows that $T^{-1}v = \frac{1}{\lambda}v$. Hence $\frac{1}{\lambda}$ is an eigenvalue of $T^{-1}$.

              If $\frac{1}{\lambda}$ is an eigenvalue of $T^{-1}$, then $\frac{1}{1/\lambda} = \lambda$ is an eigenvalue of ${(T^{-1})}^{-1} = T$.
        \item If $v$ is an eigenvector of $T$ with respect to the eigenvalue $\lambda$, then $v$ is also an eigenvector of $T^{-1}$ with respect to the eigenvalue $\frac{1}{\lambda}$.

              If $v$ is an eigenvector of $T^{-1}$ with respect to the eigenvalue $\frac{1}{\lambda}$, then $v$ is also an eigenvector of $T$ with respect to the eigenvalue $\lambda$.

              Hence $T$ and $T^{-1}$ have the same eigenvectors.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise22
\begin{exercise}
    Suppose $T \in \lmap{V}$ and there exist nonzero vectors $u$ and $w$ in $V$ such that
    \[
        Tu = 3w \qquad\text{and}\qquad Tw = 3u.
    \]

    Prove that $3$ or $-3$ is an eigenvalue of $T$.
\end{exercise}

\begin{proof}
    $T^{2}u = T(Tu) = T(3w) = 9u$. So $(T^{2} - 9I)(u) = 0$, it follows that
    \[
        (T - 3I)(T + 3I)(u) = 0\qquad (T + 3I)(T - 3I)(u) = 0.
    \]

    Assume that $(T + 3I)(u)\ne 0$ and $(T - 3I)(u)\ne 0$. Then let $v_{1} = (T + 3I)(u)$ and $v_{2} = (T - 3I)(u)$. Therefore $(T - 3I)(v_{1}) = 0$ and $(T + 3I)(v_{2}) = 0$. So $\kernel{(T - 3I)}\ne \{0\}$ and $\kernel{(T + 3I)}\ne \{0\}$, which is a contradiction.

    Hence  $(T + 3I)(u) = 0$ or $(T - 3I)(u) = 0$. Thus $3$ or $-3$ is an eigenvalue of $T$.
\end{proof}
\newpage

% chapter5:sectionA:exercise23
\begin{exercise}
    Suppose $V$ is finite-dimensional and $S, T \in \lmap{V}$. Prove that $ST$ and $TS$ have the same eigenvalues.
\end{exercise}

\begin{proof}
    Assume $\lambda$ is a nonzero eigenvalue of $ST$ and $v$ is an eigenvector of $ST$ corresponding to $\lambda$, then $(ST)(v) = \lambda v$, and $(TS)(Tv) = \lambda Tv$. Because $\lambda$ and $v$ are nonzero, $S(Tv) = \lambda v$ is nonzero, so $Tv\ne 0$. Hence $\lambda$ is an eigenvalue of $TS$.

    Assume $\lambda$ is a nonzero eigenvalue of $TS$ and $v$ is an eigenvector of $TS$ corresponding to $\lambda$, then $(TS)(v) = \lambda v$, and $(ST)(Sv) = \lambda Sv$. Because $\lambda$ and $v$ are nonzero, $T(Sv) = \lambda v$ is nonzero, so $Sv\ne 0$. Hence $\lambda$ is an eigenvalue of $ST$.

    According to Exercise~\ref{chapter3:sectionD:exercise11}, $ST$ is invertible iff $S, T$ are invertible, and $TS$ is invertible iff $S, T$ are invertible. Therefore $ST$ is not invertible iff $TS$ is not invertible. Equivalently, $(ST - 0I)$ is not invertible iff $(TS - 0I)$ is not invertible. So $0$ is an eigenvalue of $ST$ iff $0$ is an eigenvalue of $TS$.

    Hence $ST$ and $TS$ have the same eigenvalues.
\end{proof}
\newpage

% chapter5:sectionA:exercise24
\begin{exercise}
    Suppose $A$ is an $n$-by-$n$ matrix with entries in $\mathbb{F}$. Define $T\in\lmap{\mathbb{F}^{n}}$ by $Tx = Ax$, where elements of $\mathbb{F}^{n}$ are thought of as $n$-by-$1$ column vectors.
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose the sum of the entries in each row of $A$ equals $1$. Prove that $1$ is an eigenvalue of $T$.
        \item Suppose the sum of the entries in each column of $A$ equals $1$. Prove that $1$ is an eigenvalue of $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \[
                  \begin{pmatrix}
                      A_{1,1} & \cdots & A_{1,n} \\
                      \vdots  & \ddots & \vdots  \\
                      A_{n,1} & \cdots & A_{n,n}
                  \end{pmatrix}
                  \begin{pmatrix}
                      1      \\
                      \vdots \\
                      1
                  \end{pmatrix}
                  = \begin{pmatrix}
                      A_{1,1} + \cdots + A_{1,n} \\
                      \vdots                     \\
                      A_{n,1} + \cdots + A_{n,n}
                  \end{pmatrix}
                  = \begin{pmatrix}
                      1      \\
                      \vdots \\
                      1
                  \end{pmatrix}
              \]

              So $1$ is an eigenvalue of $T$.
        \item Let $T'$ be the dual map of $T$, then $T'x = A^{T}x$. The sum of the entries in each column of $A$ equals $1$ so the sum of entries in each row of $A^{T}$ equals $1$. Due to (a), it follows that $1$ is an eigenvalue of $T'$. According to Exercise~\ref{chapter5:sectionA:exercise15}, $1$ is also an eigenvalue of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise25
\begin{exercise}\label{chapter5:sectionA:exercise25}
    Suppose $T \in \lmap{V}$ and $u, w$ are eigenvectors of $T$ such that $u + w$ is also an eigenvector of $T$. Prove that $u$ and $w$ are eigenvectors of $T$ corresponding to the same eigenvalue.
\end{exercise}

\begin{proof}
    Assume that $u$ and $w$ are eigenvectors of $T$ corresponding to eigenvalues $\lambda_{1}$ and $\lambda_{2}$, respectively.

    Because $u + w$ is also an eigenvector of $T$, there exists $\lambda\in\mathbb{F}$ such that $T(u + w) = \lambda(u + w)$. So
    \[
        \lambda(u + w) = T(u + w) = Tu + Tw = \lambda_{1}u + \lambda_{2}w.
    \]

    Therefore $(\lambda - \lambda_{1})u + (\lambda - \lambda_{2})w = 0$.

    If $\lambda - \lambda_{1}$ and $\lambda - \lambda_{2}$ are both zero, then $u$ and $w$ correspond to the same eigenvalue.

    If $\lambda - \lambda_{1}$ and $\lambda - \lambda_{2}$ are not both zero, then $u$ and $w$ are linearly dependent, which means at least one vector is a scalar multiple of the other. Therefore they correspond to the same eigenvalue.

    Hence $u$ and $w$ are eigenvectors of $T$ corresponding to the same eigenvalue.
\end{proof}
\newpage

% chapter5:sectionA:exercise26
\begin{exercise}\label{chapter5:sectionA:exercise26}
    Suppose $T \in \lmap{V}$ is such that every nonzero vector in $V$ is an eigenvector of $T$. Prove that $T$ is a scalar multiple of the identity operator.
\end{exercise}

\begin{proof}
    According to Exercise~\ref{chapter5:sectionA:exercise25}, for every nonzero vector $v$, there exists $\lambda\in\mathbb{F}$ such that $Tv = \lambda v$. Of course, $T0 = \lambda 0$.

    Hence $T$ is a scalar multiple of the identity operator.
\end{proof}
\newpage

% chapter5:sectionA:exercise27
\begin{exercise}
    Suppose that $V$ is finite-dimensional and $k \in \{1, \ldots, \dim V - 1\}$. Suppose $T \in \lmap{V}$ is such that every subspace of $V$ of dimension $k$ is invariant under $T$. Prove that $T$ is a scalar multiple of the identity operator.
\end{exercise}

\begin{proof}
    Let $n = \dim V$.

    The statement is true for $k = 1$, which is Exercise~\ref{chapter5:sectionA:exercise26}.

    Assume that the statement is true for $k = p$, where $1\leq p < \dim V - 1$.

    Let $U$ be a subspace of $V$ of dimension $p$. Let $v_{1}, \ldots, v_{p}$ be a basis of $U$. Extend this list to create a basis of $V$ and let it be
    \[
        v_{1}, \ldots, v_{p}, \ldots, v_{n}
    \]

    The subspaces $\operatorname{span}(v_{1}, \ldots, v_{p}, v_{p+1})$ and $\operatorname{span}(v_{1}, \ldots, v_{p}, v_{p+2})$ are invariant under $T$. On the other hand
    \[
        \operatorname{span}(v_{1}, \ldots, v_{p}, v_{p+1})\cap \operatorname{span}(v_{1}, \ldots, v_{p}, v_{p+2}) = \operatorname{span}(v_{1}, \ldots, v_{p}) = U.
    \]

    If $u\in U$, then $Tu$ is in $\operatorname{span}(v_{1}, \ldots, v_{p}, v_{p+1})$ and $\operatorname{span}(v_{1}, \ldots, v_{p}, v_{p+2})$ so $Tu\in U$, which means $U$ is invariant under $T$. According to the induction hypothesis, $T$ is a scalar multiple of the identity operator.

    Thus, due to the principle of mathematical induction, if every subspace of $V$ of dimension $k$ (where $1\leq k < \dim V - 1$) is invariant under $T$, then $T$ is a scalar multiple of the identity operator.
\end{proof}
\newpage

% chapter5:sectionA:exercise28
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in \lmap{V}$. Prove that $T$ has at most $1 + \dim\range{T}$ distinct eigenvalues.
\end{exercise}

\begin{proof}
    Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$. Let $v_{1}, \ldots, v_{m}$ be eigenvectors corresponding to $\lambda_{1}, \ldots, \lambda_{m}$. Then $v_{1}, \ldots, v_{m}$ is linearly independent.

    If $\lambda_{1}, \ldots, \lambda_{m}$ are nonzero, then
    \begin{align*}
        m & = \dim\operatorname{span}(v_{1}, \ldots, v_{m})                       \\
          & = \dim\operatorname{span}(\lambda_{1}v_{1}, \ldots, \lambda_{m}v_{m}) \\
          & = \dim\operatorname{span}(Tv_{1}, \ldots, Tv_{m})                     \\
          & \leq \dim\range{T} < 1 + \dim\range{T}.
    \end{align*}

    If there is one eigenvalue equals $0$, I assume it is $\lambda_{1}$, then
    \begin{align*}
        m & = \dim\operatorname{span}(v_{1}, \ldots, v_{m})                           \\
          & = 1 + \dim\operatorname{span}(v_{2}, \ldots, v_{m})                       \\
          & = 1 + \dim\operatorname{span}(\lambda_{2}v_{2}, \ldots, \lambda_{m}v_{m}) \\
          & = 1 + \dim\operatorname{span}(Tv_{2}, \ldots, Tv_{m})                     \\
          & \leq 1 + \dim\range{T}.
    \end{align*}

    Thus $T$ has at most $1 + \dim\range{T}$ distinct eigenvalues.
\end{proof}
\newpage

% chapter5:sectionA:exercise29
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{R}^{3}}$ and $-4$, $5$ and $\sqrt{7}$ are eigenvalues of $T$. Prove that there exists $x\in\mathbb{R}^{3}$ such that $Tx - 9x = (-4, 5, \sqrt{7})$.
\end{exercise}

\begin{proof}
    There exist nonzero vectors $v_{1}, v_{2}, v_{3}$ which are eigenvectors of $T$ correspond to  $-4$, $5$ and $\sqrt{7}$, respectively. Moreover, $v_{1}, v_{2}, v_{3}$ is linearly independent, so $v_{1}, v_{2}, v_{3}$ is a basis of $\mathbb{R}^{3}$. So there exists $b_{1}, b_{2}, b_{3}\in\mathbb{R}$ such that
    \[
        b_{1}v_{1} + b_{2}v_{2} + b_{3}v_{3} = (-4, 5, \sqrt{7}).
    \]

    $Tv_{1} = -4v_{1}$, $Tv_{2} = 5v_{2}$, $Tv_{3} = \sqrt{7}v_{3}$. Let $x = a_{1}v_{1} + a_{2}v_{2} + a_{3}v_{3}$ where $a_{1}, a_{2}, a_{3}\in\mathbb{R}$.
    \begin{align*}
        Tx - 9x & = (-4a_{1})v_{1} + (5a_{2})v_{2} + (\sqrt{7}a_{3})v_{3} - (9a_{1})v_{1} - (9a_{2})v_{2} - (9a_{3})v_{3} \\
                & = (-13a_{1})v_{1} + (-4a_{2})v_{2} + (\sqrt{7} - 9)a_{3}v_{3}
    \end{align*}

    Let $a_{1} = \frac{b_{1}}{-13}$, $a_{2} = \frac{b_{2}}{-4}$, $a_{3} = \frac{b_{3}}{\sqrt{7} - 9}$, then $Tx - 9x = (-4, 5, \sqrt{7})$.
\end{proof}
\newpage

% chapter5:sectionA:exercise30
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $(T - 2I)(T - 3I)(T - 4I) = 0$. Suppose $\lambda$ is an eigenvalue of $T$. Prove that $\lambda = 2$ or $\lambda = 3$ or $\lambda = 4$.
\end{exercise}

\begin{proof}
    Let $v$ be an eigenvector of $T$ corresponding to $\lambda$.
    \begin{align*}
        (T - 2I)(T - 3I)(T - 4I)(v) & = (T - 2I)(T - 3I)(Tv - 4v)                                          \\
                                    & = (T - 2I)(T - 3I)((\lambda - 4)v)                                   \\
                                    & = (T - 2I)((\lambda - 4)\lambda v - 3(\lambda - 4)v)                 \\
                                    & = (T - 2I)((\lambda - 3)(\lambda - 4)v)                              \\
                                    & = \lambda (\lambda - 3)(\lambda - 4)v - 2(\lambda - 3)(\lambda - 4)v \\
                                    & = (\lambda - 2)(\lambda - 3)(\lambda - 4)v.
    \end{align*}

    Because $(T - 2I)(T - 3I)(T - 4I) = 0$ and $v$ is nonzero, it follows that
    \[
        (\lambda - 2)(\lambda - 3)(\lambda - 4) = 0.
    \]

    So $\lambda = 2$ or $\lambda = 3$ or $\lambda = 4$.
\end{proof}
\newpage

% chapter5:sectionA:exercise31
\begin{exercise}
    Given an example of $T\in\lmap{\mathbb{R}^{2}}$ such that $T^{4} = -I$.
\end{exercise}

\begin{proof}
    Let $T(x_{1}, x_{2}) = \left(\frac{x_{1}}{\sqrt{2}} + \frac{x_{2}}{\sqrt{2}}, \frac{-x_{1}}{\sqrt{2}} + \frac{x_{2}}{\sqrt{2}}\right)$.
    \begin{align*}
        T^{2}(x_{1}, x_{2}) & = (x_{2}, -x_{1}),                         \\
        T^{4}(x_{1}, x_{2}) & = T^{2}(x_{2}, -x_{1}) = (-x_{1}, -x_{2}).
    \end{align*}

    So $T^{4} = -I$.
\end{proof}
\newpage

% chapter5:sectionA:exercise32
\begin{exercise}
    Suppose $T\in\lmap{V}$ has no eigenvalues and $T^{4} = I$. Prove that $T^{2} = -I$.
\end{exercise}

\begin{proof}
    $T^{4} - I = 0$ implies $(T^{2} - I)(T^{2} + I) = 0$. Then either $T^{2} = I$ or $T^{2} = -I$. If $T^{2} = I$, then $(T - I)(T + I) = 0$, which implies $1$ or $-1$ is an eigenvalue of $T$. However, since $T$ has no eigenvalues, $T^{2}\ne I$. Hence $T^{2} = -I$.
\end{proof}
\newpage

% chapter5:sectionA:exercise33
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $m$ is a positive integer.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $T$ is injective if and only if $T^{m}$ is injective.
        \item Prove that $T$ is surjective if and only if $T^{m}$ is surjective.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I prove these using mathematical induction and the results: $ST$ is injective implies $T$ is injective; $ST$ is surjective implies $S$ is surjective; composition of two injections is an injection; composition of two surjections is a surjection.

    \begin{enumerate}[label={(\alph*)}]
        \item When $m = 1$, the statement is true.

              Assume when $m = n$, the statement is true.

              If $T^{n+1}$ is injective, then $T^{n}T$ is injective, so $T$ is injective.

              If $T$ is injective, then $T^{n}$ is injective (induction hypothesis). So $T^{n+1} = T^{n}T$ is injective.

              Thus, according to the principle of mathematical induction, for every positive integer $m$, $T$ is injective if and only if $T^{m}$ is injective.
        \item When $m = 1$, the statement is true.

              Assume when $m = n$, the statement is true.

              If $T^{n+1}$ is surjective, then $TT^{n}$ is surjective, so $T$ is surjective.

              If $T$ is surjective, then $T^{n}$ is surjective (induction hypothesis). So $T^{n+1} = T^{n}T$ is surjective.

              Thus, according to the principle of mathematical induction, for every positive integer $m$, $T$ is surjective if and only if $T^{m}$ is surjective.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise34
\begin{exercise}
    Suppose $V$ is finite-dimensional and $v_{1}, \ldots, v_{m} \in V$. Prove that the list $v_{1} , \ldots, v_{m}$ is linearly independent if and only if there exists $T \in \lmap{V}$ such that $v_{1} , \ldots, v_{m}$ are eigenvectors of $T$ corresponding to distinct eigenvalues.
\end{exercise}

In this exercise, $\mathbb{F}$ must have infinite elements.

\begin{proof}
    If there exists a linear map $T\in\lmap{V}$ such that $v_{1} , \ldots, v_{m}$ are eigenvectors of $T$ corresponding to distinct eigenvalues, then these vectors are linearly independent.

    If the list $v_{1} , \ldots, v_{m}$ is linearly independent, we can add vectors to this list to obtain a basis of $V$, and let such a basis be
    \[
        v_{1} , \ldots, v_{m}, v_{m+1}, \ldots, v_{m+n}
    \]

    Let $\lambda_{1}, \ldots, \lambda_{m}$ be distinct elements of $\mathbb{F}$. I define the linear map $T\in\lmap{V}$ as follows: $Tv_{i} = \lambda_{i}v_{i}$ for $i = 1, \ldots, m$ and $Tv_{i} = 0$ for $i > m$. So $v_{1} , \ldots, v_{m}$ are eigenvectors of $T$ corresponding to distinct eigenvalues $\lambda_{1}, \ldots, \lambda_{m}$.
\end{proof}
\newpage

% chapter5:sectionA:exercise35
\begin{exercise}
    Suppose that $\lambda_{1}, \ldots, \lambda_{n}$ is a list of distinct real numbers. Prove that the list $e^{\lambda_{1}x} , \ldots, e^{\lambda_{n}x}$ is linearly independent in the vector space of real-valued functions on $\mathbb{R}$.
\end{exercise}

\begin{proof}
    Let $D$ be the linear operator on the vector space of differentiable real-valued functions on $\mathbb{R}$ defined by $Df = f'$. Because $De^{\lambda_{i}x} = \lambda_{i}e^{\lambda_{i}x}$ so $e^{\lambda_{i}x}$ is an eigenvector of $D$ corresponding to the eigenvalue $\lambda_{i}$ for $i = 1, \ldots, n$.

    Because $\lambda_{1}, \ldots, \lambda_{n}$ are pairwise distinct, it follows that $e^{\lambda_{1}x} , \ldots, e^{\lambda_{n}x}$ is linearly independent in the vector space of differentiable real-valued functions on $\mathbb{R}$, and also in the vector space of real-valued functions on $\mathbb{R}$.
\end{proof}
\newpage

% chapter5:sectionA:exercise36
\begin{exercise}
    Suppose that $\lambda_{1}, \ldots, \lambda_{n}$ is a list of distinct positive numbers. Prove that the list $\cos{(\lambda_{1}x)} , \ldots, \cos{(\lambda_{n}x)}$ is linearly independent in the vector space of real-valued functions on $\mathbb{R}$.
\end{exercise}

\begin{proof}
    Let $D$ be the linear operator on the vector space of twice differentiable real-valued functions on $\mathbb{R}$ defined by $Df = f'$. Because $D^{2}(\cos(\lambda_{i}x)) = -\lambda_{i}^{2}\cos(\lambda_{i}x)$, $\cos(\lambda_{i}x)$ is an eigenvalue of $D^{2}$ corresponding to the eigenvalue $\lambda_{i}^{2}$ for $i = 1, \ldots, n$.

    Because $\lambda_{1}^{2}, \ldots, \lambda_{n}^{2}$ are pairwise distinct, it follows that $\cos{(\lambda_{1}x)} , \ldots, \cos{(\lambda_{n}x)}$ is linearly independent in the vector space of twice differentiable real-valued functions on $\mathbb{R}$, and also in the vector space of real-valued functions on $\mathbb{R}$.
\end{proof}
\newpage

% chapter5:sectionA:exercise37
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Define $A \in \lmap{\lmap{V}}$ by
    \[
        \mathcal{A}(S) = TS
    \]

    for each $S\in\lmap{V}$. Prove that the set of eigenvalues of $T$ equals the set of eigenvalues of $\mathcal{A}$.
\end{exercise}

\begin{proof}
    Let $\lambda$ be an eigenvalue of $T$ and $v$ be a corresponding eigenvector. Let $v_{1}, \ldots, v_{n}$ be a basis of $V$. I define the linear map $S\in\lmap{V}$ as follows: $Sv_{i} = v$ for $i = 1, \ldots, n$. Then
    \[
        (TS)(v_{i}) = Tv = \lambda v = \lambda Sv_{i}
    \]

    for $i = 1, \ldots, n$. Hence $TS = \lambda S$, which means $\lambda$ is an eigenvalue of $\mathcal{A}$.

    \bigskip
    Let $\lambda$ be an eigenvalue of $\mathcal{A}$ and $S$ be a corresponding eigenvector. Then $(TS)(v) = \lambda Sv$ for all $v\in V$. Therefore $T(Sv) = \lambda Sv$ for all $v\in V$. Because $S\ne 0$, then there exists $v_{0}\in V$ such that $Sv_{0}\ne 0$. So $T(Sv_{0}) = \lambda Sv_{0}$ and we conclude that $\lambda$ is an eigenvalue of $T$.

    Thus the set of eigenvalues of $T$ equals the set of eigenvalues of $\mathcal{A}$.
\end{proof}
\newpage

% chapter5:sectionA:exercise38
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $U$ is a subspace of $V$ invariant under $T$. The \textit{quotient operator} $T/U\in\lmap{V/U}$ is defined by
    \[
        (T/U)(v + U) = Tv + U
    \]

    for each $v\in V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that the definition of $T/U$ makes sense (which requires using the
              condition that $U$ is invariant under $T$) and show that $T/U$ is an operator
              on $V/U$.
        \item Show that each eigenvalue of $T/U$ is an eigenvalue of $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Assume $v + U = w + U$, then $v - w\in U$. Because $U$ is invariant under $T$, $Tv - Tw\in U$, so $Tv + U = Tw + U$. So
              \[
                  (T/U)(v + U) = Tv + U = Tw + U = (T/U)(w + U).
              \]

              Hence the definition of $T/U$ make sense ($T/U$ is well-defined).

              Moreover, $T/U$ is a linear map. So $T/U$ is an operator on $V/U$.
        \item Assume $\lambda$ is an eigenvalue of $T/U$ and $v + U$ be a corresponding eigenvector. According to the definition of eigenvector, $v\notin U$. Then $(T/U)(v + U) = Tv + U = \lambda v + U$, which implies $Tv - \lambda v \in U$.

              Let $Tv - \lambda v = u$. For $u_{1}\in U$,
              \[
                  T(v + u_{1}) = Tv + Tu_{1} = \lambda v + u + Tu_{1}.
              \]

              Because $v\notin U$ and $u_{1}\in U$, it follows that $v + u_{1}\notin U$, so $v + u_{1}$ is nonzero.

              If the restriction on $U$ of $T - \lambda I$ is not invertible, then $T - \lambda I$ is also not invertible. So $\lambda$ is an eigenvalue of $T$.

              If the restriction on $U$ of $T - \lambda I$ is invertible, then there exists $u_{1}$ such that $-u = (T - \lambda I)(u_{1})$. Therefore $T(v + u_{1}) = \lambda v + \lambda u_{1} = \lambda (v + u_{1})$. So $\lambda$ is an eigenvalue of $T$.

              Hence each eigenvalue of $T/U$ is an eigenvalue of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise39
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that $T$ has an eigenvalue if and only if there exists a subspace of $V$ of dimension $\dim V - 1$ that is invariant under $T$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $T$ has an eigenvalue $\lambda$.

    If $\lambda  = 0$. Let $Tv_{1}, \ldots, Tv_{m}$ be a basis of $\range{T}$, then $v_{1}, \ldots, v_{m}$ is linearly independent. Let $v$ be a vector in $V$, then there exist $a_{1}, \ldots, a_{m}\in\mathbb{F}$ such that
    \[
        Tv = a_{1}Tv_{1} + \cdots + a_{m}Tv_{m} = T(a_{1}v_{1} + \cdots + a_{m}v_{m}).
    \]

    So $v - (a_{1}v_{1} + \cdots + a_{m}v_{m})$ is in $\kernel{T}$. Let $u_{1}, \ldots, u_{p}$ be a basis of $\kernel{T}$.
    \[
        \kernel{T}\cap\operatorname{span}(v_{1}, \ldots, v_{m}) = \{ 0 \}.
    \]

    Because $\lambda = 0$, it follows that $\kernel{T}\ne \{ 0 \}$. So $p\geq 1$. The subspace
    \[
        \operatorname{span}(v_{1}, \ldots, v_{m})\oplus\operatorname{span}(u_{1}, \ldots, u_{p-1})
    \]

    is an invariant subspace of dimension $\dim V - 1$ under $T$.

    If $\lambda\ne 0$. Let $v_{1}$ be an eigenvector of $T$ corresponding to $\lambda$ and let $v_{1}, \ldots, v_{n}$ be a basis of $V$. Then the matrix of $T$ with respect to this basis is
    \[
        \begin{pmatrix}
            \lambda & A_{1,2} & \cdots & A_{1,n} \\
            0       & A_{2,2} & \cdots & A_{2,n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & A_{n,2} & \cdots & A_{n,n}
        \end{pmatrix}.
    \]

    I choose $w_{1} = v_{1}$, $w_{k} = v_{k} - \lambda^{-1}A_{1,k}v_{1}$ for $1 < k\leq n$. Then $w_{1}, \ldots, w_{n}$ is a basis of $V$ and the matrix of $T$ with respect to this basis is
    \[
        \begin{pmatrix}
            \lambda & 0       & \cdots & 0       \\
            0       & A_{2,2} & \cdots & A_{2,n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & A_{n,2} & \cdots & A_{n,n}
        \end{pmatrix}.
    \]

    Then $\operatorname{span}(w_{2}, \ldots, w_{n})$ is a subspace of $V$ of dimension $\dim V - 1$, which is invariant under $T$.

    $(\Leftarrow)$ There exists a subspace of $V$ of dimension $\dim V - 1$ that is invariant under $T$.

    Let $n = \dim V$. Let $U$ be such subspace and $v_{1}, \ldots, v_{n-1}$ be a basis of $U$. This list can be extended to become a basis of $V$, let such a basis be $v_{1}, \ldots, v_{n}$. Let $A$ be the matrix of $T$ with respect to this basis, then
    \[
        A = \begin{pmatrix}
            A_{1,1}   & \cdots & A_{1,n-1}   & A_{1,n}   \\
            \vdots    &        & \vdots      & \vdots    \\
            A_{n-1,1} & \cdots & A_{n-1,n-1} & A_{n-1,n} \\
            0         & \cdots & 0           & A_{n,n}
        \end{pmatrix}.
    \]

    $A - A_{n,n}I$ is the matrix of $T - A_{n,n}I$ with respect to $v_{1}, \ldots, v_{n}$. $A - A_{n,n}I$ is not invertible because its $n$th row is zero, hence $T - A_{n,n}I$ is not invertible. Therefore $A_{n,n}$ is an eigenvalue of $T$. So $T$ has an eigenvalue.
\end{proof}
\newpage

% chapter5:sectionA:exercise40
\begin{exercise}\label{chapter5:sectionA:exercise40}
    Suppose $S,T\in\lmap{V}$ and $S$ is invertible. Suppose $p\in\mathscr{P}(\mathbb{F})$ is a polynomial. Prove that
    \[
        p(STS^{-1}) = Sp(T)S^{-1}.
    \]
\end{exercise}

\begin{proof}
    Let $p(x) = a_{0} + a_{1}x + \cdots + a_{n}x^{n}$. We have
    \[
        {(STS^{-1})}^{k} = ST^{k}S^{-1}
    \]

    for every nonnegative integer $k$, so
    \begin{align*}
        p(STS^{-1}) & = a_{0}(SIS^{-1}) + a_{1}(STS^{-1}) + \cdots + a_{n}(ST^{n}S^{-1}) \\
                    & = S(a_{0}I + a_{1}T + \cdots + a_{n}T^{n})S^{-1}                   \\
                    & = Sp(T)S^{-1}.\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter5:sectionA:exercise41
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $U$ is a subspace of $V$ invariant under $T$. Prove that $U$ is invariant under $p(T)$ for every polynomial $p\in\mathscr{P}(\mathbb{F})$.
\end{exercise}

\begin{proof}
    Because $U$ is invariant under $T$ and $U$ is invariant under $I$, then $U$ is invariant under $T^{m}$ for every nonnegative integer $m$. Therefore $U$ is invariant under $p(T)$ for every polynomial $p\in\mathscr{P}(\mathbb{F})$.
\end{proof}
\newpage

% chapter5:sectionA:exercise42
\begin{exercise}\label{chapter5:sectionA:exercise42}
    Define $T\in\lmap{\mathbb{F}^{n}}$ by $T(x_{1}, x_{2}, x_{3}, \ldots, x_{n}) = (x_{1}, 2x_{2}, 3x_{3}, \ldots, nx_{n})$.
    \begin{enumerate}[label={(\alph*)}]
        \item Find all eigenvalues and eigenvectors of $T$.
        \item Find all subspaces of $\mathbb{F}^{n}$ that are invariant under $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Assume $\lambda$ is an eigenvalue of $T$ and let $(x_{1}, x_{2}, x_{3}, \ldots, x_{n})$ be a corresponding eigenvector. Then
              \[
                  (\lambda x_{1}, \lambda x_{2}, \lambda x_{3}, \ldots, \lambda x_{n}) = (x_{1}, 2x_{2}, 3x_{3}, \ldots, nx_{n})
              \]

              Hence $(\lambda - k)x_{k} = 0$ for $k = 1,\ldots, n$. So $\lambda = 1$, or $\lambda = 2$, \ldots, or $\lambda = n$.

              The eigenvectors corresponding to $\lambda = k$ is a scalar multiple (nonzero) of $e_{k}$ (where $e_{1}, \ldots, e_{n}$ is the standard basis of $\mathbb{F}^{n}$).
        \item Find all subspaces of $\mathbb{F}^{n}$ that are invariant under $T$.

              I will prove the following statement: If $V$ is a subspace of $\mathbb{F}^{n}$ that is invariant under $T$ and $(x_{1}, \ldots, x_{n})\in V$, then $x_{1}e_{1}, \ldots, x_{n}e_{n}\in V$.

              If $(x_{1}, \ldots, x_{n})\in V$ then for every $k = 1, \ldots, n$
              \[
                  (0, \ldots, 0, (k-1)!x_{k}, \ldots, n(n-1)\cdots (n - k + 1)x_{n})\in V.
              \]

              Therefore $(0, \ldots, 0, (n-1)!x_{n})\in V$ and $x_{n}e_{n}\in V$. So $(x_{1}, \ldots, x_{n-1}, 0)\in V$. Similarly, $x_{k}e_{k}\in V$ for $k = n-1, \ldots, 1$.

              Let $i_{1}, \ldots, i_{k}$ be the positive integers in $\{ 1, \ldots, n \}$ such that there exists an element in $V$ where the $i_{j}$th slot is nonzero. According to the statement that we have just proved, $V$ is the span of $e_{i_{1}}$, \ldots, $e_{i_{k}}$.

              Hence all subspaces of $\mathbb{F}^{n}$ that are invariant under $T$ are spans of vectors in the list $e_{1}, \ldots, e_{n}$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionA:exercise43
\begin{exercise}
    Suppose that $V$ is finite-dimensional, $\dim V > 1$, and $T\in\lmap{V}$. Prove that $\{ p(T): p\in\mathscr{P}(\mathbb{F}) \}\ne \lmap{V}$.
\end{exercise}

\begin{proof}
    This result follows Exercise~\ref{chapter5:sectionB:exercise19}.
    \bigskip

    Or using the result: $T$ commutes with all operators on $V$ if and only if $T$ is a scalar multiple of the identity operator.

    If $T$ is a scalar multiple of the identity operator then $\{ p(T): p\in\mathscr{P}(\mathbb{F}) \}$ consists of operators which are scalar multiple of the identity operator, so it is not $\lmap{V}$.

    If $T$ is not a scalar multiple of the identity operator, then there exists an operator $S\in\lmap{V}$ such that $ST\ne TS$ (this is true for $\dim V > 1$). Therefore $S$ is not in $\{ p(T): p\in\mathscr{P}(\mathbb{F}) \}$. Therefore the set is not $\lmap{V}$.

    Hence $\{ p(T): p\in\mathscr{P}(\mathbb{F}) \}\ne \lmap{V}$.
\end{proof}
\newpage

\section{The Minimal Polynomial}

% chapter5:sectionB:exercise1
\begin{exercise}
    Suppose $T\in \lmap{V}$. Prove that $9$ is an eigenvalue of $T^{2}$ if and only if $3$ or $-3$ is an eigenvalue of $T$.
\end{exercise}

\begin{proof}
    If $9$ is an eigenvalue of $T^{2}$ then there exists a nonzero vector $v$ such that $(T^{2} - 9I)(v) = 0$, so $(T - 3I)(T + 3I)(v) = 0$. Then $3$ or $-3$ is an eigenvalue of $T$ (otherwise, $(T - 3I)(T + 3I)(v) \ne 0$).

    And if $3$ is an eigenvalue of $T$, then there exists a nonzero vector $v_{1}$ such that $(T^{2} - 9I)(v_{1}) = (T + 3I)(T - 3I)(v_{1}) = 0$. If $-3$ is an eigenvalue of $T$, then there exists a nonzero vector $v_{2}$ such that $(T^{2} - 9I)(v_{2}) = (T - 3I)(T + 3I)(v_{2}) = 0$. Hence $9$ is an eigenvalue of $T^{2}$.
\end{proof}
\newpage

% chapter5:sectionB:exercise2
\begin{exercise}
    Suppose $V$ is a complex vector space and $T\in\lmap{V}$ has no eigenvalues. Prove that every subspace of $V$ invariant under $T$ is either $\{0\}$ or infinite-dimensional.
\end{exercise}

\begin{proof}
    Let $U$ be a subspace of $V$ such that $U$ is invariant under $T$.

    If $U$ is finite-dimensional and $\dim U > 0$, then $T\vert_{U}$ has an eigenvalue.

    So if $T$ has no eigenvalues, then $T\vert_{U}$ also has no eigenvalues, therefore either $\dim U = 0$ or $U$ is infinite-dimensional.
\end{proof}
\newpage

% chapter5:sectionB:exercise3
\begin{exercise}
    Suppose $n$ is a positive integer and $T\in\lmap{\mathbb{F}^{n}}$ is defined by
    \[
        T(x_{1}, \ldots, x_{n}) = (x_{1} + \cdots + x_{n}, \ldots, x_{1} + \cdots + x_{n}).
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Find all eigenvalues and eigenvectors of $T$.
        \item Find the minimal polynomial of $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $\lambda$ be an eigenvalue of $T$ and $(x_{1}, \ldots, x_{n})$ be a corresponding eigenvector. Then
              \[
                  (x_{1} + \cdots + x_{n}, \ldots, x_{1} + \cdots + x_{n}) = (\lambda x_{1}, \ldots, \lambda x_{n}).
              \]

              It follows that $\lambda x_{1} = \cdots = \lambda x_{n}$. Then either $\lambda = 0$ or $x_{1} = \cdots = x_{n}$. Hence all eigenvalues of $T$ are $0$ and $n$. The eigenvectors of $T$ corresponding to $0$ are nonzero vectors of the vector space
              \[
                  \{ (x_{1}, \ldots, x_{n}): x_{1} + \cdots + x_{n} = 0 \}.
              \]

              The eigenvectors of $T$ corresponding to $n$ are nonzero vectors of the vector space
              \[
                  \operatorname{span}((1, \ldots, 1)).
              \]
        \item Because $T$ has two different eigenvalues, it follows the the degree of the minimal polynomial of $T$ is at least $2$.
              \[
                  T^{2}(x_{1}, \ldots, x_{n}) = (n(x_{1} + \cdots + x_{n}), \ldots, n(x_{1} + \cdots + x_{n})) = nT(x_{1}, \ldots, x_{n}).
              \]

              So $T^{2} - nT = 0$. Hence $x^{2} - nx$ is the minimal polynomial of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionB:exercise4
\begin{exercise}\label{chapter5:sectionB:exercise4}
    Suppose $\mathbb{F} = \mathbb{C}$, $T\in\lmap{V}$, $p\in\mathscr{P}(\mathbb{C})$, and $\alpha\in\mathbb{C}$. Prove that $\alpha$ is an eigenvalue of $p(T)$ if and only if $\alpha = p(\lambda)$ for some eigenvalue $\lambda$ of $T$.
\end{exercise}

\begin{proof}
    If $\lambda$ is an eigenvalue of $T$ and $v$ is an eigenvector of $T$ corresponding to $\lambda$, then
    \begin{align*}
        (p(T))(v) = (a_{0}I + a_{1}T + \cdots + a_{n}T^{n})(v) & = a_{0}v + a_{1}\lambda v + \cdots + a_{n}\lambda^{n}v  \\
                                                               & = (a_{0} + a_{1}\lambda + \cdots + a_{n}\lambda^{n})(v) \\
                                                               & = p(\lambda)v                                           \\
                                                               & = \alpha v.
    \end{align*}

    Hence $\alpha$ is an eigenvalue of $p(T)$.
    \bigskip

    Let $p$ be a polynomial of degree $n$ and $v$ be an eigenvector of $p(T)$ corresponding to $\alpha$.
    \[
        (p(T) - \alpha I)(v) = 0.
    \]

    According to the fundamental theorem of algebra, $p(z) - \alpha$ can be rewritten in the following form
    \[
        p(z) - \alpha = c(z - z_{1})\cdots (z - z_{n})
    \]

    So $c(T - z_{1}I)\cdots (T - z_{n}I)(v) = 0$. If none of $z_{1}, \ldots, z_{n}$ is an eigenvalue of $T$, then $c(T - z_{1}I)\cdots (T - z_{n}I)(v) \ne 0$. Therefore there is $\lambda$ in the list $z_{1}, \ldots, z_{n}$ such that $\lambda$ is an eigenvalue of $T$, this implies $p(\lambda) - \alpha = 0$. Hence $\alpha = p(\lambda)$ for some eigenvalue $\lambda$ of $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise5
\begin{exercise}
    Give an example of an operator on $\mathbb{R}^{2}$ that shows the result in Exercise 4 does not hold if $\mathbb{C}$ is replaced with $\mathbb{R}$.
\end{exercise}

\begin{proof}
    I define $T\in\lmap{\mathbb{R}^{2}}$ as follows: $T(x, y) = (-y, x)$, this linear operator $T$ has no eigenvalue. However, $T^{2}(x, y) = (-x, -y)$, which means $-1$ is an eigenvalue of $T^{2}$.
\end{proof}
\newpage

% chapter5:sectionB:exercise6
\begin{exercise}
    Suppose $T \in \lmap{\mathbb{F}^{2}}$ is defined by $T(w, z) = (-z, w)$. Find the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    $T^{2}(w, z) = T(-z, w) = (-w, -z)$. $p(x) = x^{2} + 1$ is the minimal polynomial of $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise7
\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Give an example of $S, T\in\lmap{\mathbb{F}^{2}}$ such that the minimal polynomial of $ST$ does not equal the minimal polynomial of $TS$.
        \item Suppose $V$ is finite-dimensional and $S,T \in \lmap{V}$. Prove that if at least one of $S, T$ is invertible, then the minimal polynomial of $ST$ equals the minimal polynomial of $TS$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item I choose $S(x, y) = (0, y)$ and $T(x, y) = (0, x)$.
              \[
                  (ST)(x, y) = S(0, x) = (0, x)\qquad (TS)(x, y) = T(0, y) = (0, 0)
              \]

              The minimal polynomial of $ST$ is $p(x) = x^{2}$, meanwhile the minimal polynomial of $TS$ is $p(x) = x$.
        \item Let $p, q$ be the minimal polynomials of $ST, TS$, respectively.

              If $S$ is invertible, then
              \[
                  \begin{split}
                      0 = p(ST) = S^{-1}p(ST)S = p(S^{-1}STS) = p(TS), \\
                      0 = q(TS) = Sq(TS)S^{-1} = q(STSS^{-1}) = q(ST).
                  \end{split}
              \]

              so $\deg p\geq \deg q$ and $\deg q\geq \deg p$. Due to the uniqueness of the minimal polynomial, we deduce that $p = q$.

              If $T$ is invertible, then
              \[
                  \begin{split}
                      0 = p(ST) = Tp(ST)T^{-1} = p(TSTT^{-1}) = p(TS), \\
                      0 = q(TS) = T^{-1}q(TS)T = q(T^{-1}TST) = q(ST).
                  \end{split}
              \]

              so $\deg p\geq \deg q$ and $\deg q\geq \deg p$. Due to the uniqueness of the minimal polynomial, we deduce that $p = q$.

              Hence if at least one of $S, T$ is invertible, then the minimal polynomial of $ST$ equals the minimal polynomial of $TS$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionB:exercise8
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{R}^{2}}$ is the operator of counterclockwise rotation by $1^\circ$. Find the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    According to the definition of $T$,
    \[
        T(x, y) = (x\cos 1^{\circ} + y \sin 1^{\circ}, -x\sin 1^{\circ} + y\cos 1^{\circ}).
    \]
    \begin{align*}
        T^{2}(x, y) & = (x\cos 2^{\circ} + y\sin 2^{\circ}, -x\sin 2^{\circ} + y\cos 2^{\circ})
    \end{align*}

    So
    \begin{align*}
        T^{2}(1, 0) & = (\cos 2^{\circ}, -\sin 2^{\circ})                          \\
                    & = (2\cos^{2} 1^{\circ} - 1, -2\sin 1^{\circ}\cos 1^{\circ})  \\
                    & = 2\cos 1^{\circ}(\cos 1^{\circ}, -\sin 1^{\circ}) + (-1, 0) \\
                    & = 2\cos 1^{\circ} T(1, 0) - I(1, 0).
    \end{align*}

    The minimal polynomial of $T$ is $p(x) = x^{2} - 2x\cos 1^{\circ} + 1$.
\end{proof}
\newpage

% chapter5:sectionB:exercise9
\begin{exercise}
    Suppose $T\in\lmap{V}$ is such that with respect to some basis of $V$, all entries of the matrix of $T$ are rational numbers. Explain why all coefficients of the minimal polynomial of $T$ are rational numbers.
\end{exercise}

\begin{proof}
    Assume that all entries of the matrix of $T$ with respect to the basis $v_{1}, \ldots, v_{n}$ of $V$ are rational numbers. Let
    \[
        a_{0} + a_{1}z + \cdots + a_{m-1}z^{m-1} + z^{m}
    \]

    be the minimal polynomial of $T$. $x_{0}I + x_{1}T + \cdots + x_{m-1}T^{m-1} + T^{m} = 0$ if and only if $x_{0}v_{k} + x_{1}Tv_{k} + \cdots + x_{m-1}T^{m-1}v_{k} + T^{m}v_{k} = 0$ for all $k = 1,\ldots, m$. $T^{i}v_{k}$ can be rewritten as a linear combination of $v_{1}, \ldots, v_{m}$. So from  $x_{0}v_{k} + x_{1}Tv_{k} + \cdots + x_{m-1}T^{m-1}v_{k} + T^{m}v_{k} = 0$ for all $k = 1,\ldots, m$, we obtain a system of linear equations of $m$ unknowns $x_{0}, x_{1}, \ldots, x_{m-1}$ where all coefficients are rational numbers. The solutions of this system of linear equations are the coefficients of the minimal polynomial of $T$. This solution is also unique due to the uniqueness of minimal polynomial of $T$. The solution also must comprise of rational numbers because all coefficients are rational numbers. Hence $a_{0}, a_{1}, \ldots, a_{m-1}$ are rational numbers.
\end{proof}
\newpage

% chapter5:sectionB:exercise10
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $v\in V$. Prove that
    \[
        \operatorname{span}(v, Tv, \ldots, T^{m}v) = \operatorname{span}(v, Tv, \ldots, T^{\dim V - 1}v)
    \]

    for all integers $m\geq \dim V - 1$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$, then $\deg T\leq \dim V$.

    I give a proof using mathematical induction on $m$.

    When $m = \dim V - 1$, $\operatorname{span}(v, Tv, \ldots, T^{m}v) = \operatorname{span}(v, Tv, \ldots, T^{\dim V - 1}v)$.

    Assume $\operatorname{span}(v, Tv, \ldots, T^{m}v) = \operatorname{span}(v, Tv, \ldots, T^{\dim V - 1}v)$ for every positive integer $m$ such that $\dim V - 1\leq m < n$.

    Let $p(x) = a_{0} + a_{1}x + \cdots + a_{k-1}x^{k-1} + x^{k}$, then $k\leq \dim V$, $k\leq n$, and
    \[
        T^{n}v = -a_{k-1}T^{n-1}v - \cdots - a_{1}T^{n-k+1}v - a_{0}T^{n-k}v.
    \]

    According to the induction hypothesis, $T^{n-1}, \ldots, T^{n-k}$ are in $\operatorname{span}(v, Tv, \ldots, T^{\dim V - 1}v)$. Therefore $T^{n}v$ is also in $\operatorname{span}(v, Tv, \ldots, T^{\dim V - 1}v)$.

    Hence, due to the principle of mathematical induction
    \[
        \operatorname{span}(v, Tv, \ldots, T^{m}v) = \operatorname{span}(v, Tv, \ldots, T^{\dim V - 1}v)
    \]

    for all integers $m\geq \dim V - 1$.
\end{proof}
\newpage

% chapter5:sectionB:exercise11
\begin{exercise}
    Suppose $V$ is a two dimensional vector space, $T\in\lmap{V}$, and the matrix of $T$ with respect to some basis of $V$ is $\begin{pmatrix}a & c \\ b & d\end{pmatrix}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $T^{2} - (a + d)T + (ad - bc)I = 0$.
        \item Show that the minimal polynomial of $T$ equals
              \[
                  \begin{cases}
                      z - a                        & \text{if $b = c = 0$ and $a = d$}, \\
                      z^{2} - (a + d)z + (ad - bc) & \text{otherwise}
                  \end{cases}
              \]
    \end{enumerate}
\end{exercise}

\begin{proof}
    Let $v_{1}, v_{2}$ be the basis such that the matrix of $T$ with respect to this basis is $\begin{pmatrix}a & c \\ b & d\end{pmatrix}$.

    \begin{align*}
        T^{2}(x_{1}v_{1} + x_{2}v_{2}) & = T(x_{1}av_{1} + x_{1}bv_{2} + x_{2}cv_{1} + x_{2}dv_{2})                                                     \\
                                       & = (a + d)T(x_{1}v_{1} + x_{2}v_{2}) + T(-x_{1}dv_{1} + x_{1}bv_{2} + x_{2}cv_{1} - x_{2}av_{2})                \\
                                       & = (a + d)T(x_{1}v_{1} + x_{2}v_{2}) + (-x_{1}d + x_{2}c)(av_{1} + bv_{2}) + (x_{1}b - x_{2}a)(cv_{1} + dv_{2}) \\
                                       & = (a + d)T(x_{1}v_{1} + x_{2}v_{2}) + (bc - ad)(x_{1}v_{1} + x_{2}v_{2}).
    \end{align*}

    Hence $T^{2} - (a + d)T + (ad - bc)I = 0$.

    If $b = c = 0$ and $a = d$, then $z - a$ is the minimal polynomial of $T$.

    Otherwise, if the degree of the minimal polynomial of $T$ is $1$, then there exists $\lambda\in\mathbb{F}$ such that $T = \lambda I$. So $Tv_{1} = \lambda v_{1} = av_{1} + bv_{2}$ and $Tv_{2} = \lambda v_{2} = cv_{1} + dv_{2}$. Therefore $a = d = \lambda$ and $b = c = 0$. So the degree of the minimal polynomial of $T$ must be $2$. Therefore the minimal polynomial of $T$ is $z^{2} - (a + d)z + (ad - bc)$.
\end{proof}
\newpage

% chapter5:sectionB:exercise12
\begin{exercise}
    Define $T\in\lmap{\mathbb{F}^{n}}$ by $T(x_{1}, x_{2}, x_{3}, \ldots, x_{n}) = (x_{1}, 2x_{2}, 3x_{3}, \ldots, nx_{n})$. Find the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    According to Exercise~\ref{chapter5:sectionA:exercise42}, $T$ has $n$ distinct eigenvalues, which are $1, \ldots, n$.

    On the other hand, every eigenvalue of $T$ is a root of the minimal polynomial of $T$. Moreover, the degree of the minimal polynomial of $T$ does not exceed $n$. Therefore the minimal polynomial of $T$ is $(z - 1)(z - 2)\cdots (z - n)$.
\end{proof}
\newpage

% chapter5:sectionB:exercise13
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $p\in\mathscr{P}(\mathbb{F})$. Prove that there exists a unique $r\in \mathscr{P}(\mathbb{F})$ such that $p(T) = r(T)$ and $\deg r$ is less than the degree of the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    Let $q$ be the minimal polynomial of $T$. By the Euclid division algorithm, there exist unique polynomials $s$, $r$ such that $p = sq + r$ and $\deg r < \deg q$. Therefore $p(T) = s(T)q(T) + r(T) = r(T)$.

    If $r$ and $r'$ are polynomials such that $p(T) = r(T)$, $p(T) = r'(T)$ and $\deg r, \deg r' < \deg q$, then $(r - r')(T) = 0$. However, $\deg (r - r') < \deg q$, so $r - r' = 0$ (because $q$ is the minimal polynomial of $T$), which means $r = r'$.

    Hence there exists a unique $r\in \mathscr{P}(\mathbb{F})$ such that $p(T) = r(T)$ and $\deg r$ is less than the degree of the minimal polynomial of $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise14
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in\lmap{V}$ has minimal polynomial $4 + 5z - 6z^{2} - 7z^{3} + 2z^{4} + z^{5}$. Find the minimal polynomial of $T^{-1}$.
\end{exercise}

\begin{proof}
    Because the constant term of the minimal polynomial is nonzero, it follows that $T$ is invertible. Let $S = T^{-1}$. Let $a_{0} + a_{1}z + \cdots + z^{n}$ be the minimal polynomial of $T^{-1}$.
    \[
        4I + 5T - 6T^{2} - 7T^{3} + 2T^{4} + T^{5} = 0
    \]

    so
    \[
        0 = T^{-5}(4I + 5T - 6T^{2} - 7T^{3} + 2T^{4} + T^{5}) = 4S^{5} + 5S^{4} - 6S^{3} - 7S^{2} + 2S + I.
    \]

    Hence the minimal polynomial of $T^{-1}$ is a divisor of $1 + 2z - 7z^{2} - 6z^{3} + 5z^{4} + 4z^{5}$, and $n\leq 5$.
    \[
        a_{0}I + a_{1}S+ \cdots + a_{n-1}S^{n-1} + S^{n} = 0.
    \]

    So
    \[
        a_{0}T^{n} + a_{1}T^{n-1} + \cdots + a_{n-1}T + I = 0.
    \]

    It follows that $n\geq 5$. Hence $n = 5$.

    Thus the minimal polynomial of $T^{-1}$ is
    \[
        \frac{1}{4} + \frac{1}{2}z - \frac{7}{4}z^{2} - \frac{3}{2}z^{3} + \frac{5}{4}z^{4} + z^{5}.\qedhere
    \]
\end{proof}
\newpage

% chapter5:sectionB:exercise15
\begin{exercise}
    Suppose $V$ is a finite-dimensional complex vector space with $\dim V > 0$ and $T\in\lmap{V}$. Define $f: \mathbb{C}\to \mathbb{R}$ by
    \[
        f(\lambda) = \dim\range{(T - \lambda I)}.
    \]

    Prove that $f$ is not a continuous function.
\end{exercise}

\begin{proof}
    Because $\dim V > 0$ and $V$ is a complex vector space, then $T$ has an eigenvalue $\alpha$.

    According to Exercise~\ref{chapter5:sectionA:exercise11}, there exists a positive real number $\delta$ such that for all $\lambda$, $0 < \abs{\alpha - \lambda} < \delta$ implies $(T - \lambda I)$ is invertible. Hence
    \begin{align*}
        \lim\limits_{\lambda\to \alpha} f(\lambda) & = \lim\limits_{\lambda\to \alpha} \dim\range{(T - \lambda I)} \\
                                                   & = \dim V                                                      \\
                                                   & \ne \dim\range{(T - \alpha I)} = f(\alpha).
    \end{align*}

    Thus $f$ is not a continuous function. Moreover, all discontinuous points of $f$ are eigenvalues of $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise16
\begin{exercise}
    Suppose $a_{0}, \ldots, a_{n-1}\in\mathbb{F}$. Let $T$ be the operator on $\mathbb{F}^{n}$ whose matrix (with respect to the standard basis) is
    \[
        \begin{pmatrix}
            0 &   &        &        &   & -a_{0}   \\
            1 & 0 &        &        &   & -a_{1}   \\
              & 1 & \ddots &        &   & -a_{2}   \\
              &   &        & \ddots &   & \vdots   \\
              &   &        &        & 0 & -a_{n-2} \\
              &   &        &        & 1 & -a_{n-1}
        \end{pmatrix}
    \]

    Here all entries of the matrix are $0$ except for all 1's on the line under the
    diagonal and the entries in the last column (some of which might also be $0$). Show that the minimal polynomial of $T$ is the polynomial
    \[
        a_{0} + a_{1}z + \cdots + a_{n-1}z^{n-1} + z^{n}.
    \]
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be the standard basis of $\mathbb{F}^{n}$.
    \begin{itemize}
        \item $Ie_{1} = e_{1}$.
        \item $Te_{1} = e_{2}$.
        \item $T^{2}e_{1} = Te_{2} = e_{3}$.
        \item \ldots
        \item $T^{n-1}e_{1} = Te_{n-1} = e_{n}$.
        \item $T^{n}e_{1} = Te_{n} = -a_{0}e_{1} - a_{1}e_{2} - \cdots - a_{n-1}e_{n-1}$.
    \end{itemize}

    Because $e_{1}, \ldots, e_{n}$ is linearly independent, $Ie_{1}, Te_{1}, \ldots, T^{n-1}e_{1}$ is linearly independent. So for every $b_{0}, b_{1}, \ldots, b_{n-1}$ such that they are not all zero, $(b_{0}I + b_{1}T + \cdots + b_{n-1}T^{n-1})(v_{1})\ne 0$. So the minimal polynomial of $T$ has degree greater than $(n - 1)$. On the other hand, the degree of the minimal polynomial of $T$ does not exceed $n$. Therefore the degree of the minimal polynomial of $T$ is $n$.

    $T^{n}e_{1} = Te_{n} = -a_{0}e_{1} - a_{1}e_{2} - \cdots - a_{n-1}e_{n-1}$, it follows that
    \[
        (a_{0}I + a_{1}T + \cdots + a_{n-1}T^{n-1} + T^{n})(v_{1}) = 0.
    \]

    Moreover, for $k = 2, \ldots, n$
    \begin{align*}
        (a_{0}I + a_{1}T + \cdots + a_{n-1}T^{n-1} + T^{n})(v_{k}) & = (a_{0}I + a_{1}T + \cdots + a_{n-1}T^{n-1} + T^{n})(T^{k-1})(v_{1}) \\
                                                                   & = (T^{k-1})(a_{0}I + a_{1}T + \cdots + a_{n-1}T^{n-1} + T^{n})(v_{1}) \\
                                                                   & = T^{k-1}(0)                                                          \\
                                                                   & = 0.
    \end{align*}

    Hence $a_{0}I + a_{1}T + \cdots + a_{n-1}T^{n-1} + T^{n} = 0$. Thus $a_{0} + a_{1}z + \cdots + a_{n-1}z^{n-1} + z^{n}$ is the minimal polynomial of $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise17
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T \in \lmap{V}$, and $p$ is the minimal polynomial of $T$. Suppose $\lambda \in \mathbb{F}$. Show that the minimal polynomial of $T - \lambda I$ is the polynomial $q$ defined by $q(z) = p(z + \lambda)$.
\end{exercise}

\begin{proof}
    Let $r$ be the minimal polynomial of $T - \lambda I$. Then $r(T - \lambda I) = 0$.

    Assume $\deg r < \deg p$. I define $s\in\mathscr{P}(\mathbb{F})$ as follows: $s(z) = r(z - \lambda)$, then $s(T) = r(T - \lambda I) = 0$ and $s$ is monic. However $\deg s = \deg r < \deg p$, which contradicts $p$ being the minimal polynomial of $T$. Hence $\deg r\geq \deg p$.

    Let $q(z) = p(z + \lambda)$. $q$ is monic, $\deg q = \deg p$ and $q(T - \lambda I) = p(T) = 0$. Therefore $q$ is the minimal polynomial of $T - \lambda I$.
\end{proof}
\newpage

% chapter5:sectionB:exercise18
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T \in \lmap{V}$, and $p$ is the minimal polynomial of $T$. Suppose $\lambda \in \mathbb{F}\setminus\{0\}$. Show that the minimal polynomial of $\lambda T$ is the polynomial $q$ defined by $q(z) = \lambda^{\deg p}p\left(\dfrac{z}{\lambda}\right)$.
\end{exercise}

\begin{proof}
    Let $r$ be the minimal polynomial of $\lambda T$. Then $r(\lambda T) = 0$.

    Assume $\deg r < \deg p$. I define $s\in\mathscr{P}(\mathbb{F})$ as follows: $s(z) = \dfrac{1}{\lambda^{\deg r}}r(\lambda z)$. $s$ is monic and $s(T) = 0$. This contradicts $p$ being the minimal polynomial of $T$. Hence $\deg r\geq \deg p$.

    Let $q(z) = \lambda^{\deg p}p\left(\dfrac{z}{\lambda}\right)$. $q$ is monic and $q(\lambda T) = 0$. Therefore $q$ is the minimal polynomial of $\lambda T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise19
\begin{exercise}\label{chapter5:sectionB:exercise19}
    Suppose $V$ is finite-dimensional and $T\in\lmap{V}$. Let $\mathcal{E}$ be the subspace of $\lmap{V}$ defined by
    \[
        \mathcal{E} = \{ q(T): q\in\mathscr{P}(\mathbb{F}) \}.
    \]

    Prove that $\dim\mathcal{E}$ equals the degree of the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$. Let $q(T)$ be an element of $\mathcal{E}$. According to the Euclidean division algorithm, there exist unique polynomials $s$, $r$ such that $\deg r < \deg p$ and $q = ps + r$. So $q(T) = p(T)s(T) + r(T) = r(T)$. Therefore
    \[
        \mathcal{E} = \{ q(T): q\in\mathscr{P}(\mathbb{F}) \} = \{ q(T): q\in\mathscr{P}_{(\deg p) - 1}(\mathbb{F}) \}
    \]

    $I, T, \ldots, T^{(\deg p) - 1}$ is linearly independent, since the degree of the minimal polynomial of $T$ is $\deg p$. On the other hand, if $q(T)\in \mathcal{E}$, there exists $r\in\mathscr{P}_{(\deg p) - 1}(\mathbb{F})$ such that $q(T) = r(T)$. Moreover, $r(T)$ is in $\operatorname{span}(I, T, \ldots, T^{(\deg p) - 1})$. Therefore $I, T, \ldots, T^{(\deg p) - 1}$ is a basis of $\mathcal{E}$. Thus $\dim \mathcal{E} = \deg p$.
\end{proof}
\newpage

% chapter5:sectionB:exercise20
\begin{exercise}
    Suppose $T \in \lmap{\mathbb{F}^{4}}$ is such that the eigenvalues of $T$ are $3, 5, 8$. Prove that ${(T - 3I)}^{2}{(T - 5I)}^{2}{(T - 8I)}^{2} = 0$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$. Because the eigenvalues of $T$ are $3, 5, 8$, these are also the roots of $p$. So
    \[
        p(z) = (z - 3)(z - 5)(z - 8)q(z)
    \]

    where $q$ is a monic polynomial and $\deg q\leq 1$ (because $3\leq \deg p\leq 4$). Therefore, either $q(z) = 1$, or $q(z) = z - 3$, or $q(z) = z - 5$, or $q(z) = z - 8$ (because the roots of the minimal polynomial of $T$ are the eigenvalues of $T$). In every of these cases, we can conclude ${(T - 3I)}^{2}{(T - 5I)}^{2}{(T - 8I)}^{2} = 0$.
\end{proof}
\newpage

% chapter5:sectionB:exercise21
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that the minimal
    polynomial of $T$ has degree at most $1 + \dim \range{T}$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$ and
    \[
        p(z) = a_{0} + a_{1}z + \cdots + a_{n-1}z^{n-1} + z^{n}.
    \]

    $\range{T}$ is invariant under $T$. Let $q$ be the minimal polynomial of $T\vert_{\range{T}}$.

    Let $s(z) = z\times q(z)$. For all $v\in V$, $s(T)v = q(T)(Tv)$. On the other hand, $Tv\in \range{T}$, so
    \[
        q(T)(Tv) = q(T\vert_{\range{T}})(Tv) = 0.
    \]

    So $s$ is a polynomial multiple of $p$. Hence
    \[
        \deg p \leq \deg s = 1 + \deg q \leq 1 + \dim\range{T}.
    \]

    Thus $\deg p\leq 1 + \dim\range{T}$.
\end{proof}
\newpage

% chapter5:sectionB:exercise22
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in\lmap{V}$. Prove that $T$ is invertible if and only if $I\in\operatorname{span}(T, T^{2}, \ldots, T^{\dim V})$.
\end{exercise}

\begin{proof}
    Let the minimal polynomial of $T$ be $p(z) = a_{0} + a_{1}z + \cdots + a_{n-1}z^{n-1} + z^{n}$.

    If $T$ is invertible, then $a_{0}\ne 0$, and
    \[
        I = -a_{0}^{-1}(a_{1}T + \cdots + a_{n-1}T^{n-1} + T^{n}).
    \]

    So $I\in\operatorname{span}(T, T^{2}, \ldots, T^{n})\subseteq \operatorname{span}(T, T^{2}, \ldots, T^{\dim V})$.

    \bigskip
    If $I\in\operatorname{span}(T, T^{2}, \ldots, T^{\dim V})$, then there exist $c_{1}, \ldots, c_{\dim V}$ in $\mathbb{F}$ such that
    \[
        I = c_{1}T + \cdots + c_{\dim V}T^{\dim V}
    \]

    So the polynomial $q(z) = 1 - c_{1}z - \cdots - c_{\dim V}z^{\dim V}$ is a multiple of $p$. Assume $T$ is not invertible, then $0$ is a root of $p$. So $0$ is also a root of $q$, but this is a contradiction because $q(0) = 1$. Hence the assumption is false, and we conclude that $T$ is invertible.

    Thus $T$ is invertible if and only if $I\in\operatorname{span}(T, T^{2}, \ldots, T^{\dim V})$.
\end{proof}
\newpage

% chapter5:sectionB:exercise23
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in\lmap{V}$. Let $n = \dim V$. Prove that if $v\in V$, then $\operatorname{span}(v, Tv, \ldots, T^{n-1}v)$ is invariant under $T$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$ and
    \[
        p(z) = a_{0} + a_{1}z + \cdots + a_{m-1}z^{m-1} + z^{m}.
    \]

    $m\leq\dim V$ due to the theorem of existence of minimal polynomial. For every $k\in\{0,1,\ldots,n-1\}$, let $r_{k}(z)$ be the remainder of the polynomial division of $p(z)$ be $z^{k}$.

    If $0\leq k < m-1$, then $T(T^{k}v)\in\operatorname{span}(v, Tv, \ldots, T^{m-1}v)\subseteq\operatorname{span}(v, Tv, \ldots, T^{n-1}v)$.

    If $k\geq m-1$, then $T(T^{k}v) = r_{k+1}(T)v\in\operatorname{span}(v, Tv, \ldots, T^{m-1})\subseteq\operatorname{span}(v, Tv, \ldots, T^{n-1}v)$.

    Therefore $\operatorname{span}(v, Tv, \ldots, T^{n-1}v)$ is invariant under $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise24
\begin{exercise}
    Suppose $V$ is a finite-dimensional complex vector space. Suppose $T\in\lmap{V}$ is such that $5$ and $6$ are eigenvalues of $T$ and that $T$ has no other eigenvalues. Prove that ${(T - 5I)}^{\dim V - 1}{(T - 6I)}^{\dim V - 1} = 0$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$ and $\lambda$ be a root of $p$. Then there exists a polynomial $q$ such that $p(z) = (z - \lambda)q(z)$.
    \[
        0 = p(T) = (T - \lambda I)q(T)
    \]

    $q(T)\ne 0$ because $\deg q < \deg p$, $\deg q\ne 0$ and $p$ is the minimal polynomial of $T$. So there exists a vector $v$ in $V$ such that $q(T)v\ne 0$, then $0 = p(T)v = (T - \lambda I)(q(T)v)$. So $\lambda$ is an eigenvalue of $T$.

    According to the hypothesis, $\lambda$ is either $5$ or $6$. From this and the fundamental theorem of algebra, we have
    \[
        p(z) = {(z - 5)}^{m}{(z - 6)}^{n}
    \]

    where $m, n\geq 1$ and $m + n \leq \dim V$. Hence ${(z - 5)}^{\dim V - 1}{(z - 6)}^{\dim V - 1}$ is a multiple of $p$. Thus
    \[
        {(T - 5I)}^{\dim V - 1}{(T - 6I)}^{\dim V - 1} = 0.\qedhere
    \]
\end{proof}
\newpage

% chapter5:sectionB:exercise25
\begin{exercise}\label{chapter5:sectionB:exercise25}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $U$ is a subspace of $V$ that is invariant under $T$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of the quotient operator $T/U$.
        \item Prove that
              \[
                  \text{(minimal polynomial of $T\vert_{U}$)}\times\text{(minimal polynomial of $T/U$)}
              \]

              is a polynomial multiple of the minimal polynomial of $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $p$ be the minimal polynomial of $T$, then $p(T) = 0$.

              For every nonnegative integer $k$
              \[
                  {(T/U)}^{k}(v + U) = T^{k}v + U.
              \]

              Therefore
              \[
                  p(T/U)(v + U) = p(T)v + U = 0 + U = 0.
              \]

              So $p(T/U) = 0$. Hence $p$ is a polynomial multiple of the minimal polynomial of the quotient operator $T/U$.
        \item Let $q_{1}$ be the minimal polynomial of $T\vert_{U}$ and $q_{2}$ be the minimal polynomial of $T/U$.

              For all $v\in V$, $q_{2}(T/U)v = q_{2}(T)v + U$. However, $q_{2}(T/U) = 0$, so $q_{2}(T)v + U = 0 + U$, which means $q_{2}(T)v\in U$. Therefore
              \[
                  q_{1}(T)q_{2}(T)(v) = q_{1}(T)(q_{2}(T)v) = q_{1}(T\vert_{U})(q_{2}(T)v) = 0.
              \]

              Hence $(q_{1}q_{2})(T) = 0$. Thus $q_{1}q_{2}$ is a polynomial multiple of the minimal polynomial of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionB:exercise26
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $U$ is a subspace of $V$ that is invariant under $T$. Prove that the set of eigenvalues of $T$ equals the union of the set of eigenvalues of $T\vert_{U}$ and the set of eigenvalues of $T/U$.
\end{exercise}

\begin{proof}
    Let $\mu_{T}$, $\mu_{T\vert_{U}}$, $\mu_{T/U}$ be minimal polynomials of $T$, $T\vert_{U}$, $T/U$, respectively. We have
    \begin{enumerate}[label={(\arabic*)}]
        \item $\mu_{T}$ is a polynomial multiple of $\mu_{T\vert_U}$.
        \item $\mu_{T}$ is a polynomial multiple of $\mu_{T/U}$.
        \item $\mu_{T\vert_{U}}\times \mu_{T/U}$ is a polynomial multiple of $\mu_{T}$.
    \end{enumerate}

    According to (1) and (2), if $\lambda$ is an eigenvalue of $T\vert_{U}$ or $T/U$, then $\lambda$ is also an eigenvalue of $T$.

    According to (3), if $\lambda$ is an eigenvalue of $T$, then $\lambda$ is an eigenvalue of $T\vert_{U}$ or $T/U$.
\end{proof}
\newpage

% chapter5:sectionB:exercise27
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{R}$, $V$ is finite-dimensional, and $T\in\lmap{V}$. Prove that the minimal polynomial of $T_{\mathbb{C}}$ equals the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$, $q$ be the minimal polynomial of $T_{\mathbb{C}}$. For all $u, v\in V$ and every nonnegative integer $k$
    \[
        T_{\mathbb{C}}^{k}(u + \iota v) = T^{k}u + \iota T^{k}v.
    \]

    So
    \[
        p(T_{\mathbb{C}})(u + \iota v) = p(T)u + \iota p(T)v = 0 + \iota 0.
    \]

    Hence $p$ is a polynomial multiple of $q$. On the other hand
    \[
        0 = q(T_{\mathbb{C}})(u + \iota v) = q(T)u + \iota q(T)v.
    \]

    So $q(T) = 0$. It follows that $q$ is a polynomial multiple of $p$.

    Also, $p, q$ are monic polynomials. Hence $p = q$.
\end{proof}
\newpage

% chapter5:sectionB:exercise28
\begin{exercise}\label{chapter5:sectionB:exercise28}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that the minimal polynomial of $T' \in \lmap{V'}$ equals the minimal polynomial of $T$.
\end{exercise}

\begin{proof}
    Since $(ST)' = T'S'$, then the dual map of $T^{n}$ is ${(T')}^{n}$, for every nonnegative integer $n$. For every $p\in\mathscr{P}(\mathbb{F})$, every $\varphi\in V'$, we have
    \[
        p(T')(\varphi) = \varphi\circ p(T).
    \]

    If $p$ is the minimal polynomial of $T$, then $p$ is a polynomial multiple of the minimal polynomial of $T'$.

    If $p$ is the minimal polynomial of $T'$, then $\varphi\circ p(T) = 0$ for all $\varphi\in V'$. Assume $p(T)\ne 0$, then there exists $v\in V$ such that $p(T)v = v_{1}\ne 0$. Let $v_{1}, \ldots, v_{n}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{n}$ be the dual basis of $V$, then $\varphi_{1}(v_{1}) = 1\ne 0$, which is a contradiction to $\varphi\circ p(T) = 0$. Hence $p(T) = 0$. So $p$ is a polynomial multiple of the minimal polynomial of $T$.

    Thus the minimal polynomial of $T' \in \lmap{V'}$ equals the minimal polynomial of $T$.
\end{proof}
\newpage

% chapter5:sectionB:exercise29
\begin{exercise}
    Show that every operator on a finite-dimensional vector space of dimension at least two has an invariant subspace of dimension two.
\end{exercise}

\begin{proof}
    Unsolved.
\end{proof}
\newpage

\section{Upper-Triangular Matrices}

% chapter5:sectionC:exercise1
\begin{exercise}
    Prove or give a counterexample: If $T \in \lmap{V}$ and $T^{2}$ has an upper-triangular matrix with respect to some basis of $V$, then $T$ has an upper-triangular matrix with respect to some basis of $V$.
\end{exercise}

\begin{proof}
    I give a counterexample.

    Let $V = \mathbb{R}^{2}$ and $T(x, y) = (-y, x)$. Then $T^{2} = -I$, $T^{2}$ has an upper-triangular matrix with respect to any basis of $V$. However, the minimal polynomial of $T$ is $p(z) = z^{2} + 1$, which is not a product of monic polynomial of degree $1$. Therefore with respect to any basis of $V$, the matrix of $T$ is not an upper-triangular matrix.
\end{proof}
\newpage

% chapter5:sectionC:exercise2
\begin{exercise}\label{chapter5:sectionC:exercise2}
    Suppose $A$ and $B$ are upper-triangular matrices of the same size, with $\alpha_{1} , \ldots, \alpha_{n}$ on the diagonal of $A$ and $\beta_{1} , \ldots, \beta_{n}$ on the diagonal of $B$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $A + B$ is an upper-triangular matrix with $\alpha_{1} + \beta_{1}, \ldots, \alpha_{n} + \beta_{n}$ on the diagonal.
        \item Show that $AB$ is an upper-triangular matrix with $\alpha_{1}\beta_{1}, \ldots, \alpha_{n}\beta_{n}$ on the diagonal.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item ${(A + B)}_{j,k} = A_{j,k} + B_{j,k}$. If $j > k$, then $A_{j,k} + B_{j,k} = 0$, and it follows that ${(A + B)}_{j,k} = 0$. Hence $A + B$ is an upper-triangular matrix.

              If $j = k$, then $A_{j,j} = \alpha_{j}$ and $B_{j,j} = \beta_{j}$. So ${(A + B)}_{j,j} = \alpha_{j} + \beta_{j}$.

              Thus $A + B$ is an upper-triangular matrix with $\alpha_{1} + \beta_{1}, \ldots, \alpha_{n} + \beta_{n}$ on the diagonal.
        \item If $j > k$
              \begin{align*}
                  {(AB)}_{j,k} & = \sum^{n}_{r=1}A_{j,r}B_{r,k}                                                                                                       \\
                               & = \sum^{j}_{r=1}A_{j,r}B_{r,k} + \sum^{n}_{r=j+1}A_{j,r}B_{r,k}                                                                      \\
                               & = 0 + 0 = 0                                                     & \text{($B_{r,k} = 0$ for $r\leq j$ and $A_{j,r} = 0$ for $r > j$)}
              \end{align*}

              If $j = k$
              \begin{align*}
                  {(AB)}_{j,j} & = \sum^{n}_{r=1}A_{j,r}B_{r,j}                                                     \\
                               & = \sum^{j-1}_{r=1}A_{j,r}B_{r,j} + A_{j,j}B_{j,j} + \sum^{n}_{r=j+1}A_{j,r}B_{r,j} \\
                               & = 0 + \alpha_{j}\beta_{j} + 0                                                      \\
                               & = \alpha_{j}\beta_{j}.
              \end{align*}

              Thus $AB$ is an upper-triangular matrix with $\alpha_{1}\beta_{1}, \ldots, \alpha_{n}\beta_{n}$ on the diagonal.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionC:exercise3
\begin{exercise}
    Suppose $T \in \lmap{V}$ is invertible and $v_{1}, \ldots, v_{n}$ is a basis of $V$ with respect to which the matrix of $T$ is upper triangular, with $\lambda_{1} , \ldots, \lambda_{n}$ on the diagonal. Show that the matrix of $T^{-1}$ is also upper triangular with respect to the basis $v_{1}, \ldots, v_{n}$, with
    \[
        \frac{1}{\lambda_{1}}, \ldots, \frac{1}{\lambda_{n}}
    \]

    on the diagonal.
\end{exercise}

\begin{proof}
    Let $A = \mathcal{M}(T, (v_{1}, \ldots, v_{n}))$.

    Using mathematical induction, I will show that $T^{-1}v_{k}\in\operatorname{span}(v_{1}, \ldots, v_{k})$ for each $k = 1,\ldots, n$.

    $Tv_{1} = \lambda_{1}v_{1}$ and $T$ is invertible so $T^{-1}v_{1} = \dfrac{1}{\lambda_{1}}v_{1}$.

    Assume $T^{-1}v_{k}\in\operatorname{span}(v_{1}, \ldots, v_{k})$ for $1\leq k < n$. $(T - \lambda_{k+1}I)(v_{k+1}) \in \operatorname{span}(v_{1}, \ldots, v_{k})$ and $\operatorname{span}(v_{1}, \ldots, v_{k})$ is invariant under $T^{-1}$, so
    \[
        \frac{-1}{\lambda_{k}}T^{-1}(T - \lambda_{k+1}I)(v_{k+1}) \in \operatorname{span}(v_{1}, \ldots, v_{k}).
    \]

    So $(T^{-1} - \frac{1}{\lambda_{k+1}}I)(v_{k+1}) \in \operatorname{span}(v_{1}, \ldots, v_{k})$. Therefore $T^{-1}v_{k+1}\in \operatorname{span}(v_{1}, \ldots, v_{k+1})$.

    By the principle of mathematical induction, we conclude that $T^{-1}v_{k}\in\operatorname{span}(v_{1}, \ldots, v_{k})$ for each $k = 1,\ldots, n$. So the matrix of $T^{-1}$ with respect to $v_{1}, \ldots, v_{n}$ is upper-triangular.

    Moreover, the product of the matrix of $T$ and $T^{-1}$ with respect to $v_{1}, \ldots, v_{n}$ is the identity matrix. By Exercise~\ref{chapter5:sectionC:exercise2}, the matrix $T^{-1}$ with respect to $v_{1}, \ldots, v_{n}$ is upper-triangular with
    \[
        \frac{1}{\lambda_{1}}, \ldots, \frac{1}{\lambda_{n}}
    \]

    on the diagonal.
\end{proof}
\newpage

% chapter5:sectionC:exercise4
\begin{exercise}
    Give an example of an operator whose matrix with respect to some basis contains only $0$'s on the diagonal, but the operator is invertible.
\end{exercise}

\begin{proof}
    On $\mathbb{F}^{n}$ where $n > 1$, we define the linear operator $T$ as follows: $Te_{1} = e_{2}$, $Te_{2} = e_{3}$, \ldots, $Te_{n-1} = e_{n}$, $Te_{n} = e_{1}$. The minimal polynomial of $T$ is $z^{n} - 1$, of which constant term is nonzero. Therefore $T$ is invertible. On the other hand, the matrix of $T$ with respect to the standard basis $e_{1}, \ldots, e_{n}$ contains only $0$'s on the diagonal.
\end{proof}
\newpage

% chapter5:sectionC:exercise5
\begin{exercise}
    Give an example of an operator whose matrix with respect to some basis contains only nonzero numbers on the diagonal, but the operator is not invertible.
\end{exercise}

\begin{proof}
    On $\mathbb{F}^{n}$ where $n > 1$, we define the linear operator $T$ as follows:
    \[
        T(x_{1}, \ldots, x_{n}) = (x_{1} + \cdots + x_{n}, \ldots, x_{1} + \cdots + x_{n}).
    \]

    The minimal polynomial of $T$ is $T^{2} - nT$. All entries of the matrix of $T$ with respect to the standard basis are $1$. $T$ is not invertible because the constant term of the minimal polynomial $T^{2} - nT$ is $0$.
\end{proof}
\newpage

% chapter5:sectionC:exercise6
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$, $V$ is finite-dimensional, and $T \in \lmap{V}$. Prove that if $k \in \{1, \ldots, \dim V\}$, then $V$ has a $k$-dimensional subspace invariant under $T$.
\end{exercise}

\begin{proof}
    According to 5.47, $T$ has an upper-triangular matrix with respect to some basis $v_{1}, \ldots, v_{n}$ of $V$.

    Therefore, for every $k \in \{1, \ldots, \dim V\}$, $\operatorname{span}(v_{1}, \ldots, v_{k})$ is invariant under $T$. So for every $k \in \{1, \ldots, \dim V\}$, $V$ has a $k$-dimensional subspace invariant under $T$.
\end{proof}
\newpage

% chapter5:sectionC:exercise7
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$, and $v\in V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that there exists a unique monic polynomial $p_{v}$ of smallest degree such that $p_{v}(T)v = 0$.
        \item Prove that the minimal polynomial of $T$ is a polynomial multiple of $p_{v}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$.
    \begin{enumerate}[label={(\alph*)}]
        \item $p$ is monic and $p(T)v = 0$. So there exists a monic polynomial $p_{v}$ of smallest degree such that $p_{v}(T)v = 0$.

              Assume $q$ is a monic polynomial such that $q(T)v = 0$ and $\deg q = \deg p$. By Euclidean division algorithm, there exist unique polynomials $s$, $r$ such that $q = sp_{v} + r$ and $\deg r < \deg p_{v}$. So
              \[
                  0 = q(T)v = s(T)(p_{v}(T)v) + r(v) = r(v)
              \]

              Because $\deg r < \deg p_{v}$, it follows that $r = 0$. Hence $q = sp_{v}$. $s$ must be monic and $\deg s = 0$ so $s = 1$. Therefore $q = p_{v}$. Thus there exists a unique monic polynomial $p_{v}$ of smallest degree such that $p_{v}(T)v = 0$.
        \item By Euclidean division algorithm, there exist unique polynomials $s$, $r$ such that $p = sp_{v} + r$ and $\deg r < \deg p_{v}$.
              \[
                  0 = p(T)v = s(T)(p_{v}(T)v) + r(T)v = r(T)v.
              \]

              Because $\deg r < \deg p_{v}$, it follows that $\deg r = 0$. Hence $p$ is a polynomial multiple of $p_{v}$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionC:exercise8
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T \in \lmap{V}$, and there exists a nonzero vector $v \in V$ such that $T^{2}v + 2Tv = -2v$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that if $\mathbb{F} = \mathbb{R}$, then there does not exist a basis of $V$ with respect to which $T$ has an upper-triangular matrix.
        \item Prove that if $\mathbb{F} = \mathbb{C}$ and $A$ is an upper-triangular matrix that equals the matrix of $T$ with respect to some basis of $V$, then $-1 + \iota$ or $-1 - \iota$ appears on the diagonal of $A$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Assume $T$ has an eigenvalue $\lambda$. Let $w$ be an eigenvector of $T$ corresponding to $\lambda$, then
              \[
                  -2w = T^{2}w + 2Tw = \lambda^{2}v + 2\lambda w.
              \]

              So $(\lambda^{2} + 2\lambda + 2)v = 0$. It follows that $\lambda^{2} + 2\lambda + 2 = 0$. However, there is no real number $\lambda$ such that $\lambda^{2} + 2\lambda + 2 = 0$ so the assumption is false. Hence $T$ has no eigenvalue. Thus there does not exist a basis of $V$ with respect to which $T$ has an upper-triangular matrix.
        \item The polynomial $p$ where $p(z) = z^{2} + 2z + 2$ satisfies $p(T) = 0$, so $p$ is a polynomial multiple of the minimal polynomial of $T$. The two roots of $p$ are $-1+\iota$ and $-1-\iota$.

              So the minimal polynomial of $T$ is of the form
              \[
                  {(z + 1 - \iota)}^{m}{(z + 1 + \iota)}^{n}
              \]

              where $m$ and $n$ are not both $0$ and $m + n\leq 2$. Hence $-1 + \iota$ or $-1-\iota$ appears on the diagonal of $A$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionC:exercise9
\begin{exercise}
    Suppose $B$ is a square matrix with complex entries. Prove that there exists an invertible square matrix $A$ with complex entries such that $A^{-1}BA$ is an upper-triangular matrix.
\end{exercise}

\begin{proof}
    Let $n$ be the number of columns of $B$.

    I define the linear operator $T$ on $\mathbb{C}^{n}$ as follows: $Tx = Bx$ for all $x\in\mathbb{C}^{n,1}$.

    There exists a basis $v_{1}, \ldots, v_{n}$ of $\mathbb{C}^{n}$ such that the matrix of $T$ with respect to this basis is an upper-triangular matrix. Let
    \[
        A = \mathcal{M}(I, (v_{1}, \ldots, v_{n}), (e_{1}, \ldots, e_{n}))
    \]

    where $e_{1}, \ldots, e_{n}$ is the standard basis of $\mathbb{C}^{n}$. By the change-of-basis formula,
    \[
        \mathcal{M}(T, (v_{1}, \ldots, v_{n})) = A^{-1}BA.
    \]

    $\mathcal{M}(T, (v_{1}, \ldots, v_{n}))$ is an upper-triangular matrix.
\end{proof}
\newpage

% chapter5:sectionC:exercise10
\begin{exercise}\label{chapter5:sectionC:exercise10}
    Suppose $T\in\lmap{V}$ and $v_{1}, \ldots, v_{n}$ is a basis of $V$. Show that the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item The matrix of $T$ with respect to $v_{1}, \ldots, v_{n}$ is lower triangular.
        \item $\operatorname{span}(v_{k}, \ldots, v_{n})$ is invariant under $T$ for each $k = 1, \ldots, n$.
        \item $Tv_{k}\in \operatorname{span}(v_{k}, \ldots, v_{n})$ for each $k = 1, \ldots, n$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    $(a) \Rightarrow (b)$ Since the matrix of $T$ with respect to $v_{1}, \ldots, v_{n}$ is lower triangular, then for each $k\in 1,\ldots, n$, for each $j\in k,\ldots, n$, $Tv_{j}\in \operatorname{span}(v_{k}, \ldots, v_{n})$. Therefore $\operatorname{span}(v_{k}, \ldots, v_{n})$ is invariant under $T$.

    $(b) \Rightarrow (c)$ For each $k = 1,\ldots, n$, $\operatorname{span}(v_{k}, \ldots, v_{n})$ is invariant under $T$, then $Tv_{k}\in \operatorname{span}(v_{k}, \ldots, v_{n})$.

    $(c) \Rightarrow (a)$ Let $A$ be the matrix of $T$ with respect to $v_{1}, \ldots, v_{n}$. Since $Tv_{k}\in \operatorname{span}(v_{k}, \ldots, v_{n})$, then $A_{j,k} = 0$ if $j < k$. Therefore $A$ is a lower-triangular matrix.
\end{proof}
\newpage

% chapter5:sectionC:exercise11
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $V$ is finite-dimensional. Prove that if $T\in\lmap{V}$, then there exists a basis of $V$ with respect to which $T$ has a lower-triangular matrix.
\end{exercise}

\begin{proof}
    Because $T$ is a linear operator on a complex vector space, there exists a basis $v_{1}, \ldots, v_{n}$ with respect to which the matrix of $T$ is upper triangular.

    Let $w_{1} = v_{n}, \ldots, w_{n} = v_{1}$ then $w_{1}, \ldots, w_{n}$ is a basis of $V$. Because $Tv_{k}\in \operatorname{span}(v_{1}, \ldots, v_{k})$ for each $k = 1,\ldots, n$, then $Tw_{k}\in \operatorname{span}(w_{k}, \ldots, w_{n})$ for each $k = 1,\ldots, n$.

    By Exercise~\ref{chapter5:sectionC:exercise10}, it follows that the matrix of $T$ with respect to $w_{1}, \ldots, w_{n}$ is a lower-triangular matrix.
\end{proof}
\newpage

% chapter5:sectionC:exercise12
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T\in\lmap{V}$ has an upper-triangular matrix with respect to some basis of $V$, and $U$ is a subspace of $V$ that is invariant under $T$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $T\vert_{U}$ has an upper-triangular matrix with respect to some basis of $U$.
        \item Prove that the quotient operator $T/U$ has an upper-triangular matrix with respect to some basis of $V/U$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Let $p$ be the minimal polynomial of $T$. Because $T$ has an upper-triangular matrix with respect to some basis of $V$, then $p$ is a product of monic polynomials of degree $1$.
    \begin{enumerate}[label={(\alph*)}]
        \item The polynomial of $T$ is a polynomial multiple of the minimal polynomial of $T\vert_{U}$. Therefore the minimal polynomial of $T\vert_{U}$ is also a product of monic polynomials of degree $1$. Hence $T\vert_{U}$ has an upper-triangular matrix with respect to some basis of $U$.
        \item The polynomial of $T$ is a polynomial multiple of the minimal polynomial of $T/U$. Therefore the minimal polynomial of $T/U$ is also a product of monic polynomials of degree $1$. Hence $T/U$ has an upper-triangular matrix with respect to some basis of $U$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionC:exercise13
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in \lmap{V}$. Suppose there exists a subspace $U$ of $V$ that is invariant under $T$ such that $T\vert_{U}$ has an upper-triangular matrix with respect to some basis of $U$ and also $T/U$ has an upper-triangular matrix with respect to some basis of $V/U$. Prove that $T$ has an upper-triangular matrix with respect to some basis of $V$.
\end{exercise}

\begin{proof}
    Let $\mu_{T}$, $\mu_{T\vert_{U}}$, $\mu_{T/U}$ be minimal polynomials of $T$, $T\vert_{U}$, $T/U$, respectively.

    $T\vert_{U}$ has an upper-triangular matrix with respect to some basis of $U$, so $\mu_{T\vert_{U}}$ is a product of monic polynomial of degree $1$.

    $T/U$ has an upper-triangular matrix with respect to some basis of $U$, so $\mu_{T/U}$ is a product of monic polynomial of degree $1$.

    By Exercise~\ref{chapter5:sectionB:exercise25}, $\mu_{T\vert_{U}}\times \mu_{T/U}$ is a polynomial multiple of $\mu_{T}$.

    Therefore $\mu_{T}$ is a product of monic polynomial of degree $1$. Equivalently, $T$ has an upper-triangular matrix with respect to some basis of $V$.
\end{proof}
\newpage

% chapter5:sectionC:exercise14
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that $T$ has an upper-triangular matrix with respect to some basis of $V$ if and only if the dual operator $T'$ has an upper-triangular matrix with respect to some basis of the dual space $V'$.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter5:sectionB:exercise28}, the minimal polynomials of $T$ and its dual map $T'$ are equal.

    On the other hand, $T$ has an upper-triangular matrix with respect to some basis of $V$ if and only if the minimal polynomial of $T$ is a product of monic polynomials of degree $1$ AND $T'$ has an upper-triangular matrix with respect to some basis of $V'$ if and only if the minimal polynomial of $T'$ is a product of monic polynomials of degree $1$.

    Thus $T$ has an upper-triangular matrix with respect to some basis of $V$ if and only if the dual operator $T'$ has an upper-triangular matrix with respect to some basis of the dual space $V'$.
\end{proof}
\newpage

\section{Diagonalizable Operators}

% chapter5:sectionD:exercise1
\begin{exercise}
    Suppose $V$ is a finite-dimensional complex vector space and $T\in\lmap{V}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that if $T^{4} = I$, then $T$ is diagonalizable.
        \item Prove that if $T^{4} = T$, then $T$ is diagonalizable.
        \item Give an example of an operator $T\in\lmap{\mathbb{C}^{2}}$ such that $T^{4} = T^{2}$ and $T$ is not diagonalizable.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $z^{4} - 1$ is a polynomial multiple of the minimal polynomial of $T$. $z^{4} - 1$ has four distinct zeros in $\mathbb{C}$ (they are $1$, $-1$, $\iota$, $-\iota$), so the minimal polynomial of $T$ has no multiple zero. Hence $T$ is diagonalizable.
        \item $z^{4} - z$ is a polynomial multiple of the minimal polynomial of $T$. $z^{4} - z$ has four distinct zeros in $\mathbb{C}$ (they are $0$, $1$, $\frac{-1-\sqrt{3}\iota}{2}$, $\frac{-1+\sqrt{3}\iota}{2}$), so the minimal polynomial of $T$ has no multiple zero. Hence $T$ is diagonalizable.
        \item I define the linear operator $T\in\lmap{\mathbb{C}^{2}}$ as follows:
              \[
                  T(z_{1}, z_{2}) = (z_{2}, 0).
              \]

              The minimal polynomial of $T$ is $z^{2}$, so $T^{4} = T^{2}$. However, since the minimal polynomial of $T$ has multiple root, it follows that $T$ is not diagonalizable.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionD:exercise2
\begin{exercise}\label{chapter5:sectionD:exercise2}
    Suppose $T\in\lmap{V}$ has a diagonal matrix $A$ with respect to some basis of $V$. Prove that if $\lambda\in\mathbb{F}$, then $\lambda$ appears on the diagonal of $A$ precisely $\dim E(\lambda, T)$ times.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis with respect to which $T$ has a diagonal matrix $A$.

    If $\lambda$ is not an eigenvalue of $T$, then $\lambda$ does not appear on the diagonal of $A$, which means it appears $0 = \dim E(\lambda, T)$ times on the diagonal of $A$.

    If $\lambda$ is an eigenvalue of $T$, then $\lambda$ appears on the diagonal of $A$. Without loss of generality, assume that $v_{1}, \ldots, v_{m}$ are precisely the vectors in the list $v_{1}, \ldots, v_{n}$ that are also eigenvectors of $T$ corresponding to $\lambda$. Let $v$ be a vector in $E(\lambda, T)$. There exist scalars $x_{1}, \ldots, x_{n}$ such that
    \[
        v = x_{1}v_{1} + \cdots + x_{m}v_{m} + x_{m+1}v_{m+1} + \cdots + x_{n}v_{n}.
    \]

    Since $v_{m+1}, \ldots, v_{n}$ are also eigenvectors of $T$ but not corresponding to $\lambda$, there exist scalars $\lambda_{m+1}, \ldots, \lambda$ that are not equal to $\lambda$ such that $Tv_{m+1} = \lambda_{m+1} v_{m+1}$, \ldots, $Tv_{n} = \lambda_{n}v_{n}$. From $Tv = \lambda v$, we deduce that
    \begin{align*}
        T(x_{1}v_{1} + \cdots + x_{m}v_{m} + x_{m+1}v_{m+1} + \cdots + x_{n}v_{n})
         & = \lambda (x_{1}v_{1} + \cdots + x_{m}v_{m}) + \lambda (x_{m+1}v_{m+1} + \cdots + x_{n}v_{n}).
    \end{align*}
    \begin{multline*}
        T(x_{1}v_{1} + \cdots + x_{m}v_{m} + x_{m+1}v_{m+1} + \cdots + x_{n}v_{n}) \\
        = x_{1}Tv_{1} + \cdots + x_{m}Tv_{m} + x_{m+1}Tv_{m+1} + \cdots + x_{n}Tv_{n} \\
        = \lambda (x_{1}v_{1} + \cdots + x_{m}v_{m}) + (\lambda_{m+1}x_{m+1}v_{m+1} + \cdots + \lambda_{n}x_{n}v_{n}).
    \end{multline*}

    Hence $(\lambda - \lambda_{k})x_{k} = 0$ for each $k\in\{ m+1, \ldots, n \}$. However, $\lambda \ne \lambda_{k}$ for each $k\in\{ m+1, \ldots, n \}$ so $x_{k} = 0$ for each $k\in\{ m+1, \ldots, n \}$.

    Therefore $v_{1}, \ldots, v_{m}$ is an independent list that spans $E(\lambda, T)$, so $m = \dim E(\lambda, T)$. This means $\lambda$ appears $\dim E(\lambda, T)$ on the diagonal of $A$.

    Thus $\lambda$ appears on the diagonal of $A$ precisely $\dim E(\lambda, T)$ times.
\end{proof}
\newpage

% chapter5:sectionD:exercise3
\begin{exercise}\label{chapter5:sectionD:exercise3}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that if the operator $T$ is diagonalizable, then $V = \kernel{T}\oplus\range{T}$.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis with respect to which $T$ has a diagonal matrix, and $Tv_{k} = \lambda_{k}v_{k}$ for each $k\in \{ 1, \ldots, n \}$.

    $\kernel{T} = \kernel{(T - 0I)} = E(0, T)$, let $m = \dim E(0, T)$. By Exercise~\ref{chapter5:sectionD:exercise2}, $0$ appears $m$ times on the diagonal of the matrix of $T$ with respect to $v_{1}, \ldots, v_{n}$. Without loss of generality, let $Tv_{k} = \lambda_{k}v_{k}$ for each $k\in \{ 1, \ldots, n \}$, where $\lambda_{k} = 0$ for each $k\in\{ 1, \ldots, m \}$, and $\lambda_{k} \ne 0$ for each $k\in\{ m+1, \ldots, n \}$. Also by Exercise~\ref{chapter5:sectionD:exercise2}, $v_{1}, \ldots, v_{m}$ is a basis of $\kernel{T}$.
    \begin{align*}
        T(x_{1}v_{1} + \cdots + x_{n}v_{n}) & = x_{1}Tv_{1} + \cdots + x_{m}Tv_{m} + x_{m+1}Tv_{m+1} + \cdots + x_{n}Tv_{n} \\
                                            & = x_{m+1}Tv_{m+1} + \cdots + x_{n}Tv_{n}                                      \\
                                            & = x_{m+1}\lambda_{m+1}v_{m+1} + \cdots + x_{n}\lambda_{n}v_{n}.
    \end{align*}

    So $v_{m+1}, \ldots, v_{n}$ spans $T$ and $Tv_{m+1}, \ldots, Tv_{n}$ spans $\range{T}$. Suppose
    \[
        a_{m+1}Tv_{m+1} + \cdots + a_{n}Tv_{n} = 0.
    \]

    Then $T(a_{m+1}v_{m+1} + \cdots + a_{n}v_{n}) = 0$ and $a_{m+1}v_{m+1} + \cdots + a_{n}v_{n}\in\kernel{T}$. Since $v_{1}, \ldots, v_{m}$ is a basis of $\kernel{T}$, there exist scalars $b_{1}, \ldots, b_{m}$ such that
    \[
        a_{m+1}v_{m+1} + \cdots + a_{n}v_{n} = b_{1}v_{1} + \cdots + b_{m}v_{m}.
    \]

    Because $v_{1}, \ldots, v_{n}$ is a basis of $V$, it follows that $a_{m+1} = \cdots = a_{n} = 0$ and $b_{1} = \cdots = b_{m} = 0$. So $Tv_{m+1}, \ldots, Tv_{n}$ is a basis of $\range{T}$, $v_{m+1}, \ldots, v_{n}$ is a basis of $\range{T}$, and $\kernel{T}\cap\range{T} = \{0\}$.

    On the other hand
    \[
        x_{1}v_{1} + \cdots + x_{n}v_{n} = \underbrace{x_{1}v_{1} + \cdots + x_{m}v_{m}}_{\in\kernel{T}} + \underbrace{x_{m+1}v_{m+1} + \cdots + x_{n}v_{n}}_{\in\range{T}}
    \]

    so $V = \kernel{T} + \range{T}$. Thus $V = \kernel{T}\oplus\range{T}$.
\end{proof}
\newpage

% chapter5:sectionD:exercise4
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in\lmap{V}$. Prove that the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item $V = \kernel{T}\oplus \range{T}$.
        \item $V = \kernel{T} + \range{T}$.
        \item $\kernel{T}\cap \range{T} = \{0\}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    (a) implies (b) and (c) due to the definition of direct sum.

    According to the fundamental theorem of linear maps,
    \[
        \dim V = \dim\kernel{T} + \dim\range{T}.
    \]

    Moreover,
    \[
        \dim V = \dim\kernel{T} + \dim\range{T} = \dim (\kernel{T} + \range{T}) - \dim (\kernel{T}\cap\range{T}).
    \]

    Therefore (b) implies (c) and (c) implies (a).
\end{proof}
\newpage

% chapter5:sectionD:exercise5
\begin{exercise}
    Suppose $V$ is a finite-dimensional complex vector space and $T$. Prove that $T$ is diagonalizable if and only if
    \[
        V = \kernel{(T - \lambda I)} \oplus \range{(T - \lambda I)}
    \]

    for every $\lambda\in\mathbb{C}$.
\end{exercise}

\begin{proof}
    If $T$ is diagonalizable, then there exists a basis $v_{1}, \ldots, v_{n}$ of $V$ with respect to which $T$ has a diagonal matrix $A$. Then the matrix of $T - \lambda I$ with respect to $v_{1}, \ldots, v_{n}$ is $A - \lambda I$, which is a diagonal matrix. So $T - \lambda I$ is diagonalizable. By Exercise~\ref{chapter5:sectionD:exercise3}, $V = \kernel{(T - \lambda I)}\oplus \range{(T - \lambda I)}$ for all $\lambda\in \mathbb{C}$.

    To prove the implication of the other direction, I use mathematical induction on $\dim V$. Assume the statement is true for every complex vector space of dimension less than $\dim V$.

    Let $\lambda$ be an eigenvalue of $T$, then $\kernel{(T - \lambda I)}\ne \{0\}$ and $\range{(T - \lambda I)}$ is a proper subspace of $V$. Since $\range{(T - \lambda I)}$ is invariant under $T$ and $\range{(T - \lambda I)}$ is a proper subspace of $V$ so according to the induction hypothesis, the restriction of $T$ on $\range{(T - \lambda I)}$ is diagonalizable.

    So there exists a basis of $\range{(T - \lambda I)}$ consisting of eigenvectors of the restriction of $T$ on $\range{(T - \lambda I)}$. These vectors are also eigenvectors of $T$, let them be $v_{1}, \ldots, v_{m}$. Let $v_{m+1}, \ldots, v_{m+n}$ be a basis of $\kernel{(T - \lambda I)}$.

    Because $V = \kernel{(T - \lambda I)} \oplus \range{(T - \lambda I)}$, we conclude that $v_{1}, \ldots, v_{m}, v_{m+1}, \ldots, v_{m+n}$ is a basis of $V$. Further, these vectors are eigenvectors of $T$, so $T$ is diagonalizable.

    According to the principle of mathematical induction, if $V = \kernel{(T - \lambda I)} \oplus \range{(T - \lambda I)}$ for every $\lambda\in\mathbb{C}$, then $T$ is diagonalizable.
\end{proof}
\newpage

% chapter5:sectionD:exercise6
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{F}^{5}}$ and $\dim E(8, T) = 4$. Prove that $T - 2I$ or $T - 6I$ is invertible.
\end{exercise}

\begin{proof}
    $z - 8$ is not the minimal polynomial of $T$, because if so, $T = 8I$ and it follows that $\dim E(8, T) = 5\ne 4$.

    On the other hand, the minimal polynomial of $T$ is a multiple polynomial of $z - 8$ and has at least another zero other than $8$. Let $8, \lambda_{1}, \ldots, \lambda_{n}$ be the distinct roots of the minimal polynomial of $T$, then if $n > 1$
    \[
        \dim E(8, T) + \dim (\lambda_{1}, T) + \cdots + \dim (\lambda_{n}, T) > \dim E(8, T) + 1 = 5.
    \]

    Meanwhile,
    \[
        \dim E(8, T) + \dim (\lambda_{1}, T) + \cdots + \dim (\lambda_{n}, T) \leq \dim \mathbb{F}^{5} = 5.
    \]

    So the assumption is false. Hence $p$ has exactly one zero other than $8$, let it be $\lambda$. $\lambda$ cannot be both $2$ and $6$, so $T - 2I$ or $T - 6I$ is invertible.
\end{proof}
\newpage

% chapter5:sectionD:exercise7
\begin{exercise}
    Suppose $T\in\lmap{V}$ is invertible. Prove that
    \[
        E(\lambda, T) = E\left(\frac{1}{\lambda}, T^{-1}\right)
    \]

    for every $\lambda\in\mathbb{F}$ with $\lambda\ne 0$.
\end{exercise}

\begin{proof}
    \begin{align*}
        v\in E(\lambda, T) & \Longleftrightarrow Tv = \lambda v                                \\
                           & \Longleftrightarrow v = T^{-1}(\lambda v)                         \\
                           & \Longleftrightarrow \frac{1}{\lambda}v = T^{-1}v                  \\
                           & \Longleftrightarrow v\in E\left(\frac{1}{\lambda}, T^{-1}\right).
    \end{align*}

    Thus $E(\lambda, T) = E\left(\frac{1}{\lambda}, T^{-1}\right)$.
\end{proof}
\newpage

% chapter5:sectionD:exercise8
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in\lmap{V}$. Let $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct nonzero eigenvalues of $T$. Prove that
    \[
        \dim E(\lambda_{1}, T) + \cdots + \dim E(\lambda_{m}, T)\leq \dim\range{T}.
    \]
\end{exercise}

\begin{proof}
    Let $v$ be a vector in $E(\lambda_{1}, T) + \cdots + E(\lambda_{m}, T)$, then there exist vectors $v_{k}\in E(\lambda_{k}, T)$ for each $k\in\{ 1, \ldots, m \}$ such that
    \[
        v = v_{1} + \cdots + v_{m}.
    \]

    On the other hand
    \begin{align*}
        v & = v_{1} + \cdots + v_{m}                                                                 \\
          & = \frac{1}{\lambda_{1}}\lambda_{1}v_{1} + \cdots + \frac{1}{\lambda_{m}}\lambda_{1}v_{m} \\
          & =  \frac{1}{\lambda_{1}}Tv_{1} + \cdots + \frac{1}{\lambda_{m}}Tv_{m}                    \\
          & = T\left(\frac{1}{\lambda_{1}}v_{1} + \cdots + \frac{1}{\lambda_{m}}v_{m}\right).
    \end{align*}

    So $v\in \range{T}$, therefore $E(\lambda_{1}, T) + \cdots + E(\lambda_{m}, T)$ is a subspace of $\range{T}$.

    $\lambda_{1}, \ldots, \lambda_{m}$ are distinct eigenvalues of $T$, so the sum
    \[
        E(\lambda_{1}, T) + \cdots + E(\lambda_{m}, T)
    \]

    is a direct sum. Thus
    \[
        \dim E(\lambda_{1}, T) + \cdots + \dim E(\lambda_{m}, T) = \dim(E(\lambda_{1}, T) + \cdots + E(\lambda_{m}, T))\leq \dim\range{T}.\qedhere
    \]
\end{proof}
\newpage

% chapter5:sectionD:exercise9
\begin{exercise}
    Suppose $R, T\in \lmap{\mathbb{F}^{3}}$ each have $2, 6, 7$ as eigenvalues. Prove that there exists an invertible operator $S\in\lmap{\mathbb{F}^{3}}$ such that $R = S^{-1}TS$.
\end{exercise}

\begin{proof}
    $R, T\in \lmap{\mathbb{F}^{3}}$ each have $2, 6, 7$ as eigenvalues. So $R$ and $T$ are diagonalizable.

    Let $v_{1}, v_{2}, v_{3}$ be eigenvectors of $R$ corresponding to $2, 6, 7$, respectively. Let $w_{1}, w_{2}, w_{3}$ be eigenvectors of $T$ corresponding to $2, 6, 7$, respectively. Then $v_{1}, v_{2}, v_{3}$ and $w_{1}, w_{2}, w_{3}$ are two bases of $\mathbb{F}^{3}$. Denote $\lambda_{1} = 2$, $\lambda_{2} = 6$, $\lambda_{3} = 7$.

    I define the operator $S\in\lmap{\mathbb{F}^{3}}$ as follows: $Sw_{1} = v_{1}$, $Sw_{2} = v_{2}$, $Sw_{3} = v_{1}$. $S$ is invertible. For each $i\in\{1, 2, 3\}$,
    \[
        (S^{-1}TS)(w_{i}) = (S^{-1}T)(v_{i}) = S^{-1}(\lambda_{i}v_{i}) = \lambda_{i}S^{-1}v_{i} = \lambda_{i}w_{i} = Rw_{i}.
    \]

    Hence $R = S^{-1}TS$.
\end{proof}
\newpage

% chapter5:sectionD:exercise10
\begin{exercise}
    Find $R, T\in \lmap{\mathbb{F}^{4}}$ such that $R$ and $T$ each have $2, 6, 7$ as eigenvalues, $R$ and $T$ have no other eigenvalues, and there does not exist an invertible operator $S\in\lmap{\mathbb{F}^{4}}$ such that $R = S^{-1}TS$.
\end{exercise}

\begin{proof}
    Let
    \[
        R(z_{1}, z_{2}, z_{3}, z_{4}) = (2z_{1}, 2z_{2}, 6z_{3}, 7z_{4})
    \]

    then $R$ is diagonalizable and the minimal polynomial of $R$ is $(z - 2)(z - 6)(z - 7)$.

    Let $Se_{1} = e_{2}$, $Se_{2} = e_{3}$, $Se_{3} = e_{4}$, $Se_{4} = -168e_{1} + 220e_{2} - 98e_{3} + 17e_{4}$. The matrix of $S$ with respect to the standard basis of $\mathbb{F}^{4}$ is
    \[
        \begin{pmatrix}
            0 & 0 & 0 & -168 \\
            1 & 0 & 0 & 220  \\
            0 & 1 & 0 & -98  \\
            0 & 0 & 1 & 17
        \end{pmatrix}
    \]

    so the minimal polynomial of $S$ is
    \[
        z^{4} - 17z^{3} + 98z^{2} - 220z + 168 = {(z - 2)}^{2}(z - 6)(z - 7).
    \]

    Let $p$ denote the minimal polynomial of $R$. Assume there exists an invertible operator $S\in\lmap{\mathbb{F}^{4}}$ such that $R = S^{-1}TS$, then
    \[
        p(T) = p(SRS^{-1}) = Sp(R)S^{-1} = 0.
    \]

    So $p$ is a polynomial multiple of ${(z - 2)}^{2}(z - 6)(z - 7)$, which is a contradiction. So there does not exist an invertible operator $S\in\lmap{\mathbb{F}^{4}}$ such that $R = S^{-1}TS$ for the given operators $R$ and $T$.
\end{proof}
\newpage

% chapter5:sectionD:exercise11
\begin{exercise}
    Find $T\in\lmap{\mathbb{C}^{3}}$ such that $6$ and $7$ are eigenvalues of $T$ and such that $T$ does not have a diagonal matrix with respect to any basis of $\mathbb{C}^{3}$.
\end{exercise}

\begin{proof}
    I define the operator $T$ on $\mathbb{C}^{3}$ as follows: $Te_{1} = e_{2}$, $Te_{2} = e_{3}$, $Te_{3} = 252e_{1} - 120e_{2} + 19e_{3}$.

    The matrix of $T$ with respect to the standard basis is
    \[
        \begin{pmatrix}
            0 & 0 & 252  \\
            1 & 0 & -120 \\
            0 & 1 & 19
        \end{pmatrix}
    \]

    which is also the companion matrix of the polynomial
    \[
        z^{3} - 19z^{2} + 120z - 252 = {(z - 6)}^{2}(z - 7).
    \]

    This is also the minimal polynomial of $T$. Therefore $T$ is not diagonalizable (because it has a double root).
\end{proof}
\newpage

% chapter5:sectionD:exercise12
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{C}^{3}}$ is such that $6$ and $7$ are eigenvalues of $T$. Furthermore, suppose $T$ does not have a diagonal matrix with respect to any basis of $\mathbb{C}^{3}$. Prove that there exists $(z_{1}, z_{2}, z_{3}) \in \mathbb{C}^{3}$ such that
    \[
        T(z_{1}, z_{2}, z_{3}) = (6 + 8z_{1}, 7 + 8z_{2}, 13 + 8z_{3}).
    \]
\end{exercise}

\begin{proof}
    $T$ does not have a diagonal matrix with respect to any basis of $\mathbb{C}^{3}$, so $T$ does not have any eigenvalue other than $6$ and $7$ (otherwise, $T$ has three distinct eigenvalues, which makes $T$ diagonalizable). So $8$ is not an eigenvalue of $T$. Therefore $T - 8I$ is invertible. So there exists $(z_{1}, z_{2}, z_{3}) \in \mathbb{C}^{3}$ such that $(T - 8I)(z_{1}, z_{2}, z_{3}) = (6, 7, 13)$. Equivalently, there exists $(z_{1}, z_{2}, z_{3}) \in \mathbb{C}^{3}$ such that $T(z_{1}, z_{2}, z_{3}) = (6 + 8z_{1}, 7 + 8z_{2}, 13 + 8z_{3})$.
\end{proof}
\newpage

% chapter5:sectionD:exercise13
\begin{exercise}
    Suppose $A$ is a diagonal matrix with distinct entries on the diagonal and $B$ is a matrix of the same size as $B$. Show that $AB = BA$ if and only if $B$ is a diagonal matrix.
\end{exercise}

\begin{proof}
    Let $n$ be the number of rows of $A$, $B$. $AB = BA$ if and only if for all pairs $(j, k)$ such that $1\leq j, k\leq n$,
    \[
        \sum^{n}_{r=1}A_{j,r}B_{r,k} = \sum^{n}_{r=1}B_{j,r}A_{r,k}.
    \]

    Since $A$ is a diagonal matrix, then the statement is equivalent to
    \[
        A_{j,j}B_{j,k} = B_{j,k}A_{k,k}
    \]

    for all pairs $(j, k)$ such that $1\leq j, k\leq n$.

    Since the entries on the diagonal of $A$ are distinct, the statement is equivalent to $B_{j,k} = 0$ for all pairs $(j, k)$ such that $1\leq j, k\leq n$ and $j\ne k$. Equivalently, this means $B$ is a diagonal matrix.
\end{proof}
\newpage

% chapter5:sectionD:exercise14
\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Give an example of a finite-dimensional complex vector space and an
              operator $T$ on that vector space such that $T^{2}$ is diagonalizable but $T$ is not diagonalizable.
        \item Suppose $\mathbb{F} = \mathbb{C}$, $k$ is a positive integer, and $T\in\lmap{V}$ is invertible. Prove that $T$ is diagonalizable if and only if $T^{k}$ is diagonalizable.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item I define the linear operator $T$ on $\mathbb{C}^{2}$ as follows: $T(z_{1}, z_{2}) = (0, z_{1})$. Due to this definition, $T^{2} = 0$, so $T^{2}$ is diagonalizable. However, $T$ is not diagonalizable because the minimal polynomial of $T$ is $z^{2}$, which is not a product of different monic polynomials of degree $1$.
        \item If $T$ is diagonalizable, then there exists a basis $v_{1}, \ldots, v_{n}$ of $V$ such that the matrix of $T$ with respect to this basis is a diagonal matrix. Let $Tv_{i} = \lambda_{i}v_{i}$, where $\lambda_{i}$ is the $i$th entry on the diagonal of the matrix of $T$. Then $T^{k}v_{i} = \lambda_{i}^{k}v_{i}$. Therefore the matrix of $T^{k}$ with respect to the basis $v_{1}, \ldots, v_{n}$ is a diagonal matrix, and the $i$th entry on the diagonal of this matrix is $\lambda_{i}^{k}$.

              Assume that the statement is true for all positive integer less than $m$. If $T^{k}$ is diagonalizable, then the minimal polynomial of $T^{k}$ is
              \[
                  (z - \lambda_{1})\cdots (z - \lambda_{m})
              \]

              where $\lambda_{1}, \ldots, \lambda_{m}$ are distinct nonzero complex numbers. Each nonzero complex number $\lambda_{i}$ has $k$ $k$-roots, let them be $\lambda_{i,1}, \ldots, \lambda_{i,k}$. A $k$-root of $\lambda_{i}$ is not equal to a $k$-root of $\lambda_{j}$ for all $i\ne j$ (this is proved by contradiction). Hence
              \begin{align*}
                  (z^{k} - \lambda_{1})\cdots (z^{k} - \lambda_{m}) & = \prod^{m}_{i=1}(z^{k} - \lambda_{i})                           \\
                                                                    & = \prod^{m}_{i=1}\left(\prod^{k}_{j=1}(z - \lambda_{i,j})\right)
              \end{align*}

              Hence
              \[
                  \prod^{m}_{i=1}\left(\prod^{k}_{j=1}(T - \lambda_{i,j}I)\right) = 0.
              \]

              So this polynomial is a product of distinct monic polynomial of degree $1$ and is a multiple polynomial of the minimal polynomial of $T$. Therefore the minimal polynomial of $T$ is also a product of distinct monic polynomial of degree $1$. Therefore $T$ is diagonalizable.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionD:exercise15
\begin{exercise}
    Suppose $V$ is a finite-dimensional complex vector space, $T\in\lmap{V}$, and $p$ is the minimal polynomial of $T$. Prove that the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item $T$ is diagonalizable.
        \item There does not exist $\lambda\in\mathbb{C}$ such that $p$ is a polynomial multiple of ${(z - \lambda)}^{2}$.
        \item $p$ and its derivative $p'$ have no zeros in common.
        \item The greatest common divisor of $p$ and $p'$ is the constant polynomial $1$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    $T$ is diagonalizable if and only if $p$ is the product of distinct monic polynomial of degree $1$. Together with the second version of the fundamental theorem of algebra, it follows that (a) and (b) are equivalent.

    (b) and (c) are equivalent due to Exercise~\ref{chapter4:sectionA:exercise8}.

    Now I will prove that (c) and (d) are equivalent.

    If (c) is true, assume that the greatest common disisor of $p$ and $p'$ is a nonconstant polynomial, then $p$ and $p'$ have at least one zero in common. So the assumption is false. Therefore (d) is true. So (c) implies (d).

    If (d) is true, then there exist polynomials $s$ and $r$ such that
    \[
        sp + rp' = 1.
    \]

    From $sp + rp' = 1$, we deduce that every zero of $p$ is not a zero of $p'$ and vice versa, so $p$ and $p'$ have no zeros in common. Hence (d) implies (c).

    Thus (c) and (d) are equivalent.
\end{proof}
\newpage

% chapter5:sectionD:exercise16
\begin{exercise}
    Suppose that $T\in\lmap{V}$ is diagonalizable. Let $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct eigenvalues of $T$. Prove that a subspace $U$ of $V$ is invariant under $T$ if and only if there exist subspaces $U_{1}, \ldots, U_{m}$ of $V$ such that $U_{k}\subseteq E(\lambda_{k}, T)$ for each $k$ and $U = U_{1}\oplus \cdots \oplus U_{m}$.
\end{exercise}

\begin{proof}
    If there exist subspaces $U_{1}, \ldots, U_{m}$ of $V$ such that $U_{k}\subseteq E(\lambda_{k}, T)$ for each $k$ and $U = U_{1}\oplus \cdots \oplus U_{m}$, then for every $u\in U$, there exist $v_{1}\in U_{1}, \ldots, v_{m}\in U_{m}$ such that
    \[
        u = v_{1} + \cdots + v_{m}
    \]

    and $Tu = Tv_{1} + \cdots + Tv_{m} = \lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}\in U_{1}\oplus \cdots \oplus U_{m} = U$. Therefore $U$ is invariant under $T$.

    \bigskip

    If $U$ is invariant under $T$. Since $T$ is diagonalizable, then so is $T\vert_{U}$. Let $\alpha_{1}, \ldots, \alpha_{p}$ be the distinct eigenvalues of $T\vert_{U}$, then $\{ \alpha_{1}, \ldots, \alpha_{p} \}\subseteq \{ \lambda_{1}, \ldots, \lambda_{n} \}$.

    Let $U_{k} = E(\lambda_{k}, T\vert_{U})$ for each $k\in\{ 1, \ldots, n \}$, then
    \begin{align*}
        U & = E(\alpha_{1}, T\vert_{U})\oplus\cdots\oplus E(\alpha_{p}, T\vert_{U})   \\
          & = E(\lambda_{1}, T\vert_{U})\oplus\cdots\oplus E(\lambda_{m}, T\vert_{U}) \\
          & = U_{1}\oplus\cdots \oplus U_{m}.
    \end{align*}
\end{proof}
\newpage

% chapter5:sectionD:exercise17
\begin{exercise}
    Suppose $V$ is finite-dimensional. Prove that $\lmap{V}$ has a basis consisting of diagonalizable operators.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$.

    Let $E_{i,j}$ be the linear map such that
    \[
        E_{i,j}v_{k} = \begin{cases}
            v_{i} & \text{if $k = j$} \\
            0     & \text{otherwise}
        \end{cases}
    \]

    Then the list $E_{i,j}$ ($1\leq i, j\leq n$) is a basis of $\lmap{V}$. Replace this list by $E_{1,1}, \ldots, E_{n,n}$, $E_{i,i} + E_{i,j}$ ($1\leq i, j\leq n$, $i\ne j$), then this list is still a basis of $\lmap{V}$.

    If $i\ne j$,
    \begin{align*}
        (E_{i,i} + E_{i,j})(E_{i,i} + E_{i,j}) & = E_{i,i}E_{i,i} + E_{i,j}E_{i,i} + E_{i,i}E_{i,j} + E_{i,j}E_{i,j} \\
                                               & = E_{i,i} + 0 + E_{i,j} + 0                                         \\
                                               & = E_{i,i} + E_{i,j}
    \end{align*}

    So $z^{2} - z = z(z - 1)$ is the minimal polynomial of $E_{i,i} + E_{i,j}$. Therefore $E_{i,i} + E_{i,j}$ is diagonalizable for all $i\ne j$.

    Thus the list $E_{1,1}, \ldots, E_{n,n}$, $E_{i,i} + E_{i,j}$ ($1\leq i, j\leq n$, $i\ne j$) consisting of diagonalizable operators and is a basis of $\lmap{V}$.
\end{proof}
\newpage

% chapter5:sectionD:exercise18
\begin{exercise}
    Suppose that $T \in \lmap{V}$ is diagonalizable and $U$ is a subspace of $V$ that is invariant under $T$. Prove that the quotient operator $T/U$ is a diagonalizable operator on $V/U$.
\end{exercise}

\begin{proof}
    $T$ is diagonalizable so the minimal polynomial of $T$ is a product of distinct monic polynomial of degree $1$. On the other hand, the minimal polynomial of $T$ is a polynomial multiple of the minimal polynomial of $T/U$. Therefore the minimal polynomial of $T/U$ is also a product of distinct monic polynomial of degree $1$. Thus $T/U$ is a diagonalizable operator on $V/U$.
\end{proof}
\newpage

% chapter5:sectionD:exercise19
\begin{exercise}
    Prove or give a counterexample: If $T \in \lmap{V}$ and there exists a subspace $U$ of $V$ that is invariant under $T$ such that $T\vert_{U}$ and $T/U$ are both diagonalizable, then $T$ is diagonalizable.
\end{exercise}

\begin{proof}
    I give a counterexample.

    On $V = \mathbb{F}^{2}$, I define the linear operator $T$ as follows: $T(x, y) = (y, 0)$. The minimal polynomial of $T$ is $z^{2} = 0$, so $T$ is not diagonalizable.

    $\kernel{T} = \operatorname{span}((1, 0))$. Let $U = \kernel{T}$ then $U$ is invariant under $T$. $T\vert_{U} = 0$ so $T\vert_{U}$ is diagonalizable.

    $V/U = \operatorname{span}((0, 1) + U)$, so
    \[
        T/U((0, 1) + U) = T(0, 1) + U = (1, 0) + U = 0 + U.
    \]

    Therefore $T/U = 0$, which means $T/U$ is diagonalizable.
\end{proof}
\newpage

% chapter5:sectionD:exercise20
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that $T$ is diagonalizable if and only if the dual operator $T'$ is diagonalizable.
\end{exercise}

\begin{proof}
    A linear operator is diagonalizable if and only if its minimal polynomial is a product of distinct monic polynomial of degree $1$. By Exercise~\ref{chapter5:sectionB:exercise28}, the minimal polynomials of $T$ and its dual map $T'$ are equal. Thus $T$ is diagonalizable if and only if the dual operator $T'$ is diagonalizable.
\end{proof}
\newpage

% chapter5:sectionD:exercise21
\begin{exercise}
    The \textit{Fibonacci sequence} $F_{0}, F_{1}, F_{2}, \ldots$ is defined by
    \[
        F_{0} = 0, F_{1} = 1, \text{ and } F_{n} = F_{n-2} + F_{n-1} \text{ for $n\geq 2$}.
    \]

    Define $T\in\lmap{\mathbb{R}^{2}}$ by $T(x, y) = (y, x + y)$.

    \begin{enumerate}[label={(\alph*)}]
        \item Show that $T^{n}(0, 1) = (F_{n}, F_{n+1})$ for each nonnegative integer $n$.
        \item Find the eigenvalues of $T$.
        \item Find a basis of $\mathbb{R}^{2}$ consisting of eigenvectors of $T$.
        \item Use the solution to (c) to compute $T^{n}(0, 1)$. Conclude that
              \[
                  F_{n} = \frac{1}{\sqrt{5}}\left[{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n} - {\left(\frac{1 - \sqrt{5}}{2}\right)}^{n}\right]
              \]

              for each nonnegative integer $n$.
        \item Use (d) to conclude that if $n$ is a nonnegative integer, then the Fibonacci number $F_{n}$ is the integer that is closest to
              \[
                  \frac{1}{\sqrt{5}}{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n}.
              \]
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item I give a proof using mathematical induction.

              $T^{0}(0, 1) = (0, 1) = (F_{0}, F_{1})$.

              Assume $T^{n}(0, 1) = (F_{n}, F_{n+1})$, then according to the induction hypothesis
              \[
                  T^{n+1}(0, 1) = T(F_{n}, F_{n+1}) = (F_{n+1}, F_{n} + F_{n+1}) = (F_{n+1}, F_{n+2}).
              \]

              Thus, due to the principle of mathematical induction $T^{n}(0, 1) = (F_{n}, F_{n+1})$ for each nonnegative integer $n$.
        \item Let $\lambda$ be an eigenvalue of $T$ and $(x, y)$ be a corresponding eigenvector.
              \[
                  T(x, y) = (y, x + y) = (\lambda x, \lambda y).
              \]

              Therefore $\lambda y^{2} = \lambda x(x + y)$. If $\lambda = 0$ then $x = y = 0$, which contradicts the definition of eigenvector. So $\lambda\ne 0$ and it follows that $x(x + y) = y^{2}$. $x = 0$ if and only if $y = 0$, therefore $x, y\ne 0$. From $x(x + y) = y^{2}$, we deduce that $\frac{y}{x}$ is a root of the quadratic equation $t^{2} - t - 1 = 0$. The two roots of this equation are $\frac{1 + \sqrt{5}}{2}$ and $\frac{1 - \sqrt{5}}{2}$.

              Hence the eigenvalues of $T$ are $\frac{1 + \sqrt{5}}{2}$ and $\frac{1 - \sqrt{5}}{2}$.
        \item A basis of $\mathbb{R}^{2}$ consisting of eigenvectors of $T$ is
              \[
                  \left( 1, \frac{1+\sqrt{5}}{2} \right), \left( 1, \frac{1-\sqrt{5}}{2} \right).
              \]
        \item \begin{align*}
                  T^{n}(0, 1) & = \frac{1}{\sqrt{5}}T^{n}\left(1, \frac{1+\sqrt{5}}{2}\right) - \frac{1}{\sqrt{5}}T^{n}\left(1, \frac{1-\sqrt{5}}{2}\right)                                                                                                                                   \\
                              & = \frac{1}{\sqrt{5}}{\left(\frac{1+\sqrt{5}}{2}\right)}^{n}\left(1, \frac{1+\sqrt{5}}{2}\right) - \frac{1}{\sqrt{5}}{\left(\frac{1-\sqrt{5}}{2}\right)}^{n}\left(1, \frac{1-\sqrt{5}}{2}\right)                                                               \\
                              & = \left(\frac{1}{\sqrt{5}}\left[{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n} - {\left(\frac{1 - \sqrt{5}}{2}\right)}^{n}\right], \frac{1}{\sqrt{5}}\left[{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n+1} - {\left(\frac{1 - \sqrt{5}}{2}\right)}^{n+1}\right]\right)
              \end{align*}

              Hence
              \[
                  F_{n} = \frac{1}{\sqrt{5}}\left[{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n} - {\left(\frac{1 - \sqrt{5}}{2}\right)}^{n}\right]
              \]

              for each nonnegative integer $n$.
        \item \[
                  \abs{F_{n} - \frac{1}{\sqrt{5}}{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n}} = \abs{\frac{1}{\sqrt{5}}{\left(\frac{1 - \sqrt{5}}{2}\right)}^{n}}\leq \frac{1}{\sqrt{5}} < \frac{1}{2}.
              \]

              So $F_{n}$ is the integer that is closest to
              \[
                  \frac{1}{\sqrt{5}}{\left(\frac{1 + \sqrt{5}}{2}\right)}^{n}.
              \]
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionD:exercise22
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $A$ is an $n$-by-$n$ matrix that is the matrix of $T$ with respect to some basis of $V$. Prove that if
    \[
        \abs{A_{j,j}} > \sum^{n}_{\substack{k=1\\ k\ne j}}\abs{A_{j,k}}
    \]

    for each $j\in \{ 1, \ldots, n \}$, then $T$ is invertible.
\end{exercise}

\begin{proof}
    Assume that $T$ is not invertible, then there exists a nonzero vector $v$ such that $Tv = 0$. So $0$ is an eigenvalue of $T$.

    According to the Gershgorin disk theorem, there exists $j$ in $\{ 1, \ldots, n \}$ such that
    \[
        \abs{0 - A_{j,j}} < \sum^{n}_{\substack{k=1 \\ k\ne j}}\abs{A_{j,k}}.
    \]

    But this is a contradiction, since
    \[
        \abs{A_{j,j}} > \sum^{n}_{\substack{k=1\\ k\ne j}}\abs{A_{j,k}}
    \]

    for each $j\in \{ 1, \ldots, n \}$. Therefore the assumption is false, and thus $T$ is invertible.
\end{proof}
\newpage

% chapter5:sectionD:exercise23
\begin{exercise}
    Suppose the definition of the Gershgorin disks is changed so that the radius of the $k$th disk is the sum of the absolute values of the entries in column (instead of row) $k$ of $A$, excluding the diagonal entry. Show that the Gershgorin disk theorem (5.67) still holds with this changed definition.
\end{exercise}

\begin{proof}
    Assume that the linear operator $T$ has matrix $A$ with respect to a basis $v_{1}, \ldots, v_{n}$ of $V$, then the matrix of the dual map $T'$ of $T$ has matrix $B = A^{\top}$ with respect to the dual basis of $v_{1}, \ldots, v_{n}$.

    Let $\lambda$ be an eigenvalue of $T$, then $\lambda$ is also an eigenvalue of $T'$. According to the Gershgorin disk theorem, there exists $j$ in $\{ 1, \ldots, n \}$ such that
    \[
        \lambda \in \left\{ z : \abs{z - B_{j,j}} < \sum^{n}_{\substack{k=1\\k\ne j}} \abs{B_{j,k}} \right\}.
    \]

    Equivalently
    \[
        \lambda\in \left\{ z : \abs{z - A_{j,j}} < \sum^{n}_{\substack{k=1\\k\ne j}} \abs{A_{k,j}} \right\}.
    \]

    Thus the Gershgorin disk theorem where the radius of the $k$th disk is the sum of the absolute values of the entries in column $k$ of $A$, excluding the diagonal entry still holds.
\end{proof}
\newpage

\section{Commuting Operators}

% chapter5:sectionE:exercise1
\begin{exercise}
    Give an example of two commuting operators $S, T$ on $\mathbb{F}^{4}$ such that there is a subspace of $\mathbb{F}^{4}$ that is invariant under $S$ but not under $T$ and there is a subspace of $\mathbb{F}^{4}$ that is invariant under $T$ but not under $S$.
\end{exercise}

\begin{proof}
    I define $S$ and $T$ as follows:
    \begin{align*}
        S(x_{1}, x_{2}, x_{3}, x_{4}) & = (x_{1}, x_{1} + x_{2}, x_{1} + x_{2} + x_{3}, x_{1} + x_{2} + x_{3} + x_{4}), \\
        T(x_{1}, x_{2}, x_{3}, x_{4}) & = (x_{1}, x_{2} - x_{1}, x_{3} - x_{2}, x_{4} - x_{3}).
    \end{align*}

    Hence $ST = TS = I$. Let
    \[
        U = \{ (x_{1}, x_{2}, 0, 0): x_{1}, x_{2}\in\mathbb{F} \}\qquad V = \{ (0, 0, x_{3}, x_{4}): x_{3}, x_{4}\in\mathbb{F} \}.
    \]

    $U$ is invariant under $S$ but is not invariant under $T$. $V$ is invariant under $T$ but is not invariant under $S$.
\end{proof}
\newpage

% chapter5:sectionE:exercise2
\begin{exercise}
    Suppose $\mathcal{E}$ is a subset of $\lmap{V}$ and every element of $\mathcal{E}$ is diagonalizable. Prove that there exists a basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix if and only if every pair of elements of $\mathcal{E}$ commutes.
\end{exercise}

\begin{proof}
    Because every element of $\mathcal{E}$ is diagonalizable, then each of them has an eigenvalue.

    If there exists a basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix, then every pair of elements of $\mathcal{E}$ commutes because any two diagonal matrices commute.

    To prove the implication of the other direction, I give a proof using mathematical induction on $\dim V$.

    If $\dim V = 1$, every pair of elements of $\mathcal{E}$ commute, and all elements of $\mathcal{E}$ are simultaneously diagonalizable.

    Let $n$ be a positive integer greater than $1$. Assume that the statement is true for every vector space of dimension less than $n$. Let $\dim V = n$.

    If each element of $\mathcal{E}$ has precisely one eigenvalue, then each of them is a multiple of a scalar and the identity operator. So every pair of elements of $\mathcal{E}$ commute, and all elements of $\mathcal{E}$ are simultaneously diagonalizable with respect to any basis of $V$.

    Otherwise, there exists an operator $T\in\mathcal{E}$ such that $T$ have at least two eigenvalues. Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$, then $m\geq 2$. Because $T$ is diagonalizable,
    \[
        V = E(\lambda_{1}, T)\oplus \cdots \oplus E(\lambda_{m}, T).
    \]

    By the induction hypothesis, for each $i\in \{ 1, \ldots, m \}$, for all $S\in\mathcal{E}$, there exists a basis of $E(\lambda_{i}, T)$ consisting of eigenvectors of $S\vert_{E(\lambda_{i}, T)}$. Putting all these bases together gives a basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix.

    So due to the principle of mathematical induction, if every pair of elements of $\mathcal{E}$ commutes, then there exists a basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix.
\end{proof}
\newpage

% chapter5:sectionE:exercise3
\begin{exercise}\label{chapter5:sectionE:exercise3}
    Suppose $S, T\in\lmap{V}$ are such that $ST = TS$. Suppose $p\in \mathscr{P}(\mathbb{F})$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $\kernel{p(S)}$ is invariant under $T$.
        \item Prove that $\range{p(S)}$ is invariant under $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $v\in \kernel{p(S)}$. Then $p(S)v = 0$. Because $S$ and $T$ commute, it follows that $p(S)$ and $T$ commute.
              \[
                  p(S)(Tv) = T(p(S)v) = 0.
              \]

              So $Tv\in\kernel{p(S)}$. Thus $\kernel{p(S)}$ is invariant under $T$.
        \item Let $w\in \range{p(S)}$. Then there exists $v\in V$ such that $p(S)v = w$. Because $S$ and $T$ commute, it follows that $p(S)$ and $T$ commute.
              \[
                  Tw = T(p(S)v) = p(S)(Tv).
              \]

              Hence $Tw\in \range{p(S)}$. Thus $\range{p(S)}$ is invariant under $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter5:sectionE:exercise4
\begin{exercise}
    Prove or give a counterexample: If $A$ is a diagonal matrix and $B$ is an upper-triangular matrix of the same size as $A$, then $A$ and $B$ commute.
\end{exercise}

\begin{proof}
    I give a counterexample.
    \[
        A = \begin{pmatrix}
            1 & 0 \\
            0 & 0
        \end{pmatrix}\qquad
        B = \begin{pmatrix}
            1 & 1 \\
            0 & 0
        \end{pmatrix}
    \]

    Then
    \[
        AB = \begin{pmatrix}
            1 & 1 \\
            0 & 0
        \end{pmatrix}\qquad
        BA = \begin{pmatrix}
            1 & 0 \\
            0 & 0
        \end{pmatrix}.
    \]
\end{proof}
\newpage

% chapter5:sectionE:exercise5
\begin{exercise}
    Prove that a pair of operators on a finite-dimensional vector space commute if and only if their dual operators commute.
\end{exercise}

\begin{proof}
    Let $S$ and $T$ be two linear operators on a finite-dimensional vector space $V$. Let $v_{1}, \ldots, v_{n}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{n}$ is the dual basis.

    Let
    \[
        A = \mathcal{M}(S, (v_{1}, \ldots, v_{n}))\qquad
        B = \mathcal{M}(T, (v_{1}, \ldots, v_{n}))
    \]

    then
    \[
        A^{\top} = \mathcal{M}(S', (\varphi_{1}, \ldots, \varphi_{n}))\qquad
        B^{\top} = \mathcal{M}(T', (\varphi_{1}, \ldots, \varphi_{n}))
    \]

    $S$ and $T$ commute if and only if $A$ and $B$ commute. $S'$ and $T'$ commute if and only if $A^{\top}$ and $B^{\top}$ commute. $A$ and $B$ commute if and only if $A^{\top}$ and $B^{\top}$ commute, which is deduced from
    \[
        {(AB)}^{\top} = B^{\top}A^{\top}\qquad {(BA)}^{\top} = A^{\top}B^{\top}.
    \]

    Thus $S$ and $T$ commute if and only if $S'$ and $T'$ commute.
\end{proof}
\newpage

% chapter5:sectionE:exercise6
\begin{exercise}
    Suppose $V$ is a finite-dimensional complex vector space and $S, T\in\lmap{V}$ commute. Prove that there exist $\alpha, \lambda\in\mathbb{C}$ such that
    \[
        \range{(S - \alpha I)} + \range{(T - \lambda I)} \ne V.
    \]
\end{exercise}

\begin{quote}[Additional notes]
    It is very tempting to use a common eigenvector $v$ of $S, T$ and let $\alpha$ be the eigenvalue of $S$ corresponding to $S$, $\lambda$ be the eigenvalue of $T$ corresponding to $T$. However, this approach does not seem to work.
\end{quote}

\begin{proof}
    $n = \dim V$.

    Because $S, T$ commute and they are operators on a finite-dimensional complex vector space, there exists a basis $v_{1}, \ldots, v_{n}$ of $V$ to which the matrices of $S, T$ are upper triangular.

    Let $A = \mathcal{M}(S, (v_{1}, \ldots, v_{n})), B = \mathcal{M}(T, (v_{1}, \ldots, v_{n}))$ and choose $\alpha = A_{n,n}, \lambda = B_{n,n}$, then the last row of $A - \alpha I$ and that of $B - \lambda I$ contain only $0$'s. So
    \[
        \begin{split}
            \range{(S - \alpha I)}\subseteq \operatorname{span}(v_{1}, \ldots, v_{n-1}), \\
            \range{(T - \lambda I)}\subseteq \operatorname{span}(v_{1}, \ldots, v_{n-1}).
        \end{split}
    \]

    Therefore $\range{(S - \alpha I)} + \range{(T - \lambda I)} \subseteq \operatorname{span}(v_{1}, \ldots, v_{n-1}) \subsetneq V$.
\end{proof}
\newpage

% chapter5:sectionE:exercise7
\begin{exercise}
    Suppose $V$ is a complex vector space, $S \in \lmap{V}$ is diagonalizable, and $T\in\lmap{V}$ commutes with $S$. Prove that there is a basis of $V$ such that $S$ has a diagonal matrix with respect to this basis and $T$ has an upper-triangular matrix with respect to this basis.
\end{exercise}

\begin{proof}
    Let $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $S$. Because $S$ is diagonalizable, it follows that
    \[
        V = E(\lambda_{1}, S) \oplus \cdots \oplus E(\lambda_{m}, S).
    \]

    For each $i\in\{1, \ldots, m\}$, $S\vert_{E(\lambda_{i}, S)}$ and $T\vert_{E(\lambda_{i}, S)}$ commute, so there exists a basis of $E(\lambda_{i}, S)$ with respect to which $S$ and $T$ have upper-triangular matrix. Putting all these bases gives a basis of $V$ with respect to which $S$ and $T$ have upper-triangular matrix. Moreover, these vectors are eigenvectors of $S$, so the matrix of $S$ with respect to this basis is not just upper-triangular matrix but also diagonal.

    Thus there exists a basis of $V$ such that $S$ has a diagonal matrix and $T$ has an upper-triangular matrix with respect to this basis.
\end{proof}
\newpage

% chapter5:sectionE:exercise8
\begin{exercise}
    Suppose $m = 3$ in Example 5.72 and $D_{x}$, $D_{y}$ are the commuting partial
    differentiation operators on $\mathscr{P}_{3}(\mathbb{R}^{2})$ from that example. Find a basis of $\mathscr{P}_{3}(\mathbb{R}^{2})$ with respect to which $D_{x}$ and $D_{y}$ each have an upper-triangular matrix.
\end{exercise}

\begin{proof}
    I choose the following basis
    \[
        1, x, y, x^{2}, y^{2}, xy, x^{2}y, xy^{2}, x^{3}, y^{3}.
    \]

    \[
        \mathcal{M}(D_{x}) = \begin{pmatrix}
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 2 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 3 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
        \end{pmatrix}
    \]

    \[
        \mathcal{M}(D_{y}) = \begin{pmatrix}
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 2 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 3 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 2 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0
        \end{pmatrix}
    \]

    The matrices of $D_{x}$ and $D_{y}$ with respect to this basis are upper triangular.
\end{proof}
\newpage

% chapter5:sectionE:exercise9
\begin{exercise}\label{chapter5:sectionE:exercise9}
    Suppose $V$ is a finite-dimensional nonzero complex vector space. Suppose that $E \subseteq \lmap{V}$ is such that $S$ and $T$ commute for all $S, T\in \mathcal{E}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that there is a vector in $V$ that is an eigenvector for every element of $\mathcal{E}$.
        \item Prove that there is basis of $V$ with respect to which every element of $\mathcal{E}$ has an upper-triangular matrix.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Unsolved.
\end{proof}
\newpage

% chapter5:sectionE:exercise10
\begin{exercise}
    Give an example of two commuting operators $S$, $T$ on a finite-dimensional real vector space such that $S + T$ has a eigenvalue that does not equal an eigenvalue of $S$ plus an eigenvalue of $T$ and $ST$ has a eigenvalue that does not equal an eigenvalue of $S$ times an eigenvalue of $T$.
\end{exercise}

\begin{proof}
    I define $S, T$ on $\lmap{\mathbb{R}^{2}}$ as follows:
    \[
        S(x, y) = (y, -x)\qquad T(x, y) = (-y, x)
    \]

    Then
    \begin{align*}
        (ST)(x, y) & = S(-y, x) = (x, y) \\
        (TS)(x, y) & = T(y, -x) = (x, y)
    \end{align*}

    so $ST = TS = I$. $S$ and $T$ have no (real) eigenvalue. On the other hand, $S + T = 0$ and $ST = I$. $0$ is the only eigenvalue of $S + T$, $1$ is the only eigenvalue of $ST$.
\end{proof}
\newpage
