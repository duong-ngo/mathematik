\chapter{Linear Maps}

\section{Vector Space of Linear Maps}

% chapter3:sectionA:exercise1
\begin{exercise}
    Suppose $b, c\in\mathbb{R}$. Define $T: \mathbb{R}^{3}\to \mathbb{R}^{2}$ by
    \[
        T(x, y, z) = (2x - 4y + 3z + b, 6x + cxyz).
    \]

    Show that $T$ is linear if and only if $b = c = 0$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionA:exercise2
\begin{exercise}
    Suppose $b, c \in \mathbb{R}$. Define $T: \mathcal{P}(\mathbb{R})\to \mathbb{R}^{2}$ by
    \[
        Tp = \left( 3p(4) + 5p'(6) + bp(1)p(2), \int^{2}_{-1}x^{3}p(x)dx + c \sin p(0) \right)
    \]

    Show that $T$ is linear if and only if $b = c = 0$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionA:exercise3
\begin{exercise}
    Suppose that $T\in \mathcal{L}(\mathbb{F}^{n}, \mathbb{F}^{m})$. Show that there exist scalars $A_{j,k}\in\mathbb{F}$ for $j = 1, \ldots, m$ and $k = 1, \ldots, n$ such that
    \[
        T(x_{1}, \ldots, x_{n}) = (A_{1,1}x_{1} + \cdots + A_{1,n}x_{n}, \ldots, A_{m,1}x_{1} + \cdots + A_{m,n}x_{n})
    \]

    for every $(x_{1}, \ldots, x_{n})\in\mathbb{F}^{n}$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be the standard basis of $\mathbb{F}^{n}$ and let $T(e_{i}) = (A_{1,i}, \ldots, A_{m, i})$ for all $i\in\{ 1, 2, \ldots, n \}$. Hence
    \[
        T(x_{1}, \ldots, x_{n}) = (A_{1,1}x_{1} + \cdots + A_{1,n}x_{n}, \ldots, A_{m,1}x_{1} + \cdots + A_{m,n}x_{n})
    \]

    for every $(x_{1}, \ldots, x_{n})\in\mathbb{F}^{n}$.
\end{proof}
\newpage

% chapter3:sectionA:exercise4
\begin{exercise}
    Suppose $T\in \mathcal{L}(V, W)$ and $v_{1}, \ldots, v_{m}$ is a list of vectors in $V$ such that $Tv_{1}, \ldots, Tv_{m}$ is a linearly independent list in $W$. Prove that $v_{1}, \ldots, v_{m}$ is linearly independent.
\end{exercise}

\begin{proof}
    Let $x_{1}v_{1} + \cdots + x_{n}v_{n} = 0$ be a linear combination of $0$ in $V$.
    \[
        0 = T(0) = T(x_{1}v_{1} + \cdots + x_{n}v_{n}) = x_{1}Tv_{1} + \cdots + x_{n}Tv_{n}.
    \]

    Since $Tv_{1}, \ldots, Tv_{n}$ is linearly independent, then $x_{1}, \ldots, x_{n}$ are all zero. Hence $v_{1}, \ldots, v_{n}$ is linearly independent.
\end{proof}
\newpage

% chapter3:sectionA:exercise5
\begin{exercise}
    Prove that $\mathcal{L}(V, W)$ is a vector space, as was asserted in 3.6.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionA:exercise6
\begin{exercise}
    Prove that multiplication of linear maps has the associative, identity, and distributive properties asserted in 3.8.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionA:exercise7
\begin{exercise}
    Show that every linear map from a one-dimensional vector space to itself is
    multiplication by some scalar. More precisely, prove that if $\dim V = 1$ and $T\in \mathcal{L}(V)$, then there exists $\lambda\in\mathbb{F}$ such that $Tv = \lambda v$ for all $v\in V$.
\end{exercise}

\begin{proof}
    Let $e$ be a basis of $V$. Then for every $v\in V$, there exists uniquely $x\in\mathbb{F}$ such that $v = xe$, and there exists uniquely $\lambda\in\mathbb{F}$ such that $Te = \lambda e$.
    \[
        Tv = T(xe) = xTe = x(\lambda e) = (x\lambda)e = (\lambda x)e = \lambda(xe) = \lambda v.
    \]

    Hence the result follows.
\end{proof}
\newpage

% chapter3:sectionA:exercise8
\begin{exercise}
    Give an example of a function $\varphi: \mathbb{R}^{2}\to \mathbb{R}$ such that
    \[
        \varphi(av) = a\varphi(v)
    \]

    for all $a\in\mathbb{R}$ and all $v\in\mathbb{R}^{2}$ but $\varphi$ is not linear.
\end{exercise}

\begin{proof}
    $\varphi: (x, y) \mapsto \sqrt[3]{x^{3} + y^{3}}$ satisfies
    \[
        \varphi(ax, ay) = a\varphi(x, y)
    \]

    but $\varphi$ is not linear.
\end{proof}
\newpage

% chapter3:sectionA:exercise9
\begin{exercise}
    Give an example of a function $\varphi: \mathbb{C}\to \mathbb{C}$ such that
    \[
        \varphi(w + z) = \varphi(w) + \varphi(z)
    \]

    for all $w, z\in\mathbb{C}$ but $\varphi$ is not linear. (Here $\mathbb{C}$ is thought of as a complex vector space.)
\end{exercise}

\begin{proof}
    $\varphi: \mathbb{C}\to \mathbb{C}$ defined by
    \[
        \varphi(z) = \bar{z}
    \]

    satisfies $\varphi(z + w) = \varphi(z) + \varphi(w)$ but is not linear.
\end{proof}
\newpage

% chapter3:sectionA:exercise10
\begin{exercise}
    Prove or give a counterexample: If $q\in \mathcal{P}(\mathbb{R})$ and $T: \mathcal{P}(\mathbb{R})\to \mathcal{P}(\mathbb{R})$ is defined by $Tp = q\circ p$, then $T$ is a linear map.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $q(x) = 1$. $Tp = q\circ p$ then $(Tp)(x) = q(p(x)) = 1$ for all $x\in\mathbb{R}$. $T$ is not a linear map because
    \[
        (T(p_{1} + p_{2}))(x) = 1\ne 1 + 1 = (Tp_{1})(x) + (Tp_{2})(x)
    \]

    for all $x\in\mathbb{R}$.
\end{proof}
\newpage

% chapter3:sectionA:exercise11
\begin{exercise}\label{chapter3:sectionA:exercise11}
    Suppose $V$ is finite-dimensional and $T\in\mathcal{L}(V)$. Prove that $T$ is a scalar multiple of the identity if and only if $ST = TS$ for every $S\in \mathcal{L}(V)$.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$ and $Tv_{i}$ be $x_{i,1}v_{1} + \cdots + x_{i,n}v_{n}$ for each $i\in\{ 1, \ldots, n \}$.

    Due to the linear map lemma, for each $i\in \{ 1, \ldots, n \}$, there exists $S_{i}$ from $V$ to $V$ such that $S_{i}v_{i} = v_{i}$ and $S_{i}v_{j} = 0$ for every $j\ne i$.
    \begin{align*}
        (S_{i}T)(v_{i}) & = S_{i}(x_{i,1}v_{1} + \cdots + x_{i,n}v_{n}) = x_{i,i}v_{i}     \\
        (TS_{i})(v_{i}) & = T(S_{i}v_{i}) = Tv_{i} = x_{i,1}v_{1} + \cdots + x_{i,n}v_{n}.
    \end{align*}

    Because $S_{i}T = TS_{i}$, and $v_{1}, \ldots, v_{n}$ is linearly independent, from the two formulas above, we deduce that $x_{i,j} = 0$ for every $j\ne i$.

    Due to the linear map lemma, for every pair $i\ne j$ and $i, j\in \{ 1, \ldots, n \}$, there exists a linear map $S_{i,j} = S_{j,i}$ from $V$ to $V$ such that $S_{i,j}v_{i} = v_{j}$, $S_{i,j}v_{j} = v_{i}$ and $S_{i,j}v_{k} = 0$ where $k\ne i, j$.
    \begin{align*}
        (S_{i,j}T)(v_{i}) & = S_{i,j}(x_{i,i}v_{i}) = x_{i,i}v_{j} \\
        (TS_{i,j})(v_{i}) & = Tv_{j} = x_{j,j}v_{j}
    \end{align*}

    Because $S_{i,j}T = TS_{i,j}$, and $v_{1}, \ldots, v_{n}$ is linearly independent, from the two formulas above, we deduce that $x_{i,i} = x_{j,j}$.

    So for every $i\ne j$ in $\{1, \ldots, n\}$, $x_{i,j} = 0$ and $x_{i,i} = x_{j,j}$. Let $x_{1,1} = \lambda$, then $Tv_{i} = \lambda v_{i}$ for every $i\in\{1, \ldots, n\}$. Let $v = x_{1}v_{1} + \cdots + x_{n}v_{n}$ be an arbitrary vector in $V$, then
    \[
        Tv = x_{1}Tv_{1} + \cdots + x_{n}Tv_{n} = \lambda (x_{1}v_{1} + \cdots + x_{n}v_{n}) = \lambda v.
    \]

    Thus $T$ is a scalar multiple of the identity map.
\end{proof}
\newpage

% chapter3:sectionA:exercise12
\begin{exercise}
    Suppose $U$ is a subspace of $V$ with $U\ne V$. Suppose $S\in \mathcal{L}(V, W)$ and $S \ne 0$ (which means that $Su \ne 0$ for some $u \in U$). Define $T : V \to W$ by
    \[
        Tv = \begin{cases}
            Sv & \text{if $v\in U$},                 \\
            0  & \text{if $v\in V$ and $v\notin U$.}
        \end{cases}
    \]

    Prove that $T$ is not a linear map on $V$.
\end{exercise}

\begin{proof}
    Let $u$ be a vector in $U$ such that $Su\ne 0$, $w$ be a vector in $V$ such that $w\notin U$. Then $u + w\notin U$. Due to the definition of $T$, $T(u + w) = 0$. On the other hand, $Tu + Tw = Su + 0 = Su\ne 0$. Therefore $T(u + w)\ne Tu + Tw$, so $T$ is not a linear map on $V$.
\end{proof}
\newpage


% chapter3:sectionA:exercise13
\begin{exercise}\label{chapter3:sectionA:exercise13}
    Suppose $V$ is finite-dimensional. Prove that every linear map on a subspace of $V$ can be extended to a linear map on $V$. In other words, show that if $U$ is a subspace of $V$ and $S \in \mathcal{L}(U, W)$, then there exists $T\in \mathcal{L}(V, W)$ such that $Tu = Su$ for all $u\in U$.
\end{exercise}

\begin{proof}
    Let $u_{1}, \ldots, u_{m}$ be a basis of $U$.

    Since $V$ is finite-dimensional, we can extend the list $u_{1}, \ldots, u_{m}$ to $u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{n}$ such that the new list is a basis of $V$.

    We define a linear map $T$ from $V$ to $W$ by the images of every vector within $u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{n}$: $Tu_{1} = Su_{1}$, \ldots, $Tu_{m} = Su_{m}$, $Tv_{1}$ is some vector in $W$, \ldots, $Tv_{n}$ is some vector in $W$. According to this construction, the linear map $T$ is indeed an extension of the linear map $S$.
\end{proof}
\newpage

% chapter3:sectionA:exercise14
\begin{exercise}
    Suppose $V$ is finite-dimensional with $\dim V > 0$, and suppose $W$ is infinite-dimensional. Prove that $\mathcal{L}(V, W)$ is infinite-dimensional.
\end{exercise}

\begin{proof}
    According to Exercise~\ref{chapter2:sectionA:exercise17}, there exists a sequence of vectors $w_{1}, w_{2}, \ldots$ in $W$ such that for every positive integer $n$, the list $w_{1}, w_{2}, \ldots, w_{n}$ is linearly independent.

    Let $m = \dim V$ and $v_{1}, \ldots, v_{m}$ a basis of $V$.

    For each positive integer $n$, let $T_{n}$ be a linear map from $V$ to $W$ such that $T_{n}v_{i} = 0$ for every $i\ne 1$ and $T_{n}v_{1} = w_{n}$. Let $\lambda_{1}T_{1} + \lambda_{2}T_{2} + \cdots + \lambda_{n}T_{n} = 0$ be a linear combination of $0$, from this we deduce that
    \[ \lambda_{1}T_{1}v_{1} + \lambda_{2}T_{2}v_{1} + \cdots + \lambda_{n}T_{n}v_{1} = 0v_{1} = 0 \]

    and the left-hand side is reduced to $\lambda_{1}w_{1} + \lambda_{2}w_{2} + \cdots + \lambda_{n}w_{n} = 0$. Because $w_{1}, \ldots, w_{n}$ is linearly independent, so $\lambda_{1} = \lambda_{2} = \cdots = \lambda_{n} = 0$. Therefore $T_{1}, \ldots, T_{n}$ is linearly independent. Once again, according to Exercise~\ref{chapter2:sectionA:exercise17}, $\mathcal{L}(V, W)$ is infinite-dimensional.
\end{proof}
\newpage

% chapter3:sectionA:exercise15
\begin{exercise}
    Suppose $v_{1} , \ldots, v_{m}$ is a linearly dependent list of vectors in $V$. Suppose also that $W \ne \{0\}$. Prove that there exist $w_{1} , \ldots, w_{m} \in W$ such that no $T \in \mathcal{L}(V, W)$ satisfies $Tv_{k} = w_{k}$ for each $k = 1, \ldots, m$.
\end{exercise}

\begin{proof}
    Because $v_{1} , \ldots, v_{m}$ is a linearly dependent list, there exist scalars $\lambda_{1}, \ldots, \lambda_{m}$ which are not all zero such that $\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m} = 0$. Without loss of generality, assume that $\lambda_{1}\ne 0$.

    Since $W\ne \{0\}$, there exists a non-zero vector $w_{1}$ in $W$. Let $w_{2} = \cdots = w_{m} = 0$ (they are all zero vector), then $\lambda_{1}w_{1} + \lambda_{2}w_{2} + \cdots + \lambda_{m}w_{k} = \lambda_{1}w_{1}\ne 0$. There does not exist a linear map $T$ from $V$ to $W$ such that $Tv_{k} = w_{k}$ for each $k = 1,\ldots, m$, because $0 = T(\lambda_{1}v_{1} + \lambda_{2}v_{2} + \cdots + \lambda_{m}v_{k}) = \lambda_{1}w_{1} + \lambda_{2}w_{2} + \cdots + \lambda_{m}w_{k} \ne 0$.

    Hence there exist $w_{1}, \ldots, w_{m}\in W$ such that no $T\in\mathcal{L}(V, W)$ satisfies $Tv_{k} = w_{k}$ for each $k = 1, \ldots, m$.
\end{proof}
\newpage

% chapter3:sectionA:exercise16
\begin{exercise}
    Suppose $V$ is finite-dimensional with $\dim V > 1$. Prove that there exist $S, T \in \mathcal{L}(V)$ such that $ST \ne TS$.
\end{exercise}

\begin{proof}
    Because $V$ is finite-dimensional and $\dim V > 1$, there exist two vectors $v_{1}, v_{2}\in V$ such that they are linearly independent. Let $S', T'\in\mathcal{L}(\operatorname{span}(v_{1}, v_{2}))$ such that $S'v_{1} = v_{2}, S'v_{2} = v_{1}$ and $T'v_{1} = v_{1}, T'v_{2} = 0$. According to Exercise~\ref{chapter3:sectionA:exercise13}, $S'$ and $T'$ can both be extended to linear maps $S$ and $T$ in $\mathcal{L}(V)$, respectively.
    \begin{align*}
        (ST)(v_{1}) & = S(Tv_{1}) = Sv_{1} = v_{2} \\
        (TS)(v_{1}) & = T(Sv_{1}) = Tv_{2} = 0
    \end{align*}

    So $ST\ne TS$. Hence there exists two linear maps in $\mathcal{L}(V)$ which are not commutative.
\end{proof}
\newpage

% chapter3:sectionA:exercise17
\begin{exercise}\label{chapter3:sectionA:exercise17}
    Suppose $V$ is finite-dimensional. Show that the only two-sided ideals of $\mathcal{L}(V)$ are $\{0\}$ and $\mathcal{L}(V)$.
        [A subspace $\mathcal{E}$ of $\mathcal{L}(V)$ is called a \textbf{two-sided ideal} of $\mathcal{L}(V)$ if $TE \in \mathcal{E}$ and $ET \in \mathcal{E}$ for all $E \in \mathcal{E}$ and all $T \in \mathcal{L}(V)$.]
\end{exercise}

\begin{proof}
    Suppose that $\mathcal{E}$ is a two-sided ideal of $\mathcal{L}(V)$ and $\mathcal{E}\ne \{0\}$. I will show that the identity map $\operatorname{id}_{V}$ is in $\mathcal{E}$.

    Since $\mathcal{E}\ne \{0\}$, there exists $E\in\mathcal{E}$ such that $E$ is not the zero map. $E$ is not the zero map, so there exists a vector $v\ne 0$ such that $Ev\ne 0$. Let $v_{1}, \ldots, v_{n}$ be a basis of $V$. Since $v = x_{1}v_{1} + \cdots + x_{n}v_{n}$ for some scalars $x_{1}, \ldots, x_{n}$ and $v\ne 0$, there exists positive integer $k$ such that $x_{k}\ne 0$, then $v_{1}, \ldots, v_{k-1}, v, v_{k+1}, \ldots, v_{n}$ is also a basis of $V$. Therefore, without loss of generality, we can assume $v = v_{1}$. Let $w_{1} = Ev_{1}$. Similarly there exists a basis $w_{1}, \ldots, w_{n}$.

    Let's define linear maps $R_{i}$ and $S_{i}$ for every positive integer $i\in [\![ 1, n ]\!]$ as follows:
    \[
        R_{i}v_{k} = \begin{cases}
            v_{i} & \text{if $k = i$}  \\
            0     & \text{if $k\ne i$}
        \end{cases}
        \qquad
        S_{i}w_{k} = \begin{cases}
            v_{i} & \text{if $k = i$}  \\
            0     & \text{if $k\ne i$}
        \end{cases}
    \]

    For every positive integer $k\in [\![ 1, n ]\!]$
    \[
        \sum^{n}_{i=1}(S_{i}ER_{i})(v_{k}) = (S_{k}E)(v_{k}) = S_{k}(Ev_{k}) = S_{k}w_{k} = v_{k}.
    \]

    So $\sum^{n}_{i=1}S_{i}ER_{i}$ is the identity map $\operatorname{id}_{V}$. Since $\operatorname{id}_{V} = S_{i}ER_{i}\in\mathcal{E}$ for every positive integer $i\in [\![ 1, n ]\!]$, $\operatorname{id}_{V}\in\mathcal{E}$. Moreover, for every linear map $T$, $\operatorname{id}_{V}T\in\mathcal{E}$, which means $T\in\mathcal{E}$ for every $T\in\mathcal{L}(V)$. Therefore $\mathcal{E} = \mathcal{L}(V)$.

    Hence the only subspaces of $\mathcal{L}(V)$, which are two-side ideals, are $\{0\}$ and $\mathcal{L}(V)$.
\end{proof}
\newpage

\section{Null Spaces and Ranges}

% chapter3:sectionB:exercise1
\begin{exercise}
    Give an example of a linear map $T$ with $\dim \kernel{T} = 3$ and $\dim \range{T} = 2$.
\end{exercise}

\begin{proof}
    Here is one example. $T: \mathbb{R}^{3}\to \mathbb{R}^{3}$, where $T((x, y, z)) = (x, y, 0)$.
\end{proof}
\newpage

% chapter3:sectionB:exercise2
\begin{exercise}
    Suppose $S, T\in \mathcal{L}(V)$ are such that $\range{S}\subseteq \kernel{T}$. Prove that ${(ST)}^{2} = 0$.
\end{exercise}

\begin{proof}
    Let $v$ be a vector in $V$. ${(ST)}^{2}(v) = S(T(STv))$. Because $STv\in\range{S}\subseteq \kernel{T}$, then $T(STv) = 0$, and $S(T(STv)) = 0$. Hence ${(ST)}^{2} = 0$.
\end{proof}
\newpage

% chapter3:sectionB:exercise3
\begin{exercise}
    Suppose $v_{1}, \ldots, v_{m}$ is a list of vectors in $V$. Define $T\in \mathcal{L}(\mathbb{F}^{m}, V)$ by
    \[ T(z_{1}, \ldots, z_{m}) = z_{1}v_{1} + \cdots + z_{m}v_{m}. \]

    \begin{enumerate}[label={(\alph*)}]
        \item What property of $T$ corresponds to $v_{1}, \ldots, v_{m}$ spanning $V$?
        \item What property of $T$ corresponds to the list $v_{1}, \ldots, v_{m}$ being linearly independent?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Surjectivity of $T$.
        \item Injectivity of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionB:exercise4
\begin{exercise}
    Show that $\{T \in \mathcal{L}(\mathbb{R}^{5}, \mathbb{R}^{4} ): \dim \kernel{T} > 2\}$ is not a subspace of $\mathcal{L}(\mathbb{R}^{5}, \mathbb{R}^{4})$.
\end{exercise}

\begin{proof}
    Let $S, T$ be linear maps in $\mathcal{L}(\mathbb{R}^{5}, \mathbb{R}^{4})$ such that
    \begin{align*}
        S(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) & = (x_{1}, x_{2}, 0, 0), \\
        T(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) & = (0, 0, x_{3}, x_{4}).
    \end{align*}

    According to this definition and the fundamental theorem of linear maps, $\dim \kernel{S} = \dim \kernel{T} = 5 - 2 = 3 > 2$. However
    \[
        (S + T)(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}) = (x_{1}, x_{2}, x_{3}, x_{4})
    \]

    and $\dim\kernel{(S+T)} = 5 - 4 = 1 < 2$. So $\{T \in \mathcal{L}(\mathbb{R}^{5}, \mathbb{R}^{4} ): \dim \kernel{T} > 2\}$ is not closed under addition, so it is not a subspace of $\mathcal{L}(\mathbb{R}^{5}, \mathbb{R}^{4})$.
\end{proof}
\newpage

% chapter3:sectionB:exercise5
\begin{exercise}
    Give an example of $T \in \mathcal{L}(\mathbb{R}^{4})$ such that $\range{T} = \kernel{T}$.
\end{exercise}

\begin{proof}
    Let $T$ be a linear map in $\mathcal{L}(\mathbb{R}^{4})$ such that $T((x_{1}, x_{2}, x_{3}, x_{4})) = (x_{1} - x_{2}, x_{1} - x_{2}, x_{3} - x_{4}, x_{3} - x_{4})$.

    Then $\kernel{T} = \{ (a, a, b, b): a, b\in \mathbb{R} \}$, $\range{T} = \{ (a, a, b, b): a, b\in \mathbb{R} \}$.
\end{proof}
\newpage

% chapter3:sectionB:exercise6
\begin{exercise}
    Prove that there does not exist $T \in \mathcal{L}(\mathbb{R}^{5})$ such that $\range{T} = \kernel{T}$.
\end{exercise}

\begin{proof}
    According to the fundamental theorem of linear maps, $\dim \mathbb{R}^{5} = \dim\kernel{T} + \dim\range{T}$, so $5 = \dim\kernel{T} + \dim\range{T}$. Since $5$ is odd, then $\dim\kernel{T}\ne \dim\range{T}$, which means $\range{T}\ne \kernel{T}$.
\end{proof}
\newpage

% chapter3:sectionB:exercise7
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional with $2 \leq \dim V \leq \dim W$. Show that $\{T \in \mathcal{L}(V, W) : T \text{ is not injective} \}$ is not a subspace of $\mathcal{L}(V, W)$.
\end{exercise}

\begin{proof}
    Let $v_{1}, v_{2}, \ldots, v_{n}$ be a basis of $V$, $w_{1}, w_{2}, \ldots, w_{n}, w_{n+1}, \ldots, w_{n+m}$ be a basis of $W$. According to the hypothesis, $n\geq 2, m\geq 0$.

    I define two linear maps $S, T$ in $\mathcal{L}(V, W)$ as follows:
    \[
        Sv_{k} = \begin{cases}
            w_{1} & \text{if $k = 1$}, \\
            0     & \text{otherwise}
        \end{cases}
        \qquad
        Tv_{k} = \begin{cases}
            0     & \text{if $k = 1$}, \\
            w_{k} & \text{otherwise}
        \end{cases}
    \]

    By this definition, $S, T$ are not injective. However, $S + T$ is injective. Hence $\{T \in \mathcal{L}(V, W) : T \text{ is not injective} \}$ is not a subspace of $\mathcal{L}(V, W)$, because it is not closed under addition.
\end{proof}
\newpage

% chapter3:sectionB:exercise8
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional with $\dim V \geq \dim W \geq 2$. Show
    that $\{T \in \mathcal{L}(V, W) : T \text{ is not surjective}\}$ is not a subspace of $\mathcal{L}(V, W)$.
\end{exercise}

\begin{proof}
    Let $w_{1}, w_{2}, \ldots, w_{n}$ be a basis of $W$, $v_{1}, v_{2}, \ldots, v_{n}, v_{n+1}, \ldots, v_{n+m}$ be a basis of $V$. According to the hypothesis, $n\geq 2, m\geq 0$.

    I define two linear maps $S, T$ in $\mathcal{L}(V, W)$ as follows:
    \[
        Sv_{k} = \begin{cases}
            w_{1} & \text{if $k = 1$}, \\
            0     & \text{otherwise}
        \end{cases}
        \qquad
        Tv_{k} = \begin{cases}
            w_{k} & \text{if $1 < k\leq n$}, \\
            0     & \text{otherwise}
        \end{cases}
    \]

    By this definition, $S, T$ are not surjective. However,
    \[
        (S+T)(v_{k}) = \begin{cases}
            w_{k} & \text{if $1\leq k\leq n$}, \\
            0     & \text{otherwise}
        \end{cases}
    \]

    which means $S + T$ is surjective. Hence $\{T \in \mathcal{L}(V, W) : T \text{ is not surjective} \}$ is not a subspace of $\mathcal{L}(V, W)$, because it is not closed under addition.
\end{proof}
\newpage

% chapter3:sectionB:exercise9
\begin{exercise}\label{chapter3:sectionB:exercise9}
    Suppose $T \in \mathcal{L}(V, W)$ is injective and $v_{1}, \ldots, v_{n}$ is linearly independent in $V$. Prove that $Tv_{1} , \ldots, Tv_{n}$ is linearly independent in $W$.
\end{exercise}

\begin{proof}
    Suppose that $x_{1}Tv_{1} + \cdots + x_{n}Tv_{n} = 0$ is a linear combination of $0$. Then $T(x_{1}v_{1} + \cdots + x_{n}v_{n}) = 0$. Because $T$ is injective, it follows that $\kernel{T} = \{0\}$ and then $x_{1}v_{1} + \cdots + x_{n}v_{n} = 0$. Since $v_{1}, \ldots, v_{n}$ is linearly independent in $V$, $x_{1} = \cdots = x_{n} = 0$. Hence $Tv_{1} , \ldots, Tv_{n}$ is linearly independent in $W$.
\end{proof}
\newpage

% chapter3:sectionB:exercise10
\begin{exercise}
    Suppose $v_{1} ,\ldots, v_{n}$ spans $V$ and $T \in \mathcal{L}(V, W)$. Show that $Tv_{1} , \ldots, Tv_{n}$ spans $\range{T}$.
\end{exercise}

\begin{proof}
    Let $w$ be a vector in $\range{T}$, then there exists a vector $v\in V$ such that $Tv = w$. Because $v_{1} ,\ldots, v_{n}$ spans $V$, there exists scalars $x_{1}, \ldots, x_{n}$ such that $v = x_{1}v_{1} + \cdots + x_{n}v_{n}$. Then $w = Tv = x_{1}Tv_{1} + \cdots + x_{n}Tv_{n}$. Hence $Tv_{1}, \ldots, Tv_{n}$ spans $\range{T}$.
\end{proof}
\newpage

% chapter3:sectionB:exercise11
\begin{exercise}\label{chapter3:sectionB:exercise11}
    Suppose that $V$ is finite-dimensional and that $T \in \mathcal{L}(V, W)$. Prove that there exists a subspace $U$ of $V$ such that
    \[
        U \cap \kernel{T} = \{ 0 \}     \quad\text{and}\quad \range{T} = \{ Tu: u\in U \}.
    \]
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $\kernel{T}$. We extend this list to create a basis of $V$, and let it be
    \[ v_{1}, \ldots, v_{n}, v_{n+1}, \ldots, v_{n+m}. \]

    Let $U = \operatorname{span}(v_{n+1}, \ldots, v_{n+m})$, then $U\cap \kernel{T} = \{0\}$, and $V = U\oplus \kernel{T}$. Let $v$ be a vector in $V$. According to the definition of direct sum of subspaces, there exist uniquely two vectors $v_{0}\in\kernel{T}$ and $u\in U$ such that $v = v_{0} + u$. $Tv = Tv_{0} + Tu = Tu$. Therefore $\range{T} = \{ Tu: u\in U \}$.

    Hence $U = \operatorname{span}(v_{n+1}, \ldots, v_{n+m})$ is what we want to construct in this problem. $U$ is also a linear complement of $\kernel{T}$ in $V$.
\end{proof}
\newpage

% chapter3:sectionB:exercise12
\begin{exercise}
    Suppose $T$ is a linear map from $\mathbb{F}^{4}$ to $\mathbb{F}^{2}$ such that
    \[
        \kernel{T} = \{ (x_{1}, x_{2}, x_{3}, x_{4})\in\mathbb{F}^{4} : x_{1} = 5x_{2} \text{ and } x_{3} = 7x_{4} \}.
    \]

    Prove that $T$ is surjective.
\end{exercise}

\begin{proof}
    $\kernel{T} = \{ (5x_{2}, x_{2}, 7x_{4}, x_{4}) \}$. $(5x_{2}, x_{2}, 7x_{4}, x_{4}) = x_{2}(5, 1, 0, 0) + x_{4}(0, 0, 7, 1)$. $(5, 1, 0, 0)$ and $(0, 0, 7, 1)$ is a basis of $\kernel{T}$, so $\dim\kernel{T} = 2$. According to the fundamental theorem of linear maps, $\dim\mathbb{F}^{4} = \dim\kernel{T} + \dim\range{T}$, then $\dim\range{T} = 4 - 2 = 2 = \dim\mathbb{F}^{2}$. Therefore, $\range{T} = \mathbb{F}^{2}$, which implies $T$ is surjective.
\end{proof}
\newpage

% chapter3:sectionB:exercise13
\begin{exercise}
    Suppose $U$ is a three-dimensional subspace of $\mathbb{R}^{8}$ and that $T$ is a linear map from $\mathbb{R}^{8}$ to $\mathbb{R}^{5}$ such that $\kernel{T}= U$. Prove that $T$ is surjective.
\end{exercise}

\begin{proof}
    According to the fundamental theorem of linear maps, $\dim\mathbb{R}^{8} = \dim\kernel{T} + \dim\range{T}$, so $\dim\range{T} = 8 - \dim\kernel{T} = 8 - \dim U = 8 - 3 = 5 = \dim\mathbb{R}^{5}$. Therefore, $\range{T} = \mathbb{R}^{5}$, which implies $T$ is surjective.
\end{proof}
\newpage

% chapter3:sectionB:exercise14
\begin{exercise}
    Prove that there does not exist a linear map from $\mathbb{F}^{5}$ to $\mathbb{F}^{2}$ whose null space equals $\{(x_{1} , x_{2} , x_{3} , x_{4} , x_{5} ) \in \mathbb{F}^{5} : x_{1} = 3x_{2} \text{ and } x_{3} = x_{4} = x_{5} \}$.
\end{exercise}

\begin{proof}
    $\{(x_{1} , x_{2} , x_{3} , x_{4} , x_{5} ) \in \mathbb{F}^{5} : x_{1} = 3x_{2} \text{ and } x_{3} = x_{4} = x_{5} \} = \{ (3x_{2}, x_{2}, x_{3}, x_{3}, x_{3}) \}$.
    \[
        (3x_{2}, x_{2}, x_{3}, x_{3}, x_{3}) = x_{2}(3, 1, 0, 0, 0) + x_{3}(0, 0, 1, 1, 1)
    \]

    so the dimension of $\{(x_{1} , x_{2} , x_{3} , x_{4} , x_{5} ) \in \mathbb{F}^{5} : x_{1} = 3x_{2} \text{ and } x_{3} = x_{4} = x_{5} \}$ is $2$.

    Assume that there exists a linear map $T\in\mathcal{L}(\mathbb{F}^{5}, \mathbb{F}^{2})$ such that $\kernel{T} = \{(x_{1} , x_{2} , x_{3} , x_{4} , x_{5} ) \in \mathbb{F}^{5} : x_{1} = 3x_{2} \text{ and } x_{3} = x_{4} = x_{5} \}$. According to the fundamental theorem of linear maps, $\dim\range{T} = 5 - \dim\kernel{T} = 5 - 2 = 3$. However, this is not the case, because $\range{T}$ must be a subspace of $\mathbb{F}^{2}$.

    Hence there does not exist a linear map $T\in\mathcal{L}(\mathbb{F}^{5}, \mathbb{F}^{2})$ such that $\kernel{T} = \{(x_{1} , x_{2} , x_{3} , x_{4} , x_{5} ) \in \mathbb{F}^{5} : x_{1} = 3x_{2} \text{ and } x_{3} = x_{4} = x_{5} \}$.
\end{proof}
\newpage

% chapter3:sectionB:exercise15
\begin{exercise}\label{chapter3:sectionB:exercise15}
    Suppose there exists a linear map on $V$ whose null space and range are both finite-dimensional. Prove that $V$ is finite-dimensional.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $\kernel{T}$, and $w_{1}, \ldots, w_{m}$ a basis of $\range{T}$. There exist vectors $u_{1}, \ldots, u_{m}$ such that $Tu_{1} = w_{1}, \ldots, Tu_{m} = w_{m}$.

    Let $v$ be a vector in $V$. There exist scalar $a_{1}, \ldots, a_{m}$ such that $Tv = a_{1}w_{1} + \cdots + a_{m}w_{m}$. Then
    \[
        Tv = a_{1}w_{1} + \cdots + a_{m}w_{m} = a_{1}Tu_{1} + \cdots + a_{m}Tu_{m} = T(a_{1}u_{1} + \cdots + a_{m}u_{m})
    \]

    it follows that $v - (a_{1}u_{1} + \cdots + a_{m}u_{m})\in \kernel{T}$. So there exist scalars $b_{1}, \ldots, b_{n}$ such that
    \[
        v - (a_{1}u_{1} + \cdots + a_{m}u_{m}) = b_{1}v_{1} + \cdots + b_{n}v_{n}.
    \]

    Therefore $v = a_{1}u_{1} + \cdots + a_{m}u_{m} + b_{1}v_{1} + \cdots + b_{n}v_{n}$, which means $u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{n}$ spans $V$. Hence $V$ is finite-dimensional.
\end{proof}
\newpage

% chapter3:sectionB:exercise16
\begin{exercise}
    Suppose $V$ and $W$ are both finite-dimensional. Prove that there exists an injective linear map from $V$ to $W$ if and only if $\dim V \leq \dim W$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists an injective linear map $T$ from $V$ to $W$. According to the fundamental theorem of linear maps, $\dim V = \dim\kernel{T} + \dim\range{T}$. Since $T$ is injective, $\dim\kernel{T} = 0$, and $\dim V = \dim\range{T}$. On the other hand, $\range{T}$ is a subspace of $W$, so $\dim\range{T}\leq \dim W$. Hence $\dim V\leq \dim W$.

    $(\Leftarrow)$ $\dim V\leq \dim W$. Let $v_{1}, \ldots, v_{n}$ be a basis of $V$, and $w_{1}, \ldots, w_{n}, w_{n+1}, \ldots, w_{n+m}$ be a basis of $W$. Let $T$ be the linear map that $Tv_{k} = w_{k}$ for every positive integer $k\in [\![ 1,n ]\!]$. Then $T$ is an injective linear map from $V$ to $W$.
\end{proof}
\newpage

% chapter3:sectionB:exercise17
\begin{exercise}
    Suppose $V$ and $W$ are both finite-dimensional. Prove that there exists a surjective linear map from $V$ to $W$ if and only if $\dim V \geq \dim W$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists a surjective linear map $T$ from $V$ to $W$. According to the fundamental theorem of linear maps, $\dim V = \dim\kernel{T} + \dim\range{T}$. Since $T$ is surjective, $\dim\range{T} = \dim W$. On the other hand, $\dim\range{T} = \dim V - \dim\kernel{T}\leq \dim V$, so $\dim V\geq \dim W$.

    $(\Leftarrow)$ $\dim V\geq \dim W$. Let $w_{1}, \ldots, w_{n}$ be a basis of $W$, and $v_{1}, \ldots, v_{n}, v_{n+1}, \ldots, v_{n+m}$ be a basis of $V$. Let $T$ be the linear map that $Tv_{k} = w_{k}$ for every positive integer $k\in [\![ 1,n ]\!]$ and $Tv_{k} = 0$ for every positive integer $k\in [\![ n+1, n+m ]\!]$. Then $T$ is a surjective linear map from $V$ to $W$.
\end{proof}
\newpage

% chapter3:sectionB:exercise18
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional and that $U$ is a subspace of $V$. Prove that there exists $T\in \mathcal{L}(V, W)$ such that $\kernel{T} = U$ if and only if $\dim U \geq \dim V - \dim W$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $T\in \mathcal{L}(V, W)$ such that $\kernel{T} = U$. According to the fundamental theorem of linear maps, $\dim V = \dim\kernel{T} + \dim\range{T}$. Since $\dim\kernel{T} = \dim U$ and $\dim\range{T}\leq \dim W$, $\dim V\leq \dim U + \dim W$, which means $\dim U\geq \dim V - \dim W$.

    $(\Leftarrow)$ $\dim U\geq \dim V - \dim W$. Since $U$ is a subspace of $V$, which is finite-dimensional, then $U$ is also finite-dimensional. Let $u_{1}, \ldots, u_{n}$ be a basis of $U$, extend this list to a basis $u_{1}, \ldots, u_{n}, v_{1}, \ldots, v_{m}$ of $V$. Because $\dim U\geq \dim V - \dim W$, $\dim W\geq (m + n) - n = m$. Let $w_{1}, \ldots, w_{m}, w_{m+1}, \ldots, w_{m+p}$ be a basis of $W$. Let $T$ be the linear map such that $Tu_{i} = 0$ for every positive integer $i\in[\![ 1,n ]\!]$, $Tv_{j} = w_{j}$ for every positive integer $i\in[\![ 1,m ]\!]$. Now I have to show that $\kernel{T} = U$.

    Due to the definition of $T$, it follows that $U\subseteq \kernel{T}$. Let $v$ be a vector in $\kernel{T}$, then $v = a_{1}u_{1} + \cdots + a_{n}u_{n} + b_{1}v_{1} + \cdots + b_{m}v_{m}$. $Tv = b_{1}Tv_{1} + \cdots + b_{m}Tv_{m} = b_{1}w_{1} + \cdots + b_{m}w_{m}$. Since $w_{1}, \ldots, w_{m}$ are linearly independent, $b_{1} = \cdots = b_{m} = 0$, so $v = a_{1}u_{1} + \cdots + a_{n}u_{n}\in U$. Therefore $\kernel{T}\subseteq U$. Hence $\kernel{T} = U$.
\end{proof}
\newpage

% chapter3:sectionB:exercise19
\begin{exercise}
    Suppose $W$ is finite-dimensional and $T\in\mathcal{L}(V, W)$. Prove that $T$ is injective if and only if there exists $S\in\mathcal{L}(W, V)$ such that $ST$ is the identity operator on $V$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $S\in\mathcal{L}(W, V)$ such that $ST$ is the identity operator on $V$. Let $v\in\kernel{T}$. Since $ST = \operatorname{id}_{V}$, then $(ST)(v) = v$. On the other hand, $(ST)(v) = S(Tv) = S(0) = 0$. So $v = 0$, which means $\kernel{T} = \{0\}$. Hence $T$ is injective.

    $(\Leftarrow)$ $T$ is injective, then $\dim\kernel{T} = 0$. $W$ is finite-dimensional, so $\range{T}$ is also finite-dimensional. According to Exercise~\ref{chapter3:sectionB:exercise15}, $V$ is finite-dimensional. On the other hand, $\dim V = \dim\kernel{T} + \dim\range{T} = \dim\range{T}\leq \dim W$.

    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$. According to Exercise~\ref{chapter3:sectionB:exercise9}, $w_{1} = Tv_{1}, \ldots, w_{n} = Tv_{n}$ is linearly independent. Extend this list to a basis $w_{1}, \ldots, w_{n}, w_{n+1}, \ldots, w_{n+m}$ of $W$.

    I define a linear map $S$ from $W$ to $V$ as follows
    \[
        Sw_{k} = \begin{cases}
            v_{k} & \text{if $1\leq k\leq n$}, \\
            0     & \text{otherwise}.
        \end{cases}
    \]

    Then $ST$ is the identity operator on $V$.
\end{proof}
\newpage

% chapter3:sectionB:exercise20
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T\in\mathcal{L}(V, W)$. Prove that $T$ is surjective if and only if there exists $S\in\mathcal{L}(W, V)$ such that $TS$ is the identity operator on $W$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $S\in\mathcal{L}(W, V)$ such that $TS$ is the identity operator on $W$. Let $w\in W$. Since $TS = \operatorname{id}_{W}$, then $(TS)(w) = w$, and $T(Sw) = w$. So $w\in \range{T}$, and $\range{T} = W$. Therefore $T$ is surjective.

    $(\Leftarrow)$ $T$ is surjective, then $\range{T} = W$. According to the proof of the fundamental theorem of linear maps, if $u_{1}, \ldots, u_{m}$ is a basis of $\kernel{T}$, the extended list $u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{n}$ is a basis of of $V$, then $w_{1} = Tv_{1}, \ldots, w_{n} = Tv_{n}$ is a basis of $\range{T}$.

    I define a linear map $S$ from $W$ to $V$ by $Sw_{k} = v_{k}$ for every positive integer $k\in[\![ 1, n ]\!]$. Then $TS$ is the identity operator on $W$.
\end{proof}
\newpage

% chapter3:sectionB:exercise21
\begin{exercise}
    Suppose $V$ is finite-dimensional, $T \in \mathcal{L}(V, W)$, and $U$ is a subspace of $W$. Prove that $\{v \in V : Tv \in U\}$ is a subspace of $V$ and
    \[
        \dim\{ v\in V : Tv\in U \} = \dim \kernel{T} + \dim (U\cap \range{T}).
    \]
\end{exercise}

\begin{proof}
    Let $X = \{v \in V : Tv \in U\}$. Because $T0 = 0\in U$, $0\in X$. If $v\in X$, then $Tv\in U$, and $T(\lambda v)\in U$ for every $\lambda\in\mathbb{F}$. If $v_{1}, v_{2}\in X$, then $T(v_{1} + v_{2}) = Tv_{1} + Tv_{2}\in U$. So $X$ contains $0$, is closed under addition and scalar multiplication. Therefore $X$ is a subspace of $V$.

    Due to the definition of null space and $X$, it follows that $\kernel{T}$ is a subspace of $X$. Let $S$ be the linear map from $X$ to $U$ that $Su = Tu$ for every $u\in U$. Then $\kernel{T} = \kernel{S}$.

    $\range{S}$ is a subspace of $U\cap \range{T}$. For each vector $w$ in $U\cap \range{T}$, there exists a vector $v\in V$ such that $Tv = w$. On the other hand, since $w\in U$, then $v\in U$. So $Sv = w$, and it follows that $U\cap \range{T}$ is a subspace of $\range{S}$. Then $\range{S} = U\cap \range{T}$.

    According to the fundamental theorem of linear map,
    \[
        \dim X = \dim\kernel{S} + \dim\range{S} = \dim\kernel{T} + \dim (U\cap\range{T}).\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionB:exercise22
\begin{exercise}
    Suppose $U$ and $V$ are finite-dimensional vector spaces and $S \in \mathcal{L}(V, W)$ and $T \in \mathcal{L}(U, V)$. Prove that
    \[
        \dim\kernel{ST} \leq \dim\kernel{S} + \dim\kernel{T}.
    \]
\end{exercise}

\begin{proof}
    \[
        \begin{CD}
            U @>T>>     V @>S>>     W
        \end{CD}
    \]

    Because $U, V$ are finite-dimensional, then so are $\kernel{T}$, $\range{T}$, $\kernel{S}$, $\range{S}$.

    According to the fundamental theorem of linear maps,
    \[
        \dim\kernel{ST} = \dim U - \dim\range{ST} = \dim \kernel{T} + \dim\range{T} - \dim\range{ST}.
    \]

    and
    \[
        \dim\range{T} = \dim \kernel{S\vert_{\range{T}}} + \dim\range{ST} = \dim (\kernel{S}\cap\range{T}) + \dim\range{ST}.
    \]

    On the other hand, $\dim (\kernel{S}\cap\range{T}) \leq \dim\kernel{S}$ (where $S\vert_{\range{T}}$ is the linear map $S$ restricted on $\range{T}$ instead of the entire $V$). Therefore
    \[
        \dim\kernel{ST} \leq \dim\kernel{T} + \dim\kernel{S}.
    \]

    The equality holds if and only if $\kernel{S}\subseteq \range{T}$.
\end{proof}
\newpage

% chapter3:sectionB:exercise23
\begin{exercise}
    Suppose $U$ and $V$ are finite-dimensional vector spaces and $S \in \mathcal{L}(V, W)$ and $T \in \mathcal{L}(U, V)$. Prove that
    \[
        \dim\range{ST} \leq \min\{\dim\range{S}, \dim\range{T}\}.
    \]
\end{exercise}

\begin{proof}
    \[
        \begin{CD}
            U @>T>>     V @>S>>     W
        \end{CD}
    \]

    Because $U, V$ are finite-dimensional, then so are $\kernel{T}$, $\range{T}$, $\kernel{S}$, $\range{S}$.

    $ST\in\mathcal{L}(U, W)$. If $w\in\range{ST}$, then there exists $u\in U$ such that $(ST)(u) = w$. So $S(Tu) = w$, and $w$ is also in $\range{S}$. So $\range{ST}$ is a subspace of $\range{S}$, and $\dim\range{ST}\leq \dim\range{S}$.

    According to the fundamental theorem of linear maps,
    \[
        \dim\range{T} = \dim \kernel{S\vert_{\range{T}}} + \dim\range{ST} = \dim (\kernel{S}\cap\range{T}) + \dim\range{ST} \geq \dim\range{ST}
    \]

    So $\dim\range{ST}\leq \dim\range{T}$. Hence
    \[
        \dim\range{ST}\leq \min\{ \dim\range{S}, \dim\range{T} \}.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionB:exercise24
\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $\dim V = 5$ and $S, T \in \mathcal{L}(V)$ are such that $ST = 0$. Prove that $\dim \range{TS} \leq 2$.
        \item Give an example of $S, T \in \mathcal{L}(\mathbb{F}^{5})$ with $ST = 0$ and $\dim\range{TS} = 2$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Unsolved.
\end{proof}
\newpage

% chapter3:sectionB:exercise25
\begin{exercise}\label{chapter3:sectionB:exercise25}
    Suppose that $W$ is finite-dimensional and $S, T \in \mathcal{L}(V, W)$. Prove that $\kernel{S} \subseteq \kernel{T}$ if and only if there exists $E \in \mathcal{L}(W)$ such that $T = ES$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $E\in\mathcal{L}(W)$ such that $T = ES$. Let $v$ be a vector in $\kernel{S}$. Then $Tv = (ES)(v) = E(Sv) = E0 = 0$. So $v$ is also in $\kernel{T}$. Therefore $\kernel{S}\subseteq\kernel{T}$.

    $(\Leftarrow)$ $\kernel{S}\subseteq\kernel{T}$. Since $W$ is finite-dimensional, then so are $\range{S}$ and $\range{T}$.

    \begin{enumerate}[label={\textbf{Step \arabic*.}},itemindent={1cm}]
        \item Construct a linear complement of $\kernel{S}$ in $\kernel{T}$.

              Let $Sv_{1}, \ldots, Sv_{n}$ be a basis of $\range S\vert_{\kernel{T}}$. Let $0 = a_{1}v_{1} + \cdots + a_{n}v_{n}$ be a linear combination of $0$ in $V$. Then $0 = S(a_{1}v_{1} + \cdots + a_{n}v_{n}) = a_{1}Sv_{1} + \cdots + a_{n}Sv_{n}$. Because $Sv_{1}, \ldots, Sv_{n}$ is linearly independent, it follows that $a_{1} = \cdots = a_{n} = 0$. So $v_{1}, \ldots, v_{n}$ is linearly independent.

              Let $v$ be a vector in $\kernel{T}$. There exist scalars $b_{1}, \ldots, b_{n}$ such that $Sv = b_{1}Sv_{1} + \cdots + b_{n}Sv_{n}$. So $S(v - b_{1}v_{1} - \cdots - b_{n}v_{n}) = 0$, which means $v$ is the sum of $b_{1}v_{1} + \cdots + b_{n}v_{n}$ and a vector in $\kernel{S}$. Therefore $\kernel{T} = \kernel{S} + \operatorname{span}(v_{1}, \ldots, v_{n})$.

              A vector $x_{1}v_{1} + \cdots + x_{n}v_{n}$ in $\operatorname{span}(v_{1}, \ldots, v_{n})$ is in $\kernel{S}$ if and only if $x_{1}Sv_{1} + \cdots + x_{n}Sv_{n} = 0$. On the other hand, $x_{1}Sv_{1} + \cdots + x_{n}Sv_{n} = 0$ if and only if $x_{1} = \cdots = x_{n} = 0$. So $\kernel{S}\cap \operatorname{span}(v_{1}, \ldots, v_{n}) = \{0\}$.

              Hence $\kernel{T} = \kernel{S}\oplus\operatorname{span}(v_{1}, \ldots, v_{n})$.
        \item Construct a linear complement of $\kernel T$ in $V$.

              Let $Tu_{1}, \ldots, Tu_{m}$ be a basis of $\range{T}$.

              Let $0 = a_{1}u_{1} + \cdots + a_{n}u_{n}$ be a linear combination of $0$ in $V$. Then
              \[
                  0 = T0 = T(a_{1}u_{1} + \cdots + a_{n}u_{n}) = a_{1}Tu_{1} + \cdots + a_{n}Tu_{n}.
              \]

              Since $Tu_{1}, \ldots, Tu_{m}$ is linearly independent, $a_{1} = \cdots = a_{m} = 0$. So $u_{1}, \ldots, u_{m}$ is linearly independent.

              $x_{1}u_{1} + \cdots + x_{m}u_{m}$ is in $\kernel{T}$ if and only if $T(x_{1}u_{1} + \cdots + x_{m}u_{m}) = 0$.
              \[
                  x_{1}Tu_{1} + \cdots + x_{m}Tu_{m} = T(x_{1}u_{1} + \cdots + x_{m}u_{m}) = 0.
              \]

              $x_{1}Tu_{1} + \cdots + x_{m}Tu_{m} = 0$ if and only if $x_{1} = \cdots = x_{m} = 0$. So $\kernel{T}\cap\operatorname{span}(u_{1}, \ldots, u_{m}) = \{0\}$.

              Let $v$ be a vector in $V$, then there exist scalars $a_{1}, \ldots, a_{m}$ such that $Tv = a_{1}Tu_{1} + \cdots + a_{m}Tu_{m}$. So
              \[
                  Tv = a_{1}Tu_{1} + \cdots + a_{m}Tu_{m} = T(a_{1}u_{1} + \cdots + a_{m}u_{m}).
              \]

              It follows that $v - (a_{1}u_{1} + \cdots + a_{m}u_{m})$ is in $\kernel{T}$. So $V = \kernel{T} + \operatorname{span}(u_{1}, \ldots, u_{m})$.

              Because $V = \kernel{T} + \operatorname{span}(u_{1}, \ldots, u_{m})$ and $\kernel{T}\cap \operatorname{span}(u_{1}, \ldots, u_{m}) = \{0\}$, we conclude that $V = \kernel{T}\oplus \operatorname{span}(u_{1}, \ldots, u_{m})$.
        \item Construct a linear map $E$ in $\mathcal{L}(W)$.

              Due to \textbf{Step 1} and \textbf{Step 2}
              \[
                  V = \underbrace{\kernel{S} \oplus \operatorname{span}(v_{1}, \ldots, v_{n})}_{\kernel{T}} \oplus \operatorname{span}(u_{1}, \ldots, u_{m}).
              \]

              On the other hand, $Sv_{1}, \ldots, Sv_{n}, Su_{1}, \ldots, Su_{m}$ is a basis of $\range{S}$, because, if $x_{1}Sv_{1} + \cdots + x_{n}Sv_{n} + y_{1}Su_{1} + \cdots + y_{m}Su_{m} = 0$ is a linear combination of $0$ in $\range{S}$, then $S(x_{1}v_{1} + \cdots + x_{n}v_{n} + y_{1}u_{1} + \cdots + y_{m}u_{m}) = 0$. But $\kernel{S}\cap \operatorname{span}(v_{1}, \ldots, v_{n}, u_{1}, \ldots, u_{m}) = \{0\}$, so $x_{1}v_{1} + \cdots + x_{n}v_{n} + y_{1}u_{1} + \cdots + y_{m}u_{m} = 0$, which implies $x_{1} = \cdots = x_{n} = 0$, $y_{1} = \cdots = y_{m} = 0$ because the list $v_{1}, \ldots, v_{n}, u_{1}, \ldots, u_{m}$ is linearly independent.

              I define the linear map $E$ in $\mathcal{L}(W)$ as follows:
              \[
                  E(Sv_{i}) = 0\qquad E(Su_{j}) = Tu_{j}.
              \]

              Let $v$ be a vector in $V$. There exist $v_{0}$ in $\kernel{S}$ and scalars $a_{1}, \ldots, a_{n}, b_{1}, \ldots, b_{m}$ such that $v = v_{0} + a_{1}v_{1} + \cdots + a_{n}v_{n} + b_{1}u_{1} + \cdots + b_{m}u_{m}$. Note that $Tv_{0} = Sv_{0} = 0$ because $\kernel S\subseteq \kernel T$ and $Tv_{1} = \cdots = Tv_{n} = 0$.
              \begin{align*}
                  (ES)(v) & = E(Sv_{0} + a_{1}Sv_{1} + \cdots + a_{n}Sv_{n} + b_{1}Su_{1} + \cdots + b_{m}Su_{m})                 \\
                          & = E(Sv_{0}) + (a_{1}E(Sv_{1}) + \cdots + a_{n}E(Sv_{n})) + (b_{1}E(Su_{1}) + \cdots + b_{m}E(Su_{m})) \\
                          & = 0 + (0 + \cdots + 0) + (b_{1}Tu_{1} + \cdots + b_{m}Tu_{m})                                         \\
                          & = Tv_{0} + (a_{1}Tv_{1} + \cdots + a_{n}Tv_{n}) + (b_{1}Tu_{1} + \cdots + b_{m}Tu_{m})                \\
                          & = T(v_{0} + a_{1}v_{1} + \cdots + a_{n}v_{n} + b_{1}u_{1} + \cdots + b_{m}u_{m})                      \\
                          & = Tv.
              \end{align*}

              Hence $T = ES$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionB:exercise26
\begin{exercise}
    Suppose that $V$ is finite-dimensional and $S, T \in \mathcal{L}(V, W)$. Prove that $\range{S} \subseteq \range{T}$ if and only if there exists $E \in \mathcal{L}(V)$ such that $S = TE$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $E \in \mathcal{L}(V)$ such that $S = TE$. Let $w$ be a vector in $\range{S}$, then there exists a vector $v$ in $V$ such that $Sv = w$. So $(TE)(v) = w$, and $T(Ev) = w$. Therefore $w$ is also in $\range{T}$. Hence $\range{S}\subseteq\range{T}$.

    $(\Leftarrow)$ $\range{S} \subseteq \range{T}$.

    According to the fundamental theorem of linear maps, $\range{S}$ and $\range{T}$ are finite-dimensional.

    Let $Su_{1}, \ldots, Su_{n}$ be a basis of $\range{S}$.

    $u_{1}, \ldots, u_{n}$ is linearly independent. Extend the list $u_{1}, \ldots, u_{n}$ to create a basis of $V$ and let it be
    \[
        u_{1}, \ldots, u_{n}, u_{n+1}, \ldots, u_{n+m}, u_{n+m+1}, \ldots, u_{n+m+p}.
    \]

    Since $\range{S}\subseteq \range{T}$, there exist vectors $v_{k}$ such that $Su_{k} = Tv_{k}$ for every positive integer $k\in [\![ 1, n+m+p ]\!]$.

    I define the linear map $E$ in $\mathcal{L}(V)$ as follows: $Eu_{k} = v_{k}$ for every positive integer $k\in [\![ 1, n+m+p ]\!]$.

    Then $(TE)(u_{k}) = T(Eu_{k}) = Tv_{k} = Su_{k}$ for every positive integer $k\in [\![ 1, n+m+p ]\!]$.

    Hence $S = TE$.\qedhere
\end{proof}
\newpage

% chapter3:sectionB:exercise27
\begin{exercise}
    Suppose $P \in \mathcal{L}(V)$ and $P^{2} = P$. Prove that $V = \kernel{P}\oplus \range{P}$.
\end{exercise}

\begin{proof}
    Let $v$ be a vector in $V$. $v = (v - Pv) + Pv$. On the other hand, $P(v - Pv) = Pv - P^{2}v = 0$. So $V = \kernel{P}\oplus \range{P}$.

    Let $w$ be a vector in $\kernel{P}\cap\range{P}$. Then there exists vector $v$ in $V$ such that $Pv = w$. On the other hand, $Pw = 0$ because $w\in\kernel{P}$. So $w = Pv = P^{2}v = Pw = 0$. Therefore $\kernel{P}\cap\range{P} = \{0\}$.

    Hence $V = \kernel{P}\oplus\range{P}$.
\end{proof}
\newpage

% chapter3:sectionB:exercise28
\begin{exercise}
    Suppose $D \in \mathcal{L}(\mathcal{P}(\mathbb{R}))$ is such that $\deg Dp = (\deg p) - 1$ for every non-constant polynomial $p \in \mathcal{P}(\mathbb{R})$. Prove that $D$ is surjective.
\end{exercise}

\begin{proof}
    Let $a$ be a polynomial of degree $1$, let $c$ be a constant polynomial, then $a + c$ is a polynomial of degree $1$. $D(a + c)$ is a constant polynomial, because $\deg D(a+c) = \deg (a + c) - 1 = 0$. On the other hand, $D(a + c) = Da + Dc$, so $Dc$ is a constant polynomial.

    Suppose that $Dc\ne 0$. Because $\deg Da = 0$, it follows that $Da\ne 0$.
    \[
        D\left(\frac{-Dc}{Da}a + c\right) = \frac{-Dc}{Da}\cdot Da + Dc = 0.
    \]

    It follows that $\deg D\left(\frac{-Dc}{Da}a + c\right) = -\infty \ne 0 = \deg\left(\frac{-Dc}{Da}a + c\right) - 1$. So the assumption $Dc\ne 0$ is false. Therefore $Dc = 0$. So $0$ is in $\range{D}$.

    I will prove the following statement by using mathematical induction: for each nonnegative integer $n$, every polynomial of degree $n$ in $\mathcal{L}(\mathcal{P}(\mathbb{R}))$ is in $\range{D}$.

    When $n = 0$. Let $c_{0}$ be a nonzero constant polynomial. Let $c_{1}$ be a polynomial of degree $1$. $\deg Dc_{1} = 0$, so there exist a nonzero constant polynomial $d_{0}$ such that $Dc_{1} = d_{0}$. $D\left(\frac{c_{0}}{d_{0}}c_{1}\right) = \frac{c_{0}}{d_{0}}\cdot d_{0} = c_{0}$. So $c_{0}\in\range{D}$.

    Assume that for every nonnegative integer $n\leq k$, every polynomial of degree $k$ is in $\range{D}$. Let $p_{k+1}$ be a polynomial of degree $(k+1)$, $p_{k+2}$ be a polynomial of degree $(k+2)$, $q_{k+1} = Dp_{k+2}$. Let $a_{k+1}$ and $b_{k+1}$ be the leading coefficients of $p_{k+1}$ and $q_{k+1}$. Since
    \[
        \deg\left(\frac{1}{a_{k+1}}p_{k+1} - \frac{1}{b_{k+1}}q_{k+1}\right) \leq k
    \]

    then $\frac{1}{a_{k+1}}p_{k+1} - \frac{1}{b_{k+1}}q_{k+1}$ is in $\range{D}$. Together with $q_{k+1}$ being in $\range{D}$, we conclude that $p_{k+1}$ is also in $\range{D}$.

    Due to the principle of mathematical induction, every nonzero polynomial is in $\range{D}$.

    Because the zero polynomial and every nonzero polynomial are in $\range{D}$, it follows that every polynomial is in $D$. Thus $D$ is surjective.
\end{proof}
\newpage

% chapter3:sectionB:exercise29
\begin{exercise}
    Suppose $p \in \mathcal{P}(\mathbb{R})$. Prove that there exists a polynomial $q \in \mathcal{P}(\mathbb{R})$ such that $5q'' + 3q' = p$.
\end{exercise}

\begin{proof}
    If $p$ is the zero polynomial, then $q(x) = 1$ satisfies.

    If $p$ is a nonzero polynomial, let $p: x\mapsto a_{0} + a_{1}x + \cdots + a_{n}x^{n}$ where $a_{n}\ne 0$. Any polynomial $q$ that satisfies the differential equation must have degree $(n+1)$.

    Let $q(x) = b_{0} + b_{1}x + \cdots + b_{n+1}x^{n+1}$. The coefficient of $x^{k}$ in $5q''(x) + 3q'(x)$ where $k < n$ is
    \[
        5(k+2)(k+1)b_{k+2} + 3(k+1)b_{k+1}.
    \]

    The coefficient of $x^{k}$ in $5q''(x) + 3q'(x)$ where $k = n$ is
    \[
        (n+1)b_{n+1}.
    \]

    We obtain a system of $(n+1)$ linear equations:
    \begin{align*}
        (n+1)b_{n+1}                       & = a_{n} \\
        5(k+2)(k+1)b_{k+2} + 3(k+1)b_{k+1} & = a_{k}
    \end{align*}

    This system of linear equations has more unknowns than equations (there are $(n+2)$ unknowns and $(n+1)$ equations), so it has at least one solution. Therefore there exists a polynomial $q\in\mathcal{P}(\mathbb{R})$ such that $5q'' + 3q' = p$.
\end{proof}
\newpage

% chapter3:sectionB:exercise30
\begin{exercise}\label{chapter3:sectionB:exercise30}
    Suppose $\varphi \in \mathcal{L}(V, \mathbb{F})$ and $\varphi \ne 0$. Suppose $u \in V$ is not in $\kernel{\varphi}$. Prove that
    \[
        V = \kernel{\varphi} \oplus \{ au : a\in\mathbb{F} \}.
    \]
\end{exercise}

\begin{proof}
    Let $v$ be a vector in $V$. $\varphi u\ne 0$ so $\varphi u$ is a basis of $\mathbb{F}$. So there exists a scalar $\lambda\in\mathbb{F}$ such that $\varphi v = \lambda\cdot \varphi u$. So $\varphi(v - \lambda u) = 0$, which implies $v - \lambda u$ is in $\kernel{\varphi}$. So $V = \kernel{\varphi} + \operatorname{span}(u)$.

    Let $v_{0}$ be a vector in $\kernel{\varphi}\cap\operatorname{span}(u)$. So there exists s scalar $\lambda_{0}$ such that $v_{0} = \lambda_{0}u$. $\varphi v_{0} = 0$ and $\varphi v_{0} = \lambda_{0}\cdot \varphi u$. So $\lambda_{0} = 0$. Therefore $v_{0} = 0$, and $\kernel{\varphi}\cap\operatorname{span}(u) = \{0\}$.

    Hence $V = \kernel{\varphi} \oplus \operatorname{span}(u)$.
\end{proof}
\newpage

% chapter3:sectionB:exercise31
\begin{exercise}
    Suppose $V$ is finite-dimensional, $X$ is a subspace of $V$, and $Y$ is a finite-dimensional subspace of $W$. Prove that there exists $T \in \mathcal{L}(V, W)$ such that $\kernel{T} = X$ and $\range{T} = Y$ if and only if $\dim X + \dim Y = \dim V$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $T \in \mathcal{L}(V, W)$ such that $\kernel{T} = X$ and $\range{T} = Y$.

    According to the fundamental theorem of linear maps, $\dim V = \dim X + \dim Y$.

    $(\Leftarrow)$ $\dim V = \dim X + \dim Y$.

    Let $v_{1}, \ldots, v_{n}$ be a basis of $X$. Extend this list to create a basis of $V$, and let it be
    \[
        v_{1}, \ldots, v_{n}, v_{n+1}, \ldots, v_{n+m}.
    \]

    Let $w_{1}, \ldots, w_{m}$ be a basis of $Y$. I define the linear map $T$ from $V$ to $W$ as follows:
    \[
        Tv_{k} = \begin{cases}
            0       & \text{if $1\leq k\leq n$} \\
            w_{k-n} & \text{otherwise}
        \end{cases}
    \]

    From this definition, and the linear independence of $w_{1}, \ldots, w_{m}$, it follows that $\range{T} = Y$ and $\kernel{T} = X$.
\end{proof}
\newpage

% chapter3:sectionB:exercise32
\begin{exercise}
    Suppose $V$ is finite-dimensional with $\dim V > 1$. Show that if $\varphi: \mathcal{L}(V) \to \mathbb{F}$ is a linear map such that $\varphi(ST) = \varphi(S)\varphi(T)$ for all $S, T \in \mathcal{L}(V)$, then $\varphi = 0$.
\end{exercise}

\begin{proof}
    Let $v_{1}, v_{2}, \ldots, v_{n}$ be a basis of $V$.

    Let $S$ be a linear map in $\kernel{\varphi}$, $T$ a linear map in $\mathcal{L}(V)$. $\varphi(ST) = \varphi(S)\varphi(T) = 0$, $\varphi(TS) = \varphi(T)\varphi(S) = 0$. So $ST$, $TS$ are also in $\kernel{\varphi}$. Therefore $\varphi{T}$ is a two-sided ideal. According to Exercise~\ref{chapter3:sectionA:exercise17}, $\kernel{\varphi}$ is either $\{0\}$ or $\mathcal{L}(V)$.

    Let $R$ be the linear map in $\mathcal{L}(V)$ that $Rv_{1} = v_{2}$, $Rv_{i} = 0$ for every positive integer $i\in[\![ 2, n ]\!]$, then $R^{2} = 0$. So $\varphi(R^{2}) = \varphi(R)\varphi(R) = 0$, and $\varphi(R) = 0$. Since $R\ne 0$, then $\kernel{\varphi}\ne \{0\}$. Hence $\kernel{\varphi} = \mathcal{L}(V)$, which implies $\varphi = 0$.
\end{proof}
\newpage

% chapter3:sectionB:exercise33
\begin{exercise}
    Suppose that $V$ and $W$ are real vector spaces and $T \in \mathcal{L}(V, W)$. Define $T_{\mathbb{C}}: V_{\mathbb{C}} \to W_{\mathbb{C}}$ by
    \[
        T_{\mathbb{C}}(u + \iota v) = Tu + \iota Tv
    \]

    for all $u, v\in V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $T_{\mathbb{C}}$ is a (complex) linear map from $V_{\mathbb{C}}$ to $W_{\mathbb{C}}$.
        \item Show that $T_{\mathbb{C}}$ is injective if and only if $T$ is injective.
        \item Show that $\range T_{\mathbb{C}} = W_{\mathbb{C}}$ if and only if $\range T = W$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \begingroup\allowdisplaybreaks{}
              \begin{align*}
                  T_{\mathbb{C}}((u_{1} + \iota v_{1}) + (u_{2} + \iota v_{2})) & = T_{\mathbb{C}}((u_{1} + u_{2}) + \iota (v_{1} + v_{2}))                    \\
                                                                                & = T(u_{1} + u_{2}) + \iota T(v_{1} + v_{2})                                  \\
                                                                                & = (Tu_{1} + Tu_{2}) + \iota (Tv_{1} + Tv_{2})                                \\
                                                                                & = (Tu_{1} + \iota Tv_{1}) + (Tu_{2} + \iota Tv_{2})                          \\
                                                                                & = T_{\mathbb{C}}(u_{1} + \iota v_{1}) + T_{\mathbb{C}}(u_{2} + \iota v_{2}), \\
                  T_{\mathbb{C}}((a + b\iota)(u + \iota v))                     & = T_{\mathbb{C}}((au - bv) + \iota (av + bu))                                \\
                                                                                & = T(au - bv) + \iota T(av + bu)                                              \\
                                                                                & = (T(au) + \iota^{2} T(bv)) + (\iota T(av) + \iota T(bu))                    \\
                                                                                & = (T(au) + \iota T(bu)) + \iota (T(av) + \iota T(bv))                        \\
                                                                                & = (a + b\iota) Tu + (a + b\iota)\iota Tv                                     \\
                                                                                & = (a + b\iota) (Tu + \iota Tv)                                               \\
                                                                                & = (a + b\iota) T_{\mathbb{C}}(u + \iota v).
              \end{align*}
              \endgroup

              Thus $T_{\mathbb{C}}$ is a linear map.
        \item $(\Rightarrow)$ $T$ is injective.

              $Tu = Tv = 0$ if and only if $u = v = 0$. So $T_{\mathbb{C}}(u + \iota v) = 0$ if and only if $u + \iota v = 0 + \iota 0$. Hence $T_{\mathbb{C}}$ is injective.

              $(\Leftarrow)$ $T_{\mathbb{C}}$ is injective.

              $T_{\mathbb{C}}(u + \iota v) = 0$ if and only if $u + \iota v = 0$. If $Tu = 0$, then $0 = Tu + \iota T0 = T_{\mathbb{C}}(u + \iota 0)$. $T_{\mathbb{C}}(u + \iota 0) = 0$ if and only if $u = 0$. Hence $T$ is injective.
        \item $(\Rightarrow)$ $T$ is surjective.

              Let $w + \iota z$ be a vector in $W_{\mathbb{C}}$. There exist vectors $u, v$ in $V$ such that $Tu = w$, $Tv = z$. $T_{\mathbb{C}}(u + \iota v) = Tu + \iota Tv = w + \iota z$. Hence $T_{\mathbb{C}}$ is surjective.

              $(\Leftarrow)$ $T_{\mathbb{C}}$ is surjective.

              Let $w$ be a vector in $W$. There exists a vector $u + \iota v$ in $V_{\mathbb{C}}$ such that $T_{\mathbb{C}}(u + \iota v) = w + \iota 0$.

              So $w + \iota 0 = T_{\mathbb{C}}(u + \iota v) = Tu + \iota Tv$. Therefore $w = Tu$ and $0 = Tv$. Hence $T$ is surjective.
    \end{enumerate}
\end{proof}
\newpage

\section{Matrices}

% chapter3:sectionC:exercise1
\begin{exercise}
    Suppose $T \in \mathcal{L}(V, W)$. Show that with respect to each choice of bases of $V$ and $W$, the matrix of $T$ has at least $\dim \range{T}$ nonzero entries.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$, $w_{1}, \ldots, w_{m}$ be a basis of $W$, $\mathcal{M}(T)$ be the matrix of $T$ with respect to these two bases.

    $Tv_{i}$ is a linear combination of $w_{1}, \ldots, w_{m}$ where the coefficients are the $i$th column of $\mathcal{M}(T)$. The list $Tv_{1}, \ldots, Tv_{n}$ spans $\range{T}$. So there are $\dim\range{T}$ vectors in $Tv_{1}, \ldots, Tv_{n}$ that are linearly independent, and there are at least $\dim\range{T}$ columns of $\mathcal{M}(T)$ that are nonzero. Therefore $\mathcal{M}(T)$ has at least $\dim\range{T}$ nonzero entries.
\end{proof}
\newpage

% chapter3:sectionC:exercise2
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional and $T \in \mathcal{L}(V, W)$. Prove that $\dim \range{T} = 1$ if and only if there exist a basis of $V$ and a basis of $W$ such that with respect to these bases, all entries of $\mathcal{M}(T)$ equal $1$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exist bases $v_{1}, \ldots, v_{n}$ of $V$, $w_{1}, \ldots, w_{m}$ of $W$, such that all entries of $\mathcal{M}(T)$ equal $1$.

    $Tv_{i} = w_{1} + \cdots + w_{m}$ for every positive integer $i\in[\![1,n]\!]$. On the other hand, $w_{1} + \cdots + w_{m}\ne 0$, because $w_{1}, \ldots, w_{m}$ is linearly independent. Moreover, the list $Tv_{1}, \ldots, Tv_{n}$ spans $\range{T}$. So $\dim\range{T} = 1$.

    $(\Leftarrow)$ $\dim\range{T} = 1$.

    Suppose that $\dim V = n, \dim W = m$. Let $v_{1}$ be a vector in $V$ such that $Tv_{1}\ne 0$.

    For each positive integer $k \in [\![ 2, n ]\!]$, there exists a vector $v$ such that $v_{1}, \ldots, v_{k-1}, v$ is linearly independent. If $Tv \ne 0$, we define $v_{k} = a^{-1}v$ (where $Tv = a\cdot Tv_{1}$), otherwise, we define $v_{k} = v + v_{1}$. At the end of this process, we obtain the list $v_{1}, \ldots, v_{n}$ of linearly independent vectors that spans $V$ and $Tv_{i}\ne 0$. Moreover, $Tv_{1} = \cdots = Tv_{n}$.

    Let $w_{1} = Tv_{1}$. Extend $w_{1}$ to $w_{1}, \ldots, w_{m}$ to obtain a basis of $W$. Let $u_{1} = w_{1} - (w_{2} + \cdots + w_{m})$. The list $u_{1}, w_{2}, \ldots, w_{n}$ is linearly independent and has length $m$ so it is a basis of $W$. $Tv_{i} = u_{1} + w_{2} + \cdots + w_{m}$ for every positive integer $i\in[\![1,n]\!]$. So the matrix of $T$ with respect to bases $v_{1}, \ldots, v_{n}$ and $u_{1}, w_{2}, \ldots, w_{m}$ has all entries equal $1$.
\end{proof}
\newpage

% chapter3:sectionC:exercise3
\begin{exercise}
    Suppose $v_{1} , \ldots, v_{n}$ is a basis of $V$ and $w_{1} , \ldots, w_{m}$ is a basis of $W$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that if $S, T\in \mathcal{L}(V, W)$, then $\mathcal{M}(S + T) = \mathcal{M}(S) + \mathcal{M}(T)$.
        \item Show that if $\lambda\in\mathbb{F}$ and $T\in\mathcal{L}(V, W)$, then $\mathcal{M}(\lambda T) = \lambda \mathcal{M}(T)$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise4
\begin{exercise}
    Suppose that $D \in \mathcal{L}(\mathcal{P}_{3} (\mathbb{R}), \mathcal{P}_{2} (\mathbb{R}))$ is the differentiation map defined by $Dp = p'$. Find a basis of $\mathcal{P}_{3} (\mathbb{R})$ and a basis of $\mathcal{P}_{2} (\mathbb{R})$ such that the matrix of $D$ with respect to these bases is
    \[
        \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0
        \end{pmatrix}.
    \]
\end{exercise}

\begin{proof}
    I choose $\frac{1}{3}X^{3}$, $\frac{1}{2}X^{2}$, $X$, $1$ to be a basis of $\mathcal{P}_{3}(\mathbb{R})$, and $X^{2}$, $X$, $1$ to be a basis of $\mathcal{P}_{3}(\mathbb{R})$. Then the matrix of $D$ with respect to these bases is
    \[
        \begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 1 & 0
        \end{pmatrix}.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionC:exercise5
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional and $T \in \mathcal{L}(V, W)$. Prove that there exist a basis of $V$ and a basis of $W$ such that with respect to these bases, all entries of $\mathcal{M}(T)$ are $0$ except that the entries in row $k$, column $k$, equal $1$ if $1\leq k \leq \dim\range{T}$.
\end{exercise}

\begin{proof}
    If $T = 0$, then for any choice of bases, all entries of $\mathcal{M}(T)$ are $0$. Otherwise,

    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$. The list $Tv_{1}, \ldots, Tv_{n}$ spans $\range{T}$, and this list can be reduced to a linearly independent list that spans $\range{T}$. Let $Tu_{1}, \ldots, Tu_{\ell}$ ($\ell\leq n$) be vectors from the list $Tv_{1}, \ldots, Tv_{n}$ such that $Tu_{1}, \ldots, Tu_{\ell}$ are linearly independent and spans $\range{W}$.

    $Tu_{1}, \ldots, Tu_{\ell}$ is linearly independent in $W$, then the list $u_{1}, \ldots, u_{\ell}$ is linearly independent in $V$. Next, I will prove that $V = \kernel{T} \oplus\operatorname{span}(u_{1}, \ldots, u_{\ell})$.

    Let $v$ be a vector in $V$, there exist scalars $a_{1}, \ldots, a_{\ell}$ such that
    \[
        Tv = a_{1}Tu_{1} + \cdots + a_{\ell}Tu_{\ell}.
    \]

    So
    \[
        T(v - a_{1}u_{1} - \cdots - a_{\ell}u_{\ell}) = 0.
    \]

    Therefore $v = (a_{1}u_{1} + \cdots + a_{\ell}u_{\ell}) + u$, where $u\in\kernel{T}$. If $v_{0}$ is a vector in $\kernel{T}\cap \operatorname{span}(u_{1}, \ldots, u_{\ell})$, then there exist scalars $x_{1}, \ldots, x_{\ell}$ such that $v_{0} = x_{1}u_{1} + \cdots + x_{\ell}u_{\ell}$ and $Tv_{0} = 0$. It follows that
    \[
        x_{1}Tu_{1} + \cdots + x_{\ell}Tu_{\ell} = 0
    \]

    and this implies $x_{1} = \cdots = x_{\ell} = 0$ because $Tu_{1}, \ldots, Tu_{\ell}$ is a linearly independent list. So
    \[
        \kernel{T}\cap\operatorname{span}(u_{1}, \ldots, u_{\ell}) = \{0\}.
    \]

    Hence $V = \kernel{T}\oplus\operatorname{span}(u_{1}, \ldots, u_{\ell})$.

    Let $u_{k+1}, \ldots, u_{n}$ be a basis of $\kernel{T}$, then $u_{1}, \ldots, u_{n}$ is a basis of $V$.

    Let $w_{1} = Tu_{1}, \ldots, w_{\ell} = Tu_{\ell}$ and extend this list to create a basis of $W$, and let it be $w_{1}, \ldots, w_{m}$. Then
    \[
        Tu_{k} = \begin{cases}
            w_{k} & \text{if $1\leq k\leq \ell$ where $\ell = \dim\range{T}$}, \\
            0     & \text{otherwise}.
        \end{cases}
    \]

    The matrix of $T$ with respect to bases $u_{1}, \ldots, u_{n}$ of $V$ and $w_{1}, \ldots, w_{m}$ of $W$ satisfies: all of its entries are $0$ except the entries in row $k$, column $k$, equal $1$ when $1\leq k\leq \dim\range{T}$.
\end{proof}
\newpage

% chapter3:sectionC:exercise6
\begin{exercise}
    Suppose $v_{1}, \ldots, v_{m}$ is a basis of $V$ and $W$ is finite-dimensional. Suppose $T \in \mathcal{L}(V, W)$. Prove that there exists a basis $w_{1}, \ldots, w_{n}$ of $W$ such that all entries in the first column of $\mathcal{M}(T)$ [with respect to the bases $v_{1}, \ldots, v_{m}$ and $w_{1}, \ldots, w_{n}$] are $0$ except for possibly a $1$ in the first row, first column.
\end{exercise}

\begin{proof}
    If $T = 0$, then for any choice of basis of $W$, all entries of $\mathcal{M}(T)$ are $0$.

    If $T\ne 0$, then there exists a vector $v$ in the list $v_{1}, \ldots, v_{m}$ such that $Tv\ne 0$. Without loss of generality, suppose that $Tv_{1}\ne 0$. Let $w_{1} = Tv_{1}$. Let $w_{1}, \ldots, w_{n}$ be a basis of $W$, then $Tv_{1} = 1w_{1} + 0w_{2} + \cdots + 0w_{n}$.

    The matrix of $T$ with respect to bases $v_{1}, \ldots, v_{m}$ of $V$ and $w_{1}, \ldots, w_{n}$ of $W$ satisfies: all entries in the first column are $0$, except for a $1$ in the first row, first column.
\end{proof}
\newpage

% chapter3:sectionC:exercise7
\begin{exercise}
    Suppose $w_{1}, \ldots, w_{n}$ is a basis of $W$ and $V$ is finite-dimensional. Suppose $T \in \mathcal{L}(V, W)$. Prove that there exists a basis $v_{1}, \ldots, v_{m}$ of $V$ such that all entries in the first row of $\mathcal{M}(T)$ [with respect to the bases $v_{1}, \ldots, v_{m}$ and $w_{1}, \ldots, w_{n}$] are $0$ except for possibly a $1$ in the first row, first column.
\end{exercise}

\begin{proof}
    Let $u_{1}, \ldots, u_{m}$ be a basis of $V$. I consider the two following cases.

    If the linear combination of $Tu_{i}$ with respect to $w_{1}, \ldots, w_{n}$ has coefficient $0$ with respect to $w_{1}$ for every $i\in[\![1,n]\!]$, then in the matrix of $T$ with respect to bases $u_{1}, \ldots, u_{m}$ and $w_{1}, \ldots, w_{n}$, all entries in the first row are $0$.

    If there exists $k\in[\![1,n]\!]$ such that the coefficient of $w_{1}$ in the linear combination of $Tu_{k}$ with respect to $w_{1}, \ldots, w_{n}$ is nonzero. Let
    \[
        Tu_{k} = b_{1}w_{1} + \cdots + b_{n}w_{n}.
    \]

    I choose $v_{1} = {b^{-1}_{1}}u_{k}$, so $Tv_{1} = w_{1} + b^{-1}_{1}b_{2}w_{2} + \cdots + b^{-1}_{1}b_{n}w_{n}$.

    For each integer $k\in[\![2, m]\!]$, there exist a vector $v$ such that the list $v_{1}, \ldots, v_{k-1}, v$ is linearly independent. If $Tv = a_{1}w_{1} + \cdots + a_{n}w_{n}$, then we let $v_{k} = v - a_{1}v_{1}$. The list $v_{1}, \ldots, v_{k-1}, v_{k}$ is also linearly independent. At the end of this process, we obtain the basis $v_{1}, \ldots, v_{m}$, and in the matrix of $T$ with respect to bases $v_{1}, \ldots, v_{m}$ and $w_{1}, \ldots, w_{n}$, all entries on the first row is $0$, except for the entry in the first row, first column.
\end{proof}
\newpage

% chapter3:sectionC:exercise8
\begin{exercise}
    Suppose $A$ is an $m$-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Prove that
    \[
        {(AB)}_{j,\cdot} = A_{j,\cdot}\cdot B
    \]

    for each $1\leq j\leq m$. In other words, show that row $j$ of $AB$ equals row $j$ of $A$ times $B$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise9
\begin{exercise}
    Suppose $a = \begin{pmatrix}a_{1} & \cdots & a_{n}\end{pmatrix}$ is a $1$-by-$n$ matrix and $B$ is an $n$-by-$p$ matrix. Prove that
    \[
        aB = a_{1}B_{1,\cdot} + \cdots + a_{n}B_{n,\cdot}.
    \]

    In other words, show that $aB$ is a linear combination of the rows of $B$, with the scalars that multiply the rows coming from $a$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise10
\begin{exercise}
    Give an example of $2$-by-$2$ matrices $A$ and $B$ such that $AB \ne BA$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise11
\begin{exercise}
    Prove that the distributive property holds for matrix addition and matrix multiplication. In other words, suppose $A$, $B$, $C$, $D$, $E$, and $F$ are matrices whose sizes are such that $A(B + C)$ and $(D + E)F$ make sense. Explain why $AB + AC$ and $DF + EF$ both make sense and prove that
    \[
        A(B + C) = AB + AC \quad\text{ and }\quad (D + E)F = DF + EF.
    \]
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise12
\begin{exercise}
    Prove that matrix multiplication is associative. In other words, suppose $A$, $B$,and $C$ are matrices whose sizes are such that $(AB)C$ makes sense. Explain why $A(BC)$ makes sense and prove that
    \[
        (AB)C = A(BC).
    \]
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise13
\begin{exercise}
    Suppose $A$ is an $n$-by-$n$ matrix and $1\leq j, k\leq n$. Show that the entry in row $j$, column $k$, of $A^{3}$ (which is defined to mean $AAA$) is
    \[
        \sum^{n}_{p=1}\sum^{n}_{r=1}A_{j,p}A_{p,r}A_{r,k}
    \]
\end{exercise}

\begin{proof}
    Let $B = A^{2}$. The entry in row $j$, column $k$ of $A^{3}$ is
    \[
        \sum^{n}_{p=1}A_{j,p}B_{p,k}.
    \]

    The entry in row $p$, column $k$ of $B = A^{2}$ is
    \[
        \sum^{n}_{r=1}A_{p,r}A_{r,k}.
    \]

    So the entry in row $j$, column $k$ of $A^{3}$ is
    \[
        \sum^{n}_{p=1}A_{j,p}B_{p,k} = \sum^{n}_{p=1}A_{j,p}\sum^{n}_{r=1}A_{p,r}A_{r,k} = \sum^{n}_{p=1}\sum^{n}_{r=1}A_{j,p}A_{p,r}A_{r,k}.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionC:exercise14
\begin{exercise}
    Suppose $m$ and $n$ are positve integers. Prove that the function $A\mapsto A^{t}$ is a linear map from $\mathbb{F}^{m,n}$ to $\mathbb{F}^{n,m}$.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionC:exercise15
\begin{exercise}
    Prove that if $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix, then
    \[
        {(AC)}^{t} = C^{t}A^{t}.
    \]
\end{exercise}

\begin{proof}
    $AC$ is a $m$-by-$p$ matrix, so ${(AC)}^{t}$ is a $p$-by-$m$ matrix. $C^{t}$ is a $p$-by-$n$ matrix, $A^{t}$ is a $n$-by-$m$ matrix, so $C^{t}A^{t}$ is a $p$-by-$m$ matrix.

    The entry in row $j$, column $k$ of ${(AC)}^{t}$ is also the entry in row $k$, column $j$ of $AC$, which equals row $k$ of $A$ times column $k$ of $C$.

    The entry in row $j$, column $k$ of $C^{t}A^{t}$ equals row $j$ of $C^{t}$ times column $k$ of $A^{t}$, equals row $k$ of $A$ times column $j$ of $C$.

    Thus ${(AC)}^{t} = C^{t}A^{t}$.
\end{proof}
\newpage

% chapter3:sectionC:exercise16
\begin{exercise}
    Suppose $A$ is an $m$-by-$n$ matrix with $A\ne 0$. Prove that the rank of $A$ is $1$ if and only if there exist $(c_{1}, \ldots, c_{m})\in\mathbb{F}^{m}$ and $(d_{1}, \ldots, d_{n})\in\mathbb{F}^{n}$ such that $A_{j,k} = c_{j}d_{k}$ for every $j = 1, \ldots, m$ and every $k = 1, \ldots, n$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exist $(c_{1}, \ldots, c_{m})\in\mathbb{F}^{m}$ and $(d_{1}, \ldots, d_{n})\in\mathbb{F}^{n}$ such that $A_{j,k} = c_{j}d_{k}$ for every $j = 1, \ldots, m$ and every $k = 1, \ldots, n$.

    Let $v = \begin{pmatrix}d_{1} & \cdots & d_{n}\end{pmatrix}$ be a row matrix in $\mathbb{F}^{1,n}$. Then row $j$ of $A$, which is $A_{j,\cdot}$, equals $c_{j} v$, for every $j = 1, \ldots, m$. Because $A\ne 0$, then $v\ne 0$ and there exists a $c_{j}\ne 0$. So every other row is a multiple of the row $j$ of $A$. Therefore $\rank{A} = 1$.

    $(\Leftarrow)$ $\rank{A} = 1$.

    Let $v = (d_{1}, \ldots, d_{n})$ be a basis of the span of rows of $A$ in $\mathbb{F}^{1,n}$. Then for every $j = 1, \ldots, m$, there exists a scalar $c_{j}$ such that $A_{j,\cdot} = c_{j}v$. Hence there exist $(c_{1}, \ldots, c_{m})\in\mathbb{F}^{m}$ and $(d_{1}, \ldots, d_{n})\in\mathbb{F}^{n}$ such that $A_{j,k} = c_{j}d_{k}$ for every $j = 1, \ldots, m$ and every $k = 1, \ldots, n$.
\end{proof}
\newpage

% chapter3:sectionC:exercise17
\begin{exercise}
    Suppose $T\in \mathcal{L}(V)$, and $u_{1}, \ldots, u_{n}$ and $v_{1}, \ldots, v_{n}$ are bases of $V$. Prove that the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item $T$ is injective.
        \item The columns of $\mathcal{M}(T)$ are linearly independent in $\mathbb{F}^{n,1}$.
        \item The columns of $\mathcal{M}(T)$ span $\mathbb{F}^{n,1}$.
        \item The rows of $\mathcal{M}(T)$ span $\mathbb{F}^{1,n}$.
        \item The rows of $\mathcal{M}(T)$ are linearly independent in $\mathbb{F}^{1,n}$.
    \end{enumerate}

    Here $\mathcal{M}(T)$ means $\mathcal{M}(T, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n}))$.
\end{exercise}

\begin{proof}
    Let $A = \mathcal{M}(T, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n}))$.

    $(a) \Rightarrow (b)$ $T$ is injective, then $\sum^{n}_{i=1}a_{i}Tu_{i} = T(\sum^{n}_{i=1}a_{i}u_{i})$ is $0$ if and only if $\sum^{n}_{i=1}a_{i}u_{i} = 0$ (because when $T$ is injective, $\kernel{T} = \{0\}$). $\sum^{n}_{i=1}a_{i}u_{i} = 0$ if and only if $a_{1} = \cdots = a_{n} = 0$, because $u_{1}, \ldots, u_{n}$ is a basis of $V$. So $Tu_{1}, \ldots, Tu_{n}$ is linearly independent.

    Assume that $x_{1}A_{\cdot,1} + \cdots + x_{n}A_{\cdot,n} = 0$. Then $\sum^{n}_{j=1}A_{i,j}x_{j} = 0$ for every positive integer $i = 1, \ldots, n$, and
    \begin{align*}
        \sum^{n}_{i=1}x_{i}Tu_{i} & = \sum^{n}_{i=1}x_{i}\left(\sum^{n}_{j=1}A_{j,i}v_{j}\right)  \\
                                  & = \sum^{n}_{i=1} \left(\sum^{n}_{i=1}A_{j,i}x_{i}\right)v_{i} \\
                                  & = \sum^{n}_{i=1} 0v_{i} = 0.
    \end{align*}

    So $x_{1} = \cdots = x_{n} = 0$, because $Tu_{1}, \ldots, Tu_{n}$ is linearly independent. Therefore the columns of $\mathcal{M}(T)$ are linearly independent in $\mathbb{F}^{n,1}$.

    $(a) \Leftarrow (b)$ The columns of $\mathcal{M}(T)$ are linearly independent in $\mathbb{F}^{n,1}$.

    Assume that $x_{1}Tu_{1} + \cdots + x_{n}Tu_{n} = 0$. Then
    \begin{align*}
        0 = \sum^{n}_{i=1}x_{i}Tu_{i} & = \sum^{n}_{i=1}x_{i}\left(\sum^{n}_{j=1}A_{j,i}v_{j}\right)   \\
                                      & = \sum^{n}_{i=1} \left(\sum^{n}_{i=1}A_{j,i}x_{i}\right)v_{i}.
    \end{align*}

    Since $v_{1}, \ldots, v_{n}$ is a basis of $V$, then $\sum^{n}_{i=1}A_{j,i}x_{i} = 0$ for every $j = 1, \ldots, n$, equivalently, $\sum^{n}_{i=1}x_{i}A_{\cdot,i} = 0$. Besides, the columns of $\mathcal{M}(T)$ are linearly independent, so $x_{1} = \cdots = x_{n} = 0$. Therefore $Tu_{1}, \ldots, Tu_{n}$ is linear independent and is a basis of $V$. Let $u$ be a vector in $\kernel{T}$, then $u = \sum^{n}_{i=1}a_{i}u_{i}$. So $0 = Tu = \sum^{n}_{i=1}a_{i}Tu_{i}$, it follows that $a_{1} = \cdots = a_{n} = 0$, which means $u = 0$. Hence $\kernel{T} = \{0\}$, and we conclude that $T$ is injective.

    $(b) \Leftrightarrow (c)$ If the columns of $\mathcal{M}(T)$ are linearly independent in $\mathbb{F}^{n,1}$ then they form a basis of $\mathbb{F}^{n,1}$, since there are $n$ of them and they are linearly independent. So they also span $\mathbb{F}^{n,1}$.

    If the columns of $\mathcal{M}(T)$ span $\mathbb{F}^{n,1}$, then the dimension of the span of them is $n$, which equals to the number of columns. So they are also a basis of $\mathbb{F}^{n,1}$, which is linearly independent.

    $(c) \Leftrightarrow (d)$ The columns of $\mathcal{M}(T)$ span $\mathbb{F}^{n,1}$ if and only if the dimension of the span of the columns is $n$. The rows of $\mathcal{M}(T)$ span $\mathbb{F}^{1,n}$ if and only if the dimension of the span of the rows is $n$. On the other hand, the row rank and column rank of $\mathcal{M}(T)$ are identical, then the result follows.

    $(d) \Leftrightarrow (e)$ If the rows of $\mathcal{M}(T)$ span $\mathbb{F}^{1,n}$, then the dimension of the span of them is $n$, which equals to the number of rows. So they are also a basis of $\mathbb{F}^{1,n}$, which is linearly independent.

    If the rows of $\mathcal{M}(T)$ are linearly independent in $\mathbb{F}^{1,n}$ then they form a basis of $\mathbb{F}^{n,1}$, since there are $n$ of them and they are linearly independent. So they also span $\mathbb{F}^{1,n}$.
\end{proof}
\newpage

\section{Invertibility and Isomorphisms}

% chapter3:sectionD:exercise1
\begin{exercise}\label{chapter3:sectionD:exercise1}
    Suppose $T \in \mathcal{L}(V, W)$ is invertible. Show that $T^{-1}$ is invertible and
    \[
        {\left(T^{-1}\right)}^{-1} = T.
    \]
\end{exercise}

\begin{proof}
    Since $T^{-1}$ is the inverse of $T$, then $T^{-1}T = \operatorname{id}_{V}$ and $TT^{-1} = \operatorname{id}_{W}$. Due to the definition of inverse linear map, $T$ is the inverse of $T^{-1}$. Therefore ${\left(T^{-1}\right)}^{-1} = T$.
\end{proof}
\newpage

% chapter3:sectionD:exercise2
\begin{exercise}
    Suppose $T\in \mathcal{L}(U, V)$ and $S \in \mathcal{L}(V, W)$ are both invertible linear maps. Prove that $ST\in \mathcal{L}(U, W)$ is invertible and that ${(ST)}^{-1} = T^{-1}S^{-1}$.
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            {(ST)}{(T^{-1}S^{-1})} = S(TT^{-1})S^{-1} = S\operatorname{id}_{V}S^{-1} = SS^{-1} = \operatorname{id}_{W}, \\
            {(T^{-1}S^{-1})(ST)} = T^{-1}(S^{-1}S)T = T^{-1}\operatorname{id}_{V}T = T^{-1}T = \operatorname{id}_{U}
        \end{split}
    \]

    So $ST$ is invertible and ${(ST)}^{-1} = T^{-1}S^{-1}$.
\end{proof}
\newpage

% chapter3:sectionD:exercise3
\begin{exercise}\label{chapter3:sectionD:exercise3}
    Suppose $V$ is finite-dimensional and $T \in \mathcal{L}(V)$. Prove that the following are equivalent
    \begin{enumerate}[label={(\alph*)}]
        \item $T$ is invertible.
        \item $Tv_{1}, \ldots, Tv_{n}$ is a basis of $V$ for every basis $v_{1}, \ldots, v_{n}$ of $V$.
        \item $Tv_{1}, \ldots, Tv_{n}$ is a basis of $V$ for some basis $v_{1}, \ldots, v_{n}$ of $V$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    $(a) \Rightarrow (b)$ Let $v_{1}, \ldots, v_{n}$ be a basis of $V$. Suppose that
    \[
        0 = x_{1}Tv_{1} + \cdots + x_{n}T_{n}.
    \]

    Then $T(x_{1}v_{1} + \cdots + x_{n}v_{n}) = 0$. Because $T$ is invertible,
    \[
        x_{1}v_{1} + \cdots + x_{n}v_{n} = T^{-1}T(x_{1}v_{1} + \cdots + x_{n}v_{n}) = 0.
    \]

    It follows that $x_{1} = \cdots = x_{n} = 0$. So $Tv_{1}, \ldots, Tv_{n}$ is linearly independent. This list has length $n$ so it is a basis of $V$.

    $(b) \Rightarrow (c)$ This is obvious.

    $(c) \Rightarrow (a)$ $Tv_{1}, \ldots, Tv_{n}$ is a basis of $V$ for a basis $v_{1}, \ldots, v_{n}$ of $V$.

    Let $v$ be a vector in $\kernel{T}$, and $v = x_{1}v_{1} + \cdots + x_{n}v_{n}$ (there exist such scalar because $v_{1}, \ldots, v_{n}$ is a basis of $V$).
    \[
        0 = Tv = T(x_{1}v_{1} + \cdots + x_{n}v_{n}) = x_{1}Tv_{1} + \cdots + x_{n}Tv_{n}.
    \]

    Since $Tv_{1}, \ldots, Tv_{n}$ is linearly independent, we get $x_{1} = \cdots = x_{n}$. So $v = 0$, which means $\kernel{T} = \{0\}$ and $T$ is injective. On the other hand, $Tv_{1}, \ldots, Tv_{n}$ spans $V$, the codomain of $T$, so $T$ is surjective. $T$ is a linear map whose domain and codomain are finite-dimensional and have the same dimension, so $T$ is invertible.
\end{proof}
\newpage

% chapter3:sectionD:exercise4
\begin{exercise}
    Suppose $V$ is finite-dimensional and $\dim V > 1$. Prove that the set of noninvertible linear maps from $V$ to itself is not a subspace of $\mathcal{L}(V)$.
\end{exercise}

\begin{proof}
    Let $v_{1}, v_{2}, \ldots, v_{n}$ be a basis of $V$.

    Let $S_{i}$ be the linear map that
    \[
        S_{i}v_{j} = \begin{cases}
            v_{i} & \text{if $i = j$}, \\
            0     & \text{otherwise}.
        \end{cases}
    \]

    Then $S_{1}, \ldots, S_{n}$ are noninvertible. But $\sum^{n}_{k=1}S_{k} = \operatorname{id}_{V}$, which is invertible. So the set of noninvertible linear maps from $V$ to itself is not a subspace of $\mathcal{L}(V)$, because it is not closed under linear maps addition.
\end{proof}
\newpage

% chapter3:sectionD:exercise5
\begin{exercise}
    Suppose $V$ is finite-dimensional, $U$ is a subspace of $V$, and $S \in \mathcal{L}(U, V)$. Prove that there exists an invertible linear map $T$ from $V$ to itself such that $Tu = Su$ for every $u\in U$ if and only if $S$ is injective.
\end{exercise}

\begin{proof}
    Let $u_{1}, \ldots, u_{k}$ be a basis of $U$. Extend this list to a basis of $V$ and let it be $u_{1}, \ldots, u_{k}, u_{k+1}, \ldots, u_{n}$.

    $(\Rightarrow)$ There exists an invertible linear map $T$ from $V$ to itself such that $Tu = Su$ for every $u\in U$.

    Let $v$ be a vector in $\kernel{S}$, then $Tv = Sv = 0$. Because $T$ is invertible, then $T$ is also injective, so $Tv = 0$ implies $v = 0$. Therefore $\kernel{S} = \{0\}$. Hence $S$ is injective.

    $(\Leftarrow)$ $S$ is injective.

    Because $S$ is injective, the list $Su_{1}, \ldots, Su_{k}$ is linearly independent. Extend this list to create a basis of $V$ and let it be $Su_{1}, \ldots, Su_{k}, v_{k+1}, \ldots, v_{n}$. I define the linear map $T$ as follows:
    \[
        Tu_{i} = \begin{cases}
            Su_{i} & \text{if $1\leq i\leq k$} \\
            v_{i}  & \text{otherwise}.
        \end{cases}
    \]

    Apply Exercise~\ref{chapter3:sectionD:exercise3} to $T$ and bases $u_{1}, \ldots, u_{n}$ and $Su_{1}, \ldots, Su_{k}, v_{k+1}, \ldots, v_{n}$, we conclude that $T$ is invertible.

    Let $u$ be a vector in $U$, and $u = x_{1}u_{1} + \cdots + x_{k}u_{k}$. According to the definition of $T$
    \[
        Tu = T\left(\sum^{k}_{j=1}x_{j}u_{j}\right) = \sum^{k}_{j=1}x_{j}Tu_{j} = \sum^{k}_{j=1}Su_{j} = S\left(\sum^{k}_{j=1}x_{j}u_{j}\right) = Su.\qedhere
    \]

    for every $u\in U$.
\end{proof}
\newpage

% chapter3:sectionD:exercise6
\begin{exercise}
    Suppose that $W$ is finite-dimensional and $S, T \in \mathcal{L}(V, W)$. Prove that $\kernel{S} = \kernel{T}$ if and only if there exists an invertible $E \in \mathcal{L}(W)$ such that $S = ET$.
\end{exercise}

\begin{proof}
    $W$ is finite-dimensional, then so are $\range{S}$ and $\range{T}$.

    $(\Rightarrow)$ There exists an invertible $E \in \mathcal{L}(W)$ such that $S = ET$.

    If vector $v\in\kernel{T}$, then $Sv = (ET)(v) = E(Tv) = 0$, so $v\in\kernel{S}$. If vector $v\in\kernel{S}$, then $Tv = (E^{-1}S)v = E^{-1}(Sv) = 0$, so $v\in\kernel{T}$. Therefore $\kernel{S} = \kernel{T}$.

    $(\Leftarrow)$ $\kernel{S} = \kernel{T}$. Let $U = \kernel{S}$, of course $U = \kernel{T}$.

    Let $Sv_{1}, \ldots, Sv_{n}$ be a basis of $\range{S}$, then $v_{1}, \ldots, v_{n}$ is linearly independent in $V$. Let $v$ be a vector in $V$. There exist scalars $x_{k}$ for $k = 1,\ldots, n$ such that $Sv = x_{1}Sv_{1} + \cdots + x_{n}Sv_{n}$. So
    \[
        S(v - x_{1}v_{1} - \cdots - x_{n}v_{n}) = 0.
    \]

    Therefore $v - x_{1}v_{1} - \cdots - x_{n}v_{n}$ is in $U$ and $V = U + \operatorname{span}(v_{1}, \ldots, v_{n})$.

    Let $u$ be a vector in $U\cap \operatorname{span}(v_{1}, \ldots, v_{n})$. Then $Su = 0$ and there exist scalars $a_{k}$ for $k = 1,\ldots, n$ such that
    \[
        u = a_{1}v_{1} + \cdots + a_{n}v_{n}.
    \]

    $0 = Su = a_{1}Sv_{1} + \cdots + a_{n}Sv_{n}$. Since $Sv_{1}, \ldots, Sv_{n}$ is linearly independent, it follows that $a_{1} = \cdots = a_{n} = 0$. So $U\cap \operatorname{span}(v_{1}, \ldots, v_{n}) = \{ 0 \}$. Therefore $V = U \oplus \operatorname{span}(v_{1}, \ldots, v_{n})$.

    Let $v$ be a vector in $V$, then $v = u + c_{1}v_{1} + \cdots + c_{n}v_{n}$.
    \[
        Tv = Tu + c_{1}Tv_{1} + \cdots + c_{n}Tv_{n} = c_{1}Tv_{1} + \cdots + c_{n}Tv_{n}.
    \]

    Therefore $Tv_{1}, \ldots, Tv_{n}$ spans $\range{T}$.

    Let $d_{1}Tv_{1} + \cdots + d_{n}Tv_{n} = 0$ be a linear combination of $0$ in $\range{T}$, then $T(d_{1}v_{1} + \cdots + d_{n}v_{n}) = 0$. So $d_{1}v_{1} + \cdots + d_{n}v_{n}$ is in $\kernel{T} = U$. However, $\kernel{T}\cap\operatorname{span}(v_{1}, \ldots, v_{n}) = \{0\}$, so $d_{1}v_{1} + \cdots + d_{n}v_{n} = 0$, and $d_{1}v_{1} + \cdots + d_{n}v_{n} = 0$ implies $d_{1} = \cdots = d_{n} = 0$ because $v_{1}, \ldots, v_{n}$ is linearly independent. So $Tv_{1}, \ldots, Tv_{n}$ is linearly independent.

    Hence $Tv_{1}, \ldots, Tv_{n}$ is a basis of $\range{T}$.

    Extend $Sv_{1}, \ldots, Sv_{n}$ to create a basis of $W$ and let it be $Sv_{1}, \ldots, Sv_{n}, w_{n+1}, \ldots, w_{n+m}$.

    Extend $Tv_{1}, \ldots, Tv_{n}$ to create a basis of $W$ and let it be $Tv_{1}, \ldots, Tv_{n}, z_{n+1}, \ldots, z_{n+m}$.

    I define the linear map $E$ in $\lmap{W}$ as follows:
    \[
        E(Tv_{k}) = Sv_{k}\quad \text{(if $1\leq k\leq n$)} \qquad\text{and}\qquad Ez_{k} = w_{k} \quad\text{(if $n+1\leq k\leq n+m$)}.
    \]

    Let $v$ be a vector in $V$, and $v = u + x_{1}v_{1} + \cdots + x_{n}v_{n}$, where $u\in U$.
    \begin{align*}
        (ET)(v) & = E(Tu + x_{1}Tv_{1} + \cdots + x_{n}Tv_{n})               \\
                & = E(x_{1}Tv_{1} + \cdots + x_{n}Tv_{n})                    \\
                & = \sum^{n}_{k=1}x_{k}E(Tv_{k}) = \sum^{n}_{k=1}x_{k}Sv_{k} \\
                & = Su + \sum^{n}_{k=1}x_{k}Sv_{k}                           \\
                & = S(u + x_{1}v_{1} + \cdots + x_{n}v_{n})                  \\
                & = Sv.
    \end{align*}

    I define the linear map $F$ in $\lmap{W}$ as follows:
    \[
        F(Sv_{k}) = Tv_{k}\quad \text{(if $1\leq k\leq n$)} \qquad\text{and}\qquad Fw_{k} = z_{k} \quad\text{(if $n+1\leq k\leq n+m$)}.
    \]

    Then $EF = FE = \operatorname{id}_{W}$, due to the definition of these two linear maps. Therefore $E$ is invertible and $S = ET$.
\end{proof}
\newpage

% chapter3:sectionD:exercise7
\begin{exercise}
    Suppose that $V$ is finite-dimensional and $S, T \in \mathcal{L}(V, W)$. Prove that $\range{S} = \range{T}$ if and only if there exists an invertible $E \in \mathcal{L}(V)$ such that $S = TE$.
\end{exercise}

\begin{proof}
    Since $V$ is finite-dimensional, then so are $\range{S}$ and $\range{T}$.

    $(\Rightarrow)$ There exists an invertible $E \in \mathcal{L}(V)$ such that $S = TE$.

    Then $T = SE^{-1}$.

    If $w$ is a vector in $\range{S}$, then there exists a vector $v$ in $V$ such that $Sv = w$. So $(TE)(v) = w$, which means $T(Ev) = w$, and $w$ is also in $\range{T}$.

    If $w$ is a vector in $\range{T}$, then there exists a vector $v$ in $V$ such that $Tv = w$. So $(SE^{-1})(v) = w$, which means $S(E^{-1}v) = w$, and $w$ is also in $\range{S}$.

    Hence $\range{S} = \range{T}$.

    $(\Leftarrow)$ $\range{S} = \range{T}$. Let $Z = \range{S}$, and of course $Z = \range{T}$.

    Let $w_{1}, \ldots, w_{m}$ be a basis of $Z$. Let $Su_{k} = w_{k}$ and $Tv_{k} = w_{k}$ for $k = 1, \ldots, m$. Then $u_{1}, \ldots, u_{m}$ is linearly independent and $v_{1}, \ldots, v_{m}$ is linearly independent.

    Let $v$ be a vector in $V$, then $Tv = x_{1}Tv_{1} + \cdots + x_{m}Tv_{m}$. From this we get
    \[
        T(v - x_{1}v_{1} - \cdots - x_{m}v_{m}) = 0.
    \]

    This means $v - x_{1}v_{1} - \cdots - x_{m}v_{m}\in \kernel{T}$, so $V = \kernel{T} + \operatorname{span}(v_{1}, \ldots, v_{m})$. Let $v_{0}\in \kernel{T} \cap \operatorname{span}(v_{1}, \ldots, v_{m})$ and $v_{0} = a_{1}v_{1} + \cdots + a_{m}v_{m}$ then
    \[
        0 = Tv_{0} = T(a_{1}v_{1} + \cdots + a_{m}v_{m}) = a_{1}Tv_{1} + \cdots + a_{m}Tv_{m} = a_{1}w_{1} + \cdots + a_{m}w_{m}.
    \]

    Because $w_{1}, \ldots, w_{m}$ is linearly independent, it follows that $a_{1} = \cdots = a_{m} = 0$, and $v_{0} = 0$. Therefore $\kernel{T} + \operatorname{span}(v_{1}, \ldots, v_{m}) = \{0\}$ and $V = \kernel{T} \oplus \operatorname{span}(v_{1}, \ldots, v_{m})$. Let $v_{m+1}, \ldots, v_{m+n}$ be a basis of $\kernel{T}$.

    Similarly, $V = \kernel{S} \oplus\operatorname{span}(u_{1}, \ldots, u_{m})$. Let $u_{m+1}, \ldots, u_{m+n}$ be a basis of $\kernel{T}$.

    I define the linear map $E,F\in\lmap{V}$ as follows: $Eu_{k} = v_{k}$, $Fv_{k} = u_{k}$ for $k = 1,\ldots, m+n$. By this definition, $EF = FE = \operatorname{id}_{V}$.

    If $1\leq k\leq m$, $(TE)(u_{k}) = T(Eu_{k}) = Tv_{k} = w_{k} = Su_{k}$.

    If $m+1\leq k\leq m+n$, $(TE)(u_{k}) = T(Eu_{k}) = Tv_{k} = 0 = Su_{k}$.

    Hence $E$ is an invertible linear map in $\lmap{V}$ and $S = TE$.
\end{proof}
\newpage

% chapter3:sectionD:exercise8
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional and $S, T \in \mathcal{L}(V, W)$. Prove that there exist invertible $E_{1} \in \mathcal{L}(V)$ and $E_{2} \in \mathcal{L}(W)$ such that $S = E_{2} TE_{1}$ if and only if $\dim \kernel{S} = \dim \kernel{T}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exist invertible $E_{1} \in \mathcal{L}(V)$ and $E_{2} \in \mathcal{L}(W)$ such that $S = E_{2} TE_{1}$.

    Let $v_{S}$ be a vector in $\kernel{S}$, $v_{T}$ be a vector in $\kernel{T}$.
    \[
        \begin{split}
            Sv_{S} = 0 \Leftrightarrow (E_{2}TE_{1})(v_{S}) = 0 \Leftrightarrow (TE_{1})(v_{S}) = 0 \Leftrightarrow T(E_{1}v_{S}) = 0, \\
            Tv_{T} = 0 \Leftrightarrow (E^{-1}_{2}SE^{-1}_{1})(v_{T}) = 0 \Leftrightarrow (SE^{-1}_{1})(v_{T}) = 0 \Leftrightarrow S(E^{-1}_{1}v_{T}) = 0,
        \end{split}
    \]

    so $E_{1}v_{S}\in \kernel{T}$, and $E^{-1}v_{T}\in \kernel{S}$. I define the linear maps $F_{S}\in\lmap{\kernel{S},\kernel{T}}$ and $F_{T}\in\lmap{\kernel{T},\kernel{S}}$ as follows:
    \[
        F_{S}: v\mapsto E_{1}v\qquad  F_{T}: v\mapsto E^{-1}_{1}v
    \]

    Then $F_{S}F_{T} = \operatorname{id}_{\kernel{T}}$ and $F_{T}F_{S} = \operatorname{id}_{\kernel{S}}$. So $F_{S}$ and $F_{T}$ are isomorphisms. Therefore $\kernel{S}$ and $\kernel{T}$ are isomorphic, and $\dim\kernel{S} = \dim\kernel{T}$.

    $(\Leftarrow)$ $\dim\kernel{S} = \dim\kernel{T}$.

    According to the fundamental theorem of linear maps, $\dim\range{S} = \dim V - \dim\kernel{S}$ and $\dim\range{T} = \dim V - \dim\kernel{T}$.

    It follows that $\kernel{S}$ and $\kernel{T}$ are isomorphic, $\range{S}$ and $\range{T}$ are isomorphic.

    Let $Su_{1}, \ldots, Su_{m}$ be a basis of $\range{S}$; $Tv_{1}, \ldots, Tv_{m}$ be a basis of $\range{T}$. Remind that
    \[
        V = \kernel{S}\oplus\operatorname{span}(u_{1}, \ldots, u_{m}) = \kernel{T}\oplus\operatorname{span}(v_{1}, \ldots, v_{m}).
    \]

    Let $u_{m+1}, \ldots, u_{m+n}$ be a basis of $\kernel{S}$, and $v_{m+1}, \ldots, v_{m+n}$ be a basis of $\kernel{T}$. Then $u_{1}, \ldots, u_{m+n}$ and $v_{1}, \ldots, v_{m+n}$ are bases of $V$.

    Extend $Su_{1}, \ldots, Su_{m}$ to create a basis of $W$ and let it be $Su_{1}, \ldots, Su_{m}, u'_{m+1}, \ldots, u'_{m+p}$. Extend $Tv_{1}, \ldots, Tv_{m}$ to create a basis of $W$ and let it be $Tv_{1}, \ldots, Tv_{m}, v'_{m+1}, \ldots, v'_{m+p}$.

    I define $E_{1}\in\lmap{V}$ and $E_{2}\in\lmap{W}$ as follows:
    \begin{itemize}
        \item $E_{1}u_{k} = v_{k}$ for $k = 1, \ldots, m+n$
        \item $E_{2}(Tv_{k}) = Su_{k}$ for $k = 1,\ldots, m$, and $E_{2}v'_{k} = u'_{k}$ for $k = m+1, \ldots, m+p$.
    \end{itemize}

    Then $E_{1}$ and $E_{2}$ are invertible and $S = E_{2}TE_{1}$.
\end{proof}
\newpage

% chapter3:sectionD:exercise9
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T: V \to W$ is a surjective linear map of $V$ onto $W$. Prove that there is a subspace $U$ of $V$ such that $T\vert_{U}$ is an isomorphism of $U$ onto $W$.
\end{exercise}

\begin{proof}
    Since $V$ is finite-dimensional, $\range{T}$ is also finite-dimensional, due to the fundamental theorem of linear maps. Because $T$ is a surjective linear map, it follows that $\range{T} = W$.

    Let $Tu_{1}, \ldots, Tu_{n}$ be a basis of $\range{T} = W$, then $u_{1}, \ldots, u_{n}$ is linearly independent in $V$.

    Let $U = \operatorname{span}(u_{1}, \ldots, u_{n})$, and $u\in U$. Let $w$ be a vector in $W$, then there exist scalars $a_{1}, \ldots, a_{n}$ such that
    \[
        a_{1}Tu_{1} + \cdots + a_{n}Tu_{n} = w.
    \]

    So $T(a_{1}u_{1} + \cdots + a_{n}u_{n}) = w$. Therefore $T\vert_{U}$ is a linear map from $U$ onto $W$. On the other hand, $\dim U = \dim W = n$, so $T\vert_{U}$ is also an isomorphism of $U$ onto $W$.
\end{proof}
\newpage

% chapter3:sectionD:exercise10
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional and $U$ is a subspace of $V$. Let
    \[
        \mathcal{E} = \{ T\in\mathcal{L}(V, W): U\subseteq \kernel{T} \}.
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that $\mathcal{E}$ is a subspace of $\mathcal{L}(V, W)$.
        \item Find a formula for $\dim\mathcal{E}$ in terms of $\dim V$, $\dim W$, $\dim U$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    $0\in \lmap{V, W}$ is in $\mathcal{E}$, because $\kernel{0} = V$ and $U$ is a subspace of $V$.

    If $S, T$ are in $\mathcal{E}$, then $S + T$ is also in $\mathcal{E}$, because for every $u\in U$
    \[
        (S + T)(u) = Su + Tu = 0 + 0 = 0.
    \]

    $\lambda S$ is also in $\mathcal{E}$ for every $\lambda\in\mathbb{F}$ because for every $u\in U$
    \[
        (\lambda S)(u) = \lambda Su = \lambda 0 = 0.
    \]

    So $\mathcal{E}$ is a subspace of $\lmap{V, W}$.

    \bigskip

    Let $u_{1}, \ldots, u_{m}$ be a basis of $U$, extend this list to create a basis of $V$ and let it be $u_{1}, \ldots, u_{m}, v_{1}, \ldots, v_{n}$. Let $V_{1} = \operatorname{span}(v_{1}, \ldots, v_{n})$.

    I define a map $f$ from $\mathcal{E}$ to $\lmap{V_{1}, W}$ as follows: $f: T\mapsto T\vert_{V_{1}}$. $f$ is a linear map.

    Let $S$ be a linear map in $\kernel{f}$, then $S\vert_{V_{1}}v_{k} = 0$ for $k = 1, \ldots, n$, so $S = 0$. Therefore $\kernel{f} = 0$ and $f$ is injective.

    On the other hand, for every $R$ in $\lmap{V_{1}, W}$, there exists a linear map $Z$ such that $Zu_{1} = \cdots = Zu_{m} = 0$ and $Zv_{k} = Rv_{k}$ for $k = 1, \ldots, n$. So $f$ is surjective.

    Therefore $f$ is an isomorphism from $\mathcal{E}$ to $\lmap{V_{1}, W}$. Hence
    \[
        \dim\mathcal{E} = (\dim V_{1})(\dim W) = (\dim V - \dim U)(\dim W).
    \]
\end{proof}
\newpage

% chapter3:sectionD:exercise11
\begin{exercise}\label{chapter3:sectionD:exercise11}
    Suppose $V$ is finite-dimensional and $S, T \in \mathcal{L}(V)$. Prove that
    \[
        \text{$ST$ is invertible} \Longleftrightarrow \text{$S$ and $T$ are invertible.}
    \]
\end{exercise}

\begin{proof}
    If $S, T$ are invertible, then $ST$ is invertible.

    If $ST$ is invertible, then $ST$ is injective and surjective. For each vector $w$ in $W$, there exists vector $v$ in $V$ such that $(ST)(v) = w$. $S(Tv) = w$ implies that $S$ is surjective. If $v_{1}, v_{2}$ are two different vectors in $V$, then $(ST)(v_{1})\ne (ST)(v_{2})$, and $Tv_{1}\ne Tv_{2}$, so $T$ is injective. Because $S, T\in\lmap{V}$ and $V$ is finite-dimensional, and $S$ is surjective, $T$ is injective, it follows that $S$ and $T$ are invertible.
\end{proof}
\newpage

% chapter3:sectionD:exercise12
\begin{exercise}\label{chapter3:sectionD:exercise12}
    Suppose $V$ is finite-dimensional and $S, T, U \in \mathcal{L}(V)$ and $STU = I$. Show that $T$ is invertible and that $T^{-1} = US$.
\end{exercise}

\begin{proof}
    $I = (S)(TU) = (ST)(U)$. According to Exercise~\ref{chapter3:sectionD:exercise11}, $S$ and $U$ are invertible.
    \[
        T = ITI = (S^{-1}S)T(UU^{-1}) = S^{-1}(STU)U^{-1} = S^{-1}IU^{-1} = S^{-1}U^{-1}.
    \]

    According Exercise~\ref{chapter3:sectionD:exercise1}, $T^{-1} = US$.
\end{proof}
\newpage

% chapter3:sectionD:exercise13
\begin{exercise}
    Show that the result in Exercise~\ref{chapter3:sectionD:exercise12} can fail without the hypothesis that $V$ is finite-dimensional.
\end{exercise}

\begin{proof}
    Let $V$ be the space of sequences of real numbers.

    Let $T$ be the identity map in $\lmap{V}$. I define $U$ and $S$ in $\lmap{V}$ as follows:
    \[
        \begin{split}
            U: (a_{1}, a_{2}, \ldots) \mapsto (0, a_{1}, a_{2}, \ldots) \\
            S: (a_{1}, a_{2}, \ldots) \mapsto (a_{2}, a_{3}, \ldots)
        \end{split}
    \]

    Then $STU = I$. However, $T^{-1} = I\ne US$ because $(1, 0, \ldots)$ is in $\kernel{US}$.
\end{proof}
\newpage

% chapter3:sectionD:exercise14
\begin{exercise}
    Prove or give a counterexample: If $V$ is a finite-dimensional vector space
    and $R, S, T \in \mathcal{L}(V)$ are such that $RST$ is surjective, then $S$ is injective.
\end{exercise}

\begin{proof}
    $RST\in\lmap{V}$, $V$ is finite-dimensional, and $RST$ is surjective, then it follows that $RST$ is invertible. According to Exercise~\ref{chapter3:sectionD:exercise12}, $S$ is invertible. So $S$ is injective.
\end{proof}
\newpage

% chapter3:sectionD:exercise15
\begin{exercise}
    Suppose $T \in \mathcal{L}(V)$ and $v_{1} , \ldots, v_{m}$ is a list in $V$ such that $Tv_{1}, \ldots, Tv_{m}$ spans $V$. Prove that $v_{1}, \ldots, v_{m}$ spans $V$.
\end{exercise}

\begin{proof}
    Since $Tv_{1}, \ldots, Tv_{m}$ spans $V$, it follows that $V$ is finite-dimensional, and $T$ is surjective. So $T$ is also invertible.

    Let $v$ be a vector in $V$, then there exist scalar $a_{k}$ for $k = 1, \ldots, m$ such that
    \[
        Tv = a_{1}Tv_{1} + \cdots + a_{m}Tv_{m}.
    \]

    So
    \begin{align*}
        v = T^{-1}(Tv) & = T^{-1}(a_{1}Tv_{1} + \cdots + a_{m}Tv_{m})         \\
                       & = a_{1}T^{-1}(Tv_{1}) + \cdots + a_{m}T^{-1}(Tv_{m}) \\
                       & = a_{1}v_{1} + \cdots + a_{m}v_{m}.
    \end{align*}

    Therefore $v_{1}, \ldots, v_{m}$ spans $V$.
\end{proof}
\newpage

% chapter3:sectionD:exercise16
\begin{exercise}
    Prove that every linear map from $\mathbb{F}^{n, 1}$ to $\mathbb{F}^{m, 1}$ is given by a matrix multiplication. In other words, prove that if $T \in \lmap{\mathbb{F}^{n, 1}, \mathbb{F}^{m, 1}}$, then there exists an $m$-by-$n$ matrix $A$ such that $Tx = Ax$ for every $x \in \mathbb{F}^{n, 1}$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be the standard basis of $\mathbb{F}^{n,1}$, $f_{1}, \ldots, f_{m}$ the standard basis of $\mathbb{F}^{m,1}$.

    I define the matrix $A$ as follows:
    \[
        Te_{i} = \sum^{m}_{r=1}A_{r,i}f_{r}.
    \]

    Let $x = x_{1}e_{1} + \cdots + x_{n}e_{n}$ be a vector in $\mathbb{F}^{n,1}$.
    \begin{align*}
        Tx & = \sum^{n}_{i=1}T(x_{i}e_{i}) = \sum^{n}_{i=1}x_{i}Te_{i}     \\
           & = \sum^{n}_{i=1}x_{i}\left(\sum^{m}_{r=1}A_{r,i}f_{r}\right)  \\
           & = \sum^{m}_{r=1}\left(\sum^{n}_{i=1}A_{r,i}x_{i}\right)f_{r}.
    \end{align*}

    $\sum^{n}_{i=1}A_{r,i}x_{i}$ is the multiplication of $A_{r,\cdot}$ and $x$. So $Tx = Ax$.

    \bigskip

    Another solution: $\mathcal{M}(Tx) = \mathcal{M}(T)\mathcal{M}(x)$. Let $A = \mathcal{M}(T)$, then $Tx = Ax$ (in $\mathbb{F}^{n,1}$, $x = \mathcal{M}(x)$).
\end{proof}
\newpage

% chapter3:sectionD:exercise17
\begin{exercise}
    Suppose $V$ is finite-dimensional and $S\in\mathcal{L}(V)$. Define $\mathcal{A}\in \mathcal{L}(\mathcal{L}(V))$ by
    \[
        \mathcal{A}(T) = ST
    \]

    for $T\in\mathcal{L}(V)$.

    \begin{enumerate}[label={(\alph*)}]
        \item Show that $\dim\kernel{\mathcal{A}} = (\dim V)(\dim\kernel{S})$.
        \item Show that $\dim\range{\mathcal{A}} = (\dim V)(\dim\range{S})$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    $\kernel{\mathcal{A}}$ consists of linear operators $T$ in $V$ such that $ST = 0$.

    $ST = 0$ if and only if $\range{T} \subseteq \kernel{S}$. I define the map $f$ from $\kernel{\mathcal{A}}$ to $\lmap{V, \kernel{S}}$ as follows: $T\mapsto \bar{T}$, where $Tv = \bar{T}v$. This map $f$ is linear and an isomorphism. Therefore $\kernel{\mathcal{A}}$ and $\lmap{V, \kernel{S}}$ are isomorphic, and
    \[
        \dim\kernel{\mathcal{A}} = \dim \lmap{V, \kernel{S}} = (\dim V)(\dim\kernel{S}).
    \]

    According to the fundamental theorem of linear maps
    \begin{align*}
        \dim\range{\mathcal{A}} & = \dim\lmap{V} - \dim\kernel{\mathcal{A}}      \\
                                & = (\dim V)(\dim V) - (\dim V)(\dim \kernel{S}) \\
                                & = (\dim V)(\dim V - \dim\kernel{S})            \\
                                & = (\dim V)(\dim\range{S}).\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter3:sectionD:exercise18
\begin{exercise}
    Show that $V$ and $\mathcal{L}(\mathbb{F}, V)$ are isomorphic vector spaces.
\end{exercise}

\begin{proof}
    Let $v$ be a vector in $V$. I define a map $T_{v}$ in $\lmap{\mathbb{F}, V}$ as follows: $T_{v}: 1\mapsto v$. The map $v\mapsto T_{v}$ is a linear map.

    Let $S$ be a linear map in $\lmap{\mathbb{F}, V}$, and $v_{S} = S(1)$. The map $S\mapsto v_{S}$ is a linear map.

    Moreover, the two linear maps $v\mapsto T_{v}$ and $S\mapsto v_{S}$ are the inverses of each other. Therefore $V$ and $\mathcal{L}(\mathbb{F}, V)$ are isomorphic vector spaces.
\end{proof}
\newpage

% chapter3:sectionD:exercise19
\begin{exercise}
    Suppose $V$ is finite-dimensional and $T \in \lmap{V}$. Prove that $T$ has the same matrix with respect to every basis of $V$ if and only if $T$ is a scalar multiple of the identity operator.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $T$ is a scalar multiple of the identity operator.

    Let $v_{1}, \ldots, v_{n}$ be an arbitary basis of $V$.

    $Tv_{k} = \lambda v_{k}$ for $k = 1,\ldots, n$. So the matrix of $T$ with respect to the basis $v_{1}, \ldots, v_{n}$ is
    \[
        \begin{pmatrix}
            \lambda & \cdots & 0       \\
            \vdots  & \ddots & \vdots  \\
            0       & \cdots & \lambda
        \end{pmatrix} = \lambda I
    \]

    which is independent of the basis.

    $(\Leftarrow)$ $T$ has the same matrix with respect to every basis of $V$.

    Let $u_{1}, \ldots, u_{n}$ and $v_{1}, \ldots, v_{n}$ be bases of $V$.
    \[
        A = \mathcal{M}(T, (u_{1}, \ldots, u_{n})) \qquad A = \mathcal{M}(T, (v_{1}, \ldots, v_{n}))
    \]

    Let $C$ be $\mathcal{M}(I, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n}))$, then $A = C^{-1}AC$.

    For every $i,j$ in $1,\ldots, n$ that $i\ne j$, let $C^{i,j}$ be a matrix in $\mathbb{F}^{n,n}$ where every entry is $0$ except that the entries in the diagonal are $1$ and in row $i$, column $j$ is $1$. Let $D^{i,j}$ be a matrix in $\mathbb{F}^{n,n}$ where every entry is $0$ except that the entries in the diagonal are $1$ and in row $i$, column $j$ is $-1$. $C^{i,j}D^{i,j} = D^{i,j}C^{i,j} = I$.

    $AC^{i,j}$ is the same as $A$, except that the column $j$ of $AC^{i,j}$ is the sum of the columns $i$ and $j$ of $A$. $C^{i,j}A$ is the same as $A$, except that the row $i$ of $C^{i,j}A$ is the sum of the rows $i$ and $j$ of $A$.

    Since $AC^{i,j} = C^{i,j}A$ for every $i\ne j$ in $1,\ldots, n$, it follows that
    \begin{itemize}
        \item $A_{i,k} = 0$ for $k\ne i$,
        \item $A_{k,j} = 0$ for $k\ne j$,
        \item $A_{i,i} = A_{j,j}$.
    \end{itemize}

    Let $A_{1,1} = \lambda$, then
    \[
        A = \begin{pmatrix}
            \lambda & \cdots & 0       \\
            \vdots  & \ddots & \vdots  \\
            0       & \cdots & \lambda
        \end{pmatrix} = \lambda I.
    \]

    So $T$ is a scalar multiple of the identity operator.
\end{proof}
\newpage

% chapter3:sectionD:exercise20
\begin{exercise}
    Suppose $q\in\mathcal{P}(\mathbb{R})$. Prove that there exists a polynomial $p\in\mathcal{P}(\mathbb{R})$ such that
    \[
        q(x) = (x^{2} + x)p''(x) + 2xp'(x) + p(3)
    \]

    for all $x\in\mathbb{R}$.
\end{exercise}

\begin{proof}
    Let $m$ be the degree of $q$. I define the linear map $T$ in $\lmap{\mathcal{P}_{m}(\mathbb{R})}$ as follow: $(Tp)(x) = (x^{2} + x)p''(x) + 2xp'(x) + p(3)$.

    $(x^{2} + x)p''(x) + 2xp'(x) + p(3) = 0$ for all $x\in\mathbb{R}$ if and only if $p = 0$. Therefore $T$ is injective. On the other hand, $\lmap{\mathcal{P}_{m}(\mathbb{R})}$ is finite-dimensional, so $T$ is invertible. So there exists a polynomial $p$ in $\mathcal{P}_{m}(\mathbb{R})$ such that $q(x) = (x^{2} + x)p''(x) + 2xp'(x) + p(3)$.
\end{proof}
\newpage

% chapter3:sectionD:exercise21
\begin{exercise}
    Suppose $n$ is a positive integer and $A_{j, k} \in \mathbb{F}$ for all $j, k = 1, \ldots, n$. Prove that the following are equivalent (note that in both parts below, the number of equations equals the number of variables).
    \begin{enumerate}[label={(\alph*)}]
        \item The trivial solution $x_{1} = \cdots = x_{n} = 0$ is the only solution to the homogeneous system of equations
              \begin{align*}
                  \sum^{n}_{k=1} A_{1,k}x_{k} & = 0    \\
                                              & \vdots \\
                  \sum^{n}_{k=1} A_{n,k}x_{k} & = 0    \\
              \end{align*}
        \item For every $c_{1}, \ldots, c_{n}\in\mathbb{F}$, there exists a solution to the system of equations
              \begin{align*}
                  \sum^{n}_{k=1} A_{1,k}x_{k} & = c_{1} \\
                                              & \vdots  \\
                  \sum^{n}_{k=1} A_{n,k}x_{k} & = c_{n} \\
              \end{align*}
    \end{enumerate}
\end{exercise}

\begin{proof}
    I define the linear map $T\in\lmap{\mathbb{F}^{n,1}}$ as follows: $Tx = Ax$, where $Ax$ is the matrix multiplication of $A$ and the column matrix $x$.

    (a) is equivalent to $T$ is injective.\@(b) is equivalent to $T$ is surjective. On the other hand, $T$ is injective if and only if $T$ is surjective. Therefore (a) and (b) are equivalent.
\end{proof}
\newpage

% chapter3:sectionD:exercise22
\begin{exercise}
    Suppose $T\in\mathcal{L}(V)$ and $v_{1},\ldots, v_{n}$ is a basis of $V$. Prove that
    \[
    \text{$\mathcal{M}(T, (v_{1}, \ldots, v_{n}))$ is invertible}\Longleftrightarrow \text{$T$ is invertible.}
    \]
\end{exercise}

\begin{proof}
    Let $A = \mathcal{M}(T, (v_{1}, \ldots, v_{n}))$.

    If $A$ is invertible, then there exists a matrix $B$ such that $AB = BA = I$. I define the linear map $S$ where
    \[
        Sv_{k} = \sum^{n}_{r=1}B_{r,k}v_{r}
    \]

    for $k = 1, \ldots, n$. Then
    \begin{align*}
        (ST)(v_{k}) & = S\left(\sum^{n}_{r=1}A_{r,k}v_{r} \right) = \sum^{n}_{r=1}A_{r,k}Sv_{r} \\
                    & = \sum^{n}_{r=1}A_{r,k}\left(\sum^{n}_{p=1}B_{p,r}v_{p}\right)            \\
                    & = \sum^{n}_{p=1}\left(\sum^{n}_{r=1}B_{p,r}A_{r,k}\right)v_{p}            \\
                    & = \sum^{n}_{p=1}I_{p,k}v_{p} = v_{k},                                     \\
        (TS)(v_{k}) & = T\left(\sum^{n}_{r=1}B_{r,k}v_{r}\right) = \sum^{n}_{r=1}B_{r,k}Tv_{r}  \\
                    & = \sum^{n}_{r=1}B_{r,k}\left(\sum^{n}_{p=1}A_{p,r}v_{p}\right)            \\
                    & = \sum^{n}_{p=1}\left(\sum^{n}_{r=1}A_{p,r}B_{r,k}\right)v_{p}            \\
                    & = \sum^{n}_{p=1}I_{p,k}v_{p} = v_{k}.
    \end{align*}

    So $ST = TS = \operatorname{id}_{V}$. Therefore $T$ is invertible.

    If $T$ is invertible, then there exists $S\in\lmap{V}$ such that $ST = TS = \operatorname{id}_{V}$.
    \[
        \begin{split}
            I = \mathcal{M}(\operatorname{id}_{V}, (v_{1}, \ldots, v_{n})) = \mathcal{M}(ST, (v_{1}, \ldots, v_{n})) = \mathcal{M}(S, (v_{1}, \ldots, v_{n}))\mathcal{M}(T, (v_{1}, \ldots, v_{n})), \\
            I = \mathcal{M}(\operatorname{id}_{V}, (v_{1}, \ldots, v_{n})) = \mathcal{M}(TS, (v_{1}, \ldots, v_{n})) = \mathcal{M}(T, (v_{1}, \ldots, v_{n}))\mathcal{M}(S, (v_{1}, \ldots, v_{n})).
        \end{split}
    \]

    So $\mathcal{M}(T, (v_{1}, \ldots, v_{n}))$ is invertible.
\end{proof}
\newpage

% chapter3:sectionD:exercise23
\begin{exercise}
    Suppose that $u_{1}, \ldots, u_{n}$ and $v_{1} \ldots, v_{n}$ are bases of $V$. Let $T\in\lmap{V}$ be such that $Tv_{k} = u_{k}$ for each $k = 1,\ldots, n$. Prove that
    \[
        \mathcal{M}(T, (v_{1}, \ldots, v_{n})) = \mathcal{M}(I, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n})).
    \]
\end{exercise}

\begin{proof}
    Because $Tv_{k} = u_{k}$ for each $k = 1,\ldots, n$, it follows that $\mathcal{M}(T, (v_{1}, \ldots, v_{n}), (u_{1}, \ldots, u_{n})) = I$.
    \begin{align*}
        \mathcal{M}(T, (v_{1}, \ldots, v_{n})) & = \mathcal{M}(TI, (v_{1}, \ldots, v_{n}))                                                                                      \\
                                               & = \mathcal{M}(T, (v_{1}, \ldots, v_{n}), (u_{1}, \ldots, u_{n}))\mathcal{M}(I, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n})) \\
                                               & = I\mathcal{M}(I, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n}))                                                              \\
                                               & = \mathcal{M}(I, (u_{1}, \ldots, u_{n}), (v_{1}, \ldots, v_{n})).\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter3:sectionD:exercise24
\begin{exercise}
    Suppose $A$ and $B$ are square matrices of the same size and $AB = I$. Prove that $BA = I$.
\end{exercise}

\begin{proof}
    Let $n$ be the number of rows of $A$. $T_{A}, T_{B}$ are linear maps in $\lmap{\mathbb{F}^{n,1}}$ where $T_{A}x = Ax$ and $T_{B}x = Bx$.

    $AB = I$ means $T_{A}T_{B} = \operatorname{id}_{\mathbb{F}^{n,1}}$. According to Exercise~\ref{chapter3:sectionD:exercise11}, $T_{A}$ and $T_{B}$ are invertible.
    \[
        T^{-1}_{A} = T^{-1}_{A}\operatorname{id}_{\mathbb{F}^{n,1}} = T^{-1}_{A}(T_{A}T_{B}) = (T^{-1}_{A}T_{A})T_{B} = T_{B}.
    \]

    Therefore $T_{B}T_{A} = T^{-1}_{A}T_{A} = \operatorname{id}_{\mathbb{F}^{n,1}}$, and $BA = I$.
\end{proof}
\newpage

\section{Products and Quotients of Vector Spaces}

% chapter3:sectionE:exercise1
\begin{exercise}
    Suppose $T$ is a function from $V$ to $W$. The graph of $T$ is the subset of $V\times W$ defined by
    \[
        \text{graph of $T$} = \{ (v, Tv)\in V\times W: v\in V \}.
    \]

    Prove that $T$ is a linear map if and only if the graph of $T$ is a subspace of $V\times W$.
\end{exercise}

\begin{proof}
    If $T$ is a linear map, then
    \begin{itemize}
        \item $(0,0)$ is in the graph of $T$.
        \item if $(v_{1}, Tv_{1})$ and $(v_{2}, Tv_{2})$ are in the graph of $T$ then $(v_{1} + v_{2}, Tv_{1} + Tv_{2}) = (v_{1} + v_{2}, T(v_{1} + v_{2}))$ is in the graph of $T$.
        \item if $(v, Tv)$ is in the graph of $T$, then $\lambda (v, Tv) = (\lambda v, \lambda Tv) = (\lambda v, T(\lambda v))$ is in the graph of $T$.
    \end{itemize}

    so the graph of $T$ is a subspace of $V\times W$.

    If the graph of $T$ is a subspace of $V\times W$, then for every $v_{1}, v_{2}\in V$ and $\lambda\in\mathbb{F}$,
    \begin{align*}
        (v_{1}, Tv_{1}) + (v_{2}, Tv_{2}) & = (v_{1}+v_{2}, Tv_{1} + Tv_{2}),  \\
        \lambda (v_{1}, Tv_{1})           & = (\lambda v_{1}, \lambda Tv_{1}).
    \end{align*}

    Due to the definition of a function and the graph of a function, $(v_{1}+v_{2}, Tv_{1} + Tv_{2}) = (v_{1} + v_{2}, T(v_{1} + v_{2}))$ and $(\lambda v_{1}, \lambda Tv_{1}) = (\lambda v_{1}, T(\lambda v_{1}))$.

    Therefore $Tv_{1} + Tv_{2} = T(v_{1} + V_{2})$ and $T(\lambda v_{1}) = \lambda Tv_{1}$. Hence $T$ is a linear map from $V$ to $W$.
\end{proof}
\newpage

% chapter3:sectionE:exercise2
\begin{exercise}
    Suppose that $V_{1} , \ldots, V_{m}$ are vector spaces such that $V_{1} \times \cdots \times V_{m}$ is finite-dimensional. Prove that $V_{k}$ is finite-dimensional for each $k = 1, \ldots, m$.
\end{exercise}

\begin{proof}
    Assume that at least one vector space $V_{k}$ is infinite-dimensional, then there exists a sequence $\{ v_{n} \}$ of vectors in $V_{k}$ such that any finite list from this sequence (starting from the first) is linearly independent. The sequence of vectors $\{ (\ldots, 0, v_{n}, 0, \ldots) \}$ satisfies the same property. Therefore $V_{1}\times \cdots \times V_{m}$ is infinite-dimensional.

    Hence $V_{k}$ is finite-dimensional for each $k = 1,\ldots, m$.
\end{proof}
\newpage

% chapter3:sectionE:exercise3
\begin{exercise}
    Suppose $V_{1} , \ldots, V_{m}$ are vector spaces. Prove that $\lmap{V_{1} \times \cdots\times V_{m} , W}$ and $\lmap{V_{1} , W} \times \cdots \times \lmap{V_{m} , W}$ are isomorphic vector spaces.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionE:exercise4
\begin{exercise}
    Suppose $W_{1} , \ldots, W_{m}$ are vector spaces. Prove that $\lmap{V, W_{1} \times \cdots \times W_{m}}$ and $\lmap{V, W_{1}} \times \cdots \times \lmap{V, W_{m}}$ are isomorphic vector spaces.
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionE:exercise5
\begin{exercise}
    For $m$ a positive integer, define $V^{m}$ by
    \[
        V^{m} = \underbrace{V\times \cdots \times V}_{\text{$m$ times}}.
    \]

    Prove that $V^{m}$ and $\lmap{\mathbb{F}^{m}, V}$ are isomorphic vector spaces.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{m}$ be a basis of $\mathbb{F}^{m}$.

    Let $(v_{1}, \ldots, v_{m})$ be a vector in $V^{m}$. I define a linear map $T$ from $V^{m}$ to $\lmap{\mathbb{F}^{m}, V}$ as follows: $T((v_{1}, \ldots, v_{m}))$ is the linear map that maps $e_{k}$ to $v_{k}$ for $k = 1,\ldots, m$.

    I define a linear map $S$ from $\lmap{\mathbb{F}^{m}, V}$ to $V^{m}$ as follows: $SF$ is $(Fe_{1}, \ldots, Fe_{m})$, where $F\in\lmap{\mathbb{F}^{n}, V}$.

    $ST = \operatorname{id}_{\lmap{\mathbb{F}^{m}, V}}$ and $TS = \operatorname{id}_{V^{m}}$, so $T$ and $S$ are isomorphism. Thus $V^{m}$ and $\lmap{\mathbb{F}^{m}, V}$ are isomorphic.
\end{proof}
\newpage

% chapter3:sectionE:exercise6
\begin{exercise}
    Suppose that $v$, $x$ are vectors in $V$ and that $U$, $W$ are subspaces of $V$ such that $v + U = x + W$. Prove that $U = W$.
\end{exercise}

\begin{proof}
    Because $v + U = x + W$, there exist vectors $u_{0}\in U$, $w_{0}\in W$ such that $v + 0 = x + w_{0}$ and $v + u_{0} = x + 0$. So $w_{0} = v - x$ and $u_{0} = x - v$. Therefore $v - x$ is in $U$ and $W$.

    Let $u$ be a vector in $U$. Because $v + U = x + W$, there exists a vector $w$ in $W$ such that $v + u = x + w$. $u = (x - v) + w\in W$, so $U\subseteq W$.

    Let $w$ be a vector in $W$. Because $v + U = x + W$, there exists a vector $u$ in $U$ such that $v + u = x + w$. $w = u + (v - x)\in U$, so $W\subseteq U$.

    Thus $U = W$.
\end{proof}
\newpage

% chapter3:sectionE:exercise7
\begin{exercise}
    Let $U = \{(x, y, z) \in \mathbb{R}^{3}: 2x + 3y + 5z = 0\}$. Suppose $A \subseteq \mathbb{R}^{3}$. Prove that $A$ is a translate of $U$ if and only if there exists $c\in\mathbb{R}$ such that
    \[
        A = \{ (x, y, z)\in\mathbb{R}^{3}: 2x + 3y + 5z = c \}.
    \]
\end{exercise}

\begin{proof}
    If there exists $c\in\mathbb{R}$ such that $A = \{ (x, y, z)\in\mathbb{R}^{3}: 2x + 3y + 5z = c \}$, let $(x_{1}, y_{1}, z_{1})$ and $(x_{2}, y_{2}, z_{2})$ be elements of $A$, then
    \[
        2(x_{2} - x_{1}) + 3(y_{2} - y_{1}) + 5(z_{2} - z_{1}) = c - c = 0.
    \]

    So $(x_{2} - x_{1}, y_{2} - y_{1}, z_{2} - z_{1})\in U$. Therefore $A$ is a translate of $U$.

    \bigskip
    If $A$ is a translate of $U$, then there exists $(x_{1}, y_{1}, z_{1})$ such that $A = (x_{1}, y_{1}, z_{1}) + U$. Let $(x_{2}, y_{2}, z_{2})$ be an element of $A$, then $(x_{2} - x_{1}, y_{2} - y_{1}, z_{2} - z_{1})\in U$, which means
    \[
        2x_{2} + 3y_{2} + 5z_{2} = 2x_{1} + 3y_{1} + 5z_{1}.
    \]

    Let $c = 2x_{1} + 3y_{1} + 5z_{1}$, we conclude that $A = \{ (x, y, z)\in\mathbb{R}^{3}: 2x + 3y + 5z = c \}$.
\end{proof}
\newpage

% chapter3:sectionE:exercise8
\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $T \in \lmap{V, W}$ and $c \in W$. Prove that $\{x \in V : Tx = c\}$ is either the empty set or is a translate of $\kernel{T}$.
        \item Explain why the set of solutions to a system of linear equations such as 3.27 is either the empty set or is a translate of some subspace of $\mathbb{F}^{n}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionE:exercise9
\begin{exercise}
    Prove that a nonempty subset $A$ of $V$ is a translate of some subspace of $V$ if and only if $\lambda v + (1 - \lambda)w\in A$ for all $v, w\in A$ and all $\lambda \in\mathbb{F}$.
\end{exercise}

In this exercise $\text{char}(\mathbb{F})\ne 2$. Here is a counterexample: $\mathbb{F} = \mathbb{F}_{2}$; $V = \mathbb{F}^{2}_{2}$ and $A = \{ (0,0), (0,1), (1,1) \}$.

\begin{proof}
    $(\Rightarrow)$ $A$ is a translate of a subspace $U$ of $V$.

    There exists a vector $v_{0}\in V$ such that $A = v_{0} + U$. If $v, w\in A$, then there exist vectors $u_{1}, u_{2}\in U$ such that $v = v_{0} + u_{1}$ and $w = v_{0} + u_{2}$. For every $\lambda\in\mathbb{F}$,
    \[
        \lambda v + (1 - \lambda)w = \lambda (v_{0} + u_{1}) + (1 - \lambda)(v_{0} + u_{2}) = v_{0} + \underbrace{(\lambda u_{1} + (1 - \lambda)u_{2})}_{\in U} \in A.
    \]

    $(\Leftarrow)$ $\lambda v + (1 - \lambda)w\in A$ for all $v, w\in A$ and all $\lambda \in\mathbb{F}$.

    Let $v_{0}$ be an element of $A$, and $U = \{ (-v_{0}) + a : a\in A \}$.

    Due to this definition and $v_{0}\in A$, it follows that $0\in U$.

    If $u\in U$, then $u + v_{0}\in A$. For all $\lambda\in\mathbb{F}$, $\lambda (u + v_{0}) + (1 - \lambda)v_{0}\in A$, we have $\lambda u + v_{0}\in A$, which means $\lambda u\in U$. So $U$ is closed under scalar multiplication.

    If $u_{1}, u_{2}\in U$ then $u_{1} + v_{0}, u_{2} + v_{0}\in A$.
    \[
        \frac{1}{2}(u_{1} + v_{0}) + \left(1 - \frac{1}{2}\right)(u_{2} + v_{0})\in A.
    \]

    Then $\frac{1}{2}(u_{1} + u_{2}) + v_{0}\in A$. Therefore $\frac{1}{2}(u_{1} + u_{2})\in U$. Since $U$ is closed under scalar multiplication, $u_{1} + u_{2}\in U$. So $U$ is closed under addition.

    Hence $U$ is a subspace of $V$, and $A = v_{0} + U$ is a translate of $U$.
\end{proof}
\newpage

% chapter3:sectionE:exercise10
\begin{exercise}
    Suppose $A_{1} = v + U_{1}$ and $A_{2} = w + U_{2}$ for some $v, w \in V$ and some subspaces $U_{1}, U_{2}$ of $V$. Prove that the intersection $A_{1} \cap A_{2}$ is either a translate of some subspace of $V$ or is the empty set.
\end{exercise}

\begin{proof}
    Assume that $A_{1}\cap A_{2}$ is a nonempty set. Let $v_{0}$ be an element of $A_{1}\cap A_{2}$.

    Let $a$ be an element of $A_{1}\cap A_{2}$, then $a - v_{0}\in U_{1}$ and $a - v_{0}\in U_{2}$. Therefore $a - v_{0}\in U_{1}\cap U_{2}$. So $a\in v_{0} + (U_{1}\cap U_{2})$, and $A_{1}\cap A_{2}\subseteq v_{0} + (U_{1}\cap U_{2})$.

    Let $b$ be an element of $v_{0} + (U_{1}\cap U_{2})$, then there exists $u\in U_{1}\cap U_{2}$ such that $b = v_{0} + u$. Because $u\in U_{1}$ and $u\in U_{2}$, it follows that $b\in v_{0} + U_{1} = v + U_{1}$ and $b\in v_{0} + U_{2} = w + U_{2}$. So $b\in A_{1}\cap A_{2}$.

    Hence $A_{1}\cap A_{2} = v_{0} + (U_{1}\cap U_{2})$. Thus $A_{1}\cap A_{2}$ is either the empty set or a translate of a subspace of $V$.
\end{proof}
\newpage

% chapter3:sectionE:exercise11
\begin{exercise}
    Suppose $U = \{(x_{1} , x_{2} , \ldots ) \in \mathbb{F}^{\infty} : x_{k} \ne 0 \text{ for only finitely many $k$} \}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $U$ is a subspace of $\mathbb{F}^{\infty}$.
        \item Prove that $\mathbb{F}^{\infty}/U$ is infinite-dimensional.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item I skip this exercise.
        \item Let $(p_{n})$ be the sequence of prime numbers.

              Let $u_{k}$ be the vector in $\mathbb{F}^{\infty}$ where the $n$th slot is $p^{n}_{k}$ and the other slots are $0$. $u_{1} + U, u_{2} + U, \ldots$ are pairwise distinct, since $u_{i} - u_{j}$ has infinite nonzero slots.

              Assume that $u_{1} + U$, \ldots, $u_{k} + U$ is linearly dependent, then there exist scalars $a_{1}, \ldots, a_{k}$ which are not all $0$ such that
              \[
                  (a_{1}u_{1} + U) + \cdots + (a_{k}u_{k} + U) = 0 + U.
              \]

              If $a_{i}\ne 0$ then $(a_{1}u_{1} + U) + \cdots + (a_{k}u_{k} + U)$ has infinite slots which are nonzero. Therefore the assumption is false, so for every positive integer $k$, $u_{1} + U$, $u_{2} + U$, \ldots, $u_{k} + U$ is linearly independent. Hence $\mathbb{F}^{\infty}/U$ is infinite-dimensional.
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionE:exercise12
\begin{exercise}
    Suppose $v_{1} , \ldots, v_{m} \in V$. Let
    \[
        A = \{ \lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}: \lambda_{1}, \ldots,\lambda_{m}\in\mathbb{F} \text{ and } \lambda_{1} + \cdots + \lambda_{m} = 1 \}.
    \]
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $A$ is a translate of some subspace of $V$.
        \item Prove that if $B$ is a translate of some subspace of $V$ and $\{ v_{1}, \ldots, v_{m} \} \subseteq B$, then $A\subseteq B$.
        \item Prove that $A$ is a translate of some subspace of $V$ of dimension less than $m$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $v_{0}$ be a vector in $A$, then there exist scalars $\lambda_{k}$ for $k = 1, \ldots, m$ such that $\lambda_{1} + \cdots + \lambda_{m} = 1$ and
              \[
                  v_{0} = \lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}.
              \]

              Let $U = \{ a - v_{0} : a\in A \}$. Because $v_{0}\in A$, it follows that $0\in U$.

              Let $u, w$ be elements of $U$, then there exist scalars $x_{k}$ and $y_{k}$ for $k = 1,\ldots, m$ such that $x_{1} + \cdots + x_{m} = 1$, $y_{1} + \cdots + y_{m} = 1$, and
              \[
                  u + v_{0} = x_{1}v_{1} + \cdots + x_{m}v_{m}\qquad w + v_{0} = y_{1}v_{1} + \cdots + y_{m}v_{m}.
              \]
              \begin{align*}
                  (u + w) + v_{0} & = (u + v_{0}) + (w + v_{0}) + (-1)v_{0}                                                                                        \\
                                  & = (x_{1}v_{1} + \cdots + x_{m}v_{m}) + (y_{1}v_{1} + \cdots + y_{m}v_{m}) + (-1)(\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}) \\
                                  & = (x_{1} + y_{1} - \lambda_{1})v_{1} + \cdots + (x_{m} + y_{m} - \lambda_{m})v_{m}.
              \end{align*}

              Since $\sum^{m}_{k=1}(x_{k} + y_{k} - \lambda_{k}) = \sum^{m}_{k=1}x_{k} + \sum^{m}_{k=1}y_{k} - \sum^{m}_{k=1}\lambda_{k} = 1 + 1 - 1 = 1$, then $(u + w) + v_{0}$ is in $A$. So $u + w$ is in $U$.

              For every scalar $\lambda\in\mathbb{F}$
              \begin{align*}
                  \lambda u + v_{0} & = (\lambda x_{1}v_{1} + \cdots + \lambda x_{m}v_{m} - \lambda v_{0}) + v_{0}                                  \\
                                    & = (\lambda x_{1}v_{1} + \cdots + \lambda x_{m}v_{k}) + (1 - \lambda)v_{0}                                     \\
                                    & = (\lambda x_{1} + (1 - \lambda)\lambda_{1})v_{1} + \cdots + (\lambda x_{m} + (1 - \lambda)\lambda_{m})v_{m}.
              \end{align*}

              Since $\sum^{m}_{k=1}(\lambda x_{k} + (1 - \lambda)\lambda_{k}) = \sum^{m}_{k=1}\lambda x_{k} + \sum^{m}_{k=1}(1 - \lambda)\lambda_{k} = \lambda + (1 - \lambda) = 1$, then $\lambda u + v_{0}$ is in $A$. So $\lambda u$ is in $U$.

              Hence $U$ is a vector space, and $A = v_{0} + U$, which is a translate of $U$.
        \item Let $B = v + W$, where $W$ is a subspace of $V$.

              Since $v_{k}\in B$ for $k = 1, \ldots, m$, it follows that $v_{k} - v\in W$.

              Let $\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}$ be an element of $A$ (where $\lambda_{1} + \cdots + \lambda_{m} = 1$).
              \begin{align*}
                  (\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}) - v & = (\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}) - (\lambda_{1}v + \cdots + \lambda_{m}v) \\
                                                                     & = \lambda_{1}(v_{1} - v) + \cdots + \lambda_{m}(v_{m} - v).
              \end{align*}

              Because $\lambda_{1}(v_{1} - v) + \cdots + \lambda_{m}(v_{m} - v)$ is a linear combination of $v_{1} - v$, \ldots, $v_{m} - v$, then $(\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}) - v\in W$. Hence $\lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}\in B$, which implies $A\subseteq B$.
        \item (Continue with (a)) Let $u$ be a vector in $U$, then $u + v_{0}$ is in $A$. Then there exist scalars $a_{k}$ for $k = 1, \ldots, m$ such that $a_{1} + \cdots + a_{m} = 1$ and
              \[
                  u + v_{0} = a_{1}v_{1} + \cdots + a_{m}v_{m}
              \]

              So
              \[
                  u = (a_{1} - \lambda_{1})v_{1} + \cdots + (a_{m} - \lambda_{m})v_{m}
              \]

              where $\sum^{m}_{k=1}(a_{k} - \lambda_{k}) = 1 - 1 = 0$.

              Let $x_{1}, \ldots, x_{m}$ be scalars such that $x_{1} + \cdots + x_{m} = 0$, then
              \[
                  (x_{1}v_{1} + \cdots + x_{m}v_{m}) + v_{0} = (x_{1} + \lambda_{1})v_{1} + \cdots + (x_{m} + \lambda_{m})v_{m}
              \]

              where $\sum^{m}_{k=1}(x_{k} + \lambda_{k}) = 0 + 1 = 1$. Then $x_{1}v_{1} + \cdots + x_{m}v_{m} = 0$.

              Hence
              \[
                  U = \{ x_{1}v_{1} + \cdots + x_{m}v_{m} : x_{1}, \ldots, x_{m}\in \mathbb{F} \text{ and } x_{1} + \cdots + x_{m} = 0 \}.
              \]

              I define a subspace $U_{1}$ of $V$ as the span of $v_{1}, \ldots, v_{m-1}$, and define a map $T$ from $U$ to $U_{1}$ as follows:
              \[
                  T: x_{1}v_{1} + \cdots + x_{m}v_{m} \mapsto x_{1}v_{1} + \cdots + x_{m-1}v_{m-1}.
              \]

              I define a map $S$ from $U_{1}$ to $U$ as follows:
              \[
                  S: x_{1}v_{1} + \cdots + x_{m-1}v_{m-1} \mapsto x_{1}v_{1} + \cdots + x_{m-1}v_{m-1} + (1 - x_{1} - \cdots - x_{m-1})v_{m}.
              \]

              $T$ and $S$ are linear maps and $ST = \operatorname{id}_{U}$, $TS = \operatorname{id}_{U_{1}}$. Therefore $T$ is an isomorphism and $U$ is isomorphic to $U_{1}$. The dimension of $U_{1}$ is less than or equal to $(m - 1)$, so the dimension of $U$ is less than $m$.

              Hence $A$ is the translate of a subspace of $V$ of dimension less than $m$.
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionE:exercise13
\begin{exercise}
    Suppose $U$ is a subspace of $V$ such that $V/U$ is finite-dimensional. Prove that $V$ is isomorphic to $U \times (V/U)$.
\end{exercise}

\begin{proof}
    Let $T$ be the linear map from $V$ to $V/U$, which is defined by $Tv = v + U$. $Tv = 0 + U$ if and only if $v + U = 0 + U$. $v + U = 0 + U$ if and only if $v\in U$. So $\kernel{T} = U$.

    Let $v_{1} + U, \ldots, v_{n} + U$ be a basis of $V/U$, and $v$ a vector in $V$. There exist scalars $a_{k}$ for $k = 1,\ldots, n$ such that
    \[
        Tv = v + U = (a_{1}v_{1} + U) + \cdots + (a_{n}v_{n} + U).
    \]

    Hence for all $v\in V$, we have $v - (a_{1}v_{1} + \cdots + a_{n}v_{n})\in U$, so $V = U + \operatorname{span}(v_{1}, \ldots, v_{n})$.

    If $v$ is a vector in $U \cap \operatorname{span}(v_{1}, \ldots, v_{n})$, then $Tv = 0 + U = x_{1}Tv_{1} + \cdots + x_{n}Tv_{n}$ for some scalars $x_{1}, \ldots, x_{n}$. We deduce that $x_{1} = \cdots = x_{n} = 0$ because $Tv_{1}, \ldots, Tv_{n}$ is a basis of $V/U$. This implies $v = 0$, so $U \cap \operatorname{span}(v_{1}, \ldots, v_{n}) = \{0\}$.

    Hence $V = U \oplus \operatorname{span}(v_{1}, \ldots, v_{n})$. I define the map $S$ from $V$ to $U\times (V/U)$ as follows: $Sv = (u, v + U)$, where $v = u + (v - u)$, $u\in U$ and $v - u\in \operatorname{span}(v_{1}, \ldots, v_{n})$ (the only way to write $v$ as the sum of a vector in $U$ and a vector in $\operatorname{span}(v_{1}, \ldots, v_{n})$).

    Next, I prove that $S$ is a linear map.

    Let $w_{1}, w_{2}$ be vectors in $V$. There exist a unique $u_{1}\in U$ such that $w_{1} - u_{1}\in \operatorname{span}(v_{1}, \ldots, v_{n})$, an a unique $u_{2}\in U$ such that $w_{2} - u_{2}\in \operatorname{span}(v_{1}, \ldots, v_{n})$.
    \begin{align*}
        S(w_{1} + w_{2}) & = (u_{1} + u_{2}, T(w_{1} + w_{2})) \\
                         & = (u_{1} + u_{2}, Tw_{1} + Tw_{2})  \\
                         & = (u_{1}, Tw_{1}) + (u_{2}, Tw_{2}) \\
                         & = Sw_{1} + Sw_{2},                  \\
        S(\lambda w_{1}) & = (\lambda u_{1}, T(\lambda w_{1})) \\
                         & = (\lambda u_{1}, \lambda Tw_{1})   \\
                         & = \lambda (u_{1}, Tw_{1})           \\
                         & = \lambda Sw_{1}.
    \end{align*}

    So $S$ is indeed a linear map. I define a map $R$ from $U\times (V/U)$ to $V$ as follows:
    \[
        R((u, v + U)) = u + a_{1}v_{1} + \cdots + a_{n}v_{n},
    \]

    where $v + U = (a_{1}v_{1} + U) + \cdots + (a_{n}v_{n} + U)$ ($a_{1}, \ldots, a_{n}$ are unique because $v_{1} + U, \ldots, v_{n} + U$ is a basis of $V/U$).
    \begin{align*}
        R((u_{1}, w_{1} + U) + (u_{2}, w_{2} + U)) & = R((u_{1} + u_{2}, (w_{1} + w_{2}) + U))                                                 \\
                                                   & = (u_{1} + u_{2}) + ((b_{1} + c_{1})v_{1} + \cdots + (b_{n} + c_{n})v_{n})                \\
                                                   & \phantom{=}\text{where ($w_{1} + U = (b_{1}v_{1} + U) + \cdots + (b_{n}v_{n} + U))$}      \\
                                                   & \phantom{=}\text{and ($w_{2} + U = (c_{1}v_{1} + U) + \cdots + (c_{n}v_{n} + U)$)}        \\
                                                   & = (u_{1} + b_{1}v_{1} + \cdots + b_{n}v_{n}) + (u_{2} + c_{1}v_{1} + \cdots + c_{n}v_{n}) \\
                                                   & = R((u_{1}, w_{1} + U)) + R((u_{2}, w_{2} + U)),                                          \\
        R(\lambda (u_{1}, w_{1} + U))              & = R((\lambda u_{1}, \lambda w_{1} + U))                                                   \\
                                                   & = \lambda u_{1} + \lambda (c_{1}v_{1} + \cdots + c_{n}v_{n})                              \\
                                                   & = \lambda (u_{1} + c_{1}v_{1} + \cdots + c_{n}v_{n})                                      \\
                                                   & = \lambda R((u_{1}, w_{1} + U)).
    \end{align*}

    So $R$ is also a linear map. On the other hand,
    \begin{align*}
        (RS)(v)          & = R((u, v + U))                                                                  \\
                         & = R((u, (u + a_{1}v_{1} + \cdots + a_{n}v_{n}) + U))                             \\
                         & = R((u, (a_{1}v_{1} + \cdots + a_{n}v_{n}) + U))                                 \\
                         & = u + a_{1}v_{1} + \cdots + a_{n}v_{n}                                           \\
                         & = v,                                                                             \\
        (SR)((u, w + U)) & = S(u + d_{1}v_{1} + \cdots + d_{n}v_{n})                                        \\
                         & \phantom{=}\text{(where $w + U = (d_{1}v_{1} + U) + \cdots + (d_{n}v_{n} + U)$)} \\
                         & = (u, (d_{1}v_{1} + \cdots + d_{n}v_{n}) + U)                                    \\
                         & = (u, w + U).
    \end{align*}

    So $S$ and $R$ are isomorphisms. Thus $V$ is isomorphic to $U\times (V/U)$.
\end{proof}
\newpage

% chapter3:sectionE:exercise14
\begin{exercise}
    Suppose $U$ and $W$ are subspaces of $V$ and $V = U \oplus W$. Suppose $w_{1} , \ldots, w_{m}$ is a basis of $W$. Prove that $w_{1} + U, \ldots, w_{m} + U$ is a basis of $V/U$.
\end{exercise}

\begin{proof}
    Let $v$ be a vector in $V$. There exists a unique pair of vectors $u, w$ where $u\in U, w\in W$ such that $v = u + w$. Since $w_{1}, \ldots, w_{m}$ is a basis of $W$, there exist scalars $a_{k}$ for $k = 1,\ldots, m$ such that $w = a_{1}w_{1} + \cdots + a_{m}w_{m}$. Then
    \[
        v + U = (u + w) + U = w + U = a_{1}(w_{1} + U) + \cdots + a_{m}(w_{m} + U)
    \]

    which means $w_{1} + U, \ldots, w_{m} + U$ spans $V/U$.

    Suppose that $a_{1}(w_{1} + U) + \cdots + a_{m}(w_{m} + U) = 0 + U$, then $a_{1}w_{1} + \cdots + a_{m}w_{m}\in U$. But $U\cap W = \{0\}$ because $V = U\oplus W$, it follows that $a_{1} = \cdots = a_{m} = 0$. Therefore $w_{1} + U, \ldots, w_{m} + U$ is linearly independent.

    Hence $w_{1} + U, \ldots, w_{m} + U$ is a basis of $V/U$.
\end{proof}
\newpage

% chapter3:sectionE:exercise15
\begin{exercise}
    Suppose $U$ is a subspace of $V$ and $v_{1} + U, \ldots, v_{m} + U$ is a basis of $V/U$ and $u_{1} , \ldots, u_{n}$ is a basis of $U$. Prove that $v_{1} , \ldots, v_{m} , u_{1} , \ldots, u_{n}$ is a basis of $V$.
\end{exercise}

\begin{proof}
    If $a_{1}v_{1} + \cdots + a_{m}v_{m} \in U$, then
    \[
        a_{1}(v_{1} + U) + \cdots + a_{m}(v_{m} + U) = (a_{1}v_{1} + \cdots + a_{m}v_{m}) + U = 0 + U.
    \]

    This implies $a_{1} = \cdots = a_{m} = 0$, because $v_{1} + U, \ldots, v_{m} + U$ is a basis of $V/U$. So $\operatorname{span}(v_{1}, \ldots, v_{m})\cap U = \{0\}$. Particularly, $a_{1}v_{1} + \cdots + a_{m}v_{m} = 0\in U$ if and only if $a_{1} = \cdots = a_{m}$, which means $v_{1}, \ldots, v_{m}$ is linearly independent.

    Let $v$ be a vector in $V$. There exist scalars $b_{1}, \ldots, b_{m}$ such that
    \[
        v + U = b_{1}(v_{1} + U) + \cdots + b_{m}(v_{m} + U) = (b_{1}v_{1} + \cdots + b_{m}v_{m}) + U
    \]

    because $v_{1} + U, \ldots, v_{m} + U$ is a basis of $V/U$. So $v - (b_{1}v_{1} + \cdots + b_{m}v_{m})\in U$. Therefore $V = U + \operatorname{span}(v_{1}, \ldots, v_{m})$. Together with $U\cap \operatorname{span}(v_{1}, \ldots, v_{m}) = \{0\}$, it follows that $V = U \oplus \operatorname{span}(v_{1}, \ldots, v_{m})$.

    $v_{1}, \ldots, v_{m}$ is a basis of $\operatorname{span}(v_{1}, \ldots, v_{m})$, $u_{1}, \ldots, u_{n}$ is a basis of $U$, so $v_{1} , \ldots, v_{m} , u_{1} , \ldots, u_{n}$ is a basis of $V$.
\end{proof}
\newpage

% chapter3:sectionE:exercise16
\begin{exercise}
    Suppose $\varphi \in \lmap{V,\mathbb{F}}$ and $\varphi\ne 0$. Prove that $\dim V/(\kernel{\varphi}) = 1$.
\end{exercise}

\begin{proof}
    Because $V/(\kernel{\varphi})$ is isomorphic to $\range{\varphi}$ and $\dim\range{\varphi} = 1$, it follows that
    \[
        \dim V/(\kernel{\varphi}) = 1.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionE:exercise17
\begin{exercise}
    Suppose $U$ is a subspace of $V$ such that $\dim V/U = 1$. Prove that there exists $\varphi \in \lmap{V, \mathbb{F}}$ such that $\kernel{\varphi} = U$.
\end{exercise}

\begin{proof}
    Because $\dim V/U = 1$, there exists a vector $w\in V$ such that $w + U$ is a basis of $V/U$.

    Let $T$ be the map from $V$ to $V/U$ where $Tv = v + U$. Let $\varphi$ be the map from $V$ to $\mathbb{F}$, defined by $\varphi v = \lambda$, where $Tv = v + U = \lambda w + U$. $\varphi$ is well-defined, because $w + U$ is a basis of $V/U$. $T$ and $\varphi$ are both linear maps.

    $\varphi v = 0$ if and only if $v + U = 0 + U$, equivalently, $v\in U$. So $\kernel{\varphi} = U$.

    I constructed a linear map $\varphi\in\lmap{V, \mathbb{F}}$ where $\kernel{\varphi} = U$.
\end{proof}
\newpage

% chapter3:sectionE:exercise18
\begin{exercise}
    Suppose that $U$ is a subspace of $V$ such that $V/U$ is finite-dimensional.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that if $W$ is a finite-dimensional subspace of $V$ and $V = U + W$, then $\dim W \geq \dim V/U$.
        \item Prove that there exists a finite-dimensional subspace $W$ of $V$ such that $\dim W = \dim V/U$ and $V = U\oplus W$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Let's define $T\in\lmap{V, V/U}$ by $Tv = v + U$, then $\kernel{T} = U$.

    \begin{enumerate}[label={(\alph*)}]
        \item I consider $T\vert_{W}: W\to V/U$. Let $v + U$ be a vector in $V/U$, then $Tv = v + U$. Since $V = U + W$, there exist vector $u$ in $U$ and $w$ in $W$ such that $v = u + w$.
              \[
                  Tw = T(v - u) = (v - u) + U = v + U.
              \]

              Therefore $T\vert_{W}$ is surjective. Hence $\dim W\geq \dim V/U$, due to the fundamental theorem of linear maps.
        \item Let $w_{1} + U, \ldots, w_{n} + U$ be a basis of $V/U$.
              \[
                  a_{1}(w_{1} + U) + \cdots + a_{n}(w_{n} + U) = 0 + U
              \]

              if and only if $a_{1} = \cdots = a_{n} = 0$ because $w_{1} + U, \ldots, w_{n} + U$ be a basis of $V/U$. On the other hand,
              \[
                  (a_{1}w_{1} + \cdots + a_{n}w_{n}) + U = a_{1}(w_{1} + U) + \cdots + a_{n}(w_{n} + U)
              \]

              so $a_{1}w_{1} + \cdots + a_{n}w_{n}\in U$ if and only if $a_{1} = \cdots = a_{n} = 0$. Therefore $\operatorname{span}(w_{1}, \ldots, w_{n})\cap U = \{ 0 \}$  and $a_{1}w_{1} + \cdots + a_{n}w_{n} = 0\in U$ if and only if $a_{1} = \cdots = a_{n} = 0$, which means $w_{1}, \ldots, w_{n}$ is linearly independent.

              Let $W = \operatorname{span}(w_{1}, \ldots, w_{n})$. Because $w_{1}, \ldots, w_{n}$ is linearly independent, it follows that $w_{1}, \ldots, w_{n}$ is a basis of $W$ and then $\dim W = n = \dim V/U$.

              Let $v$ be a vector in $V$. Because $w_{1} + U, \ldots, w_{n} + U$ be a basis of $V/U$, there exist scalars $x_{1}, \ldots, x_{n}$ such that
              \[
                  Tv = a_{1}(w_{1} + U) + \cdots + a_{n}(w_{n} + U).
              \]

              Equivalently,
              \[
                  v + U = (a_{1}w_{1} + \cdots + a_{n}w_{n}) + U.
              \]

              Therefore
              \[
                  v - (a_{1}w_{1} + \cdots + a_{n}w_{n})\in U
              \]

              which means $V = U + W$. Together with $U\cap W = \{0\}$, we conclude that $V = U\oplus W$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionE:exercise19
\begin{exercise}
    Suppose $T\in\lmap{V, W}$ and $U$ is a subspace of $V$. Let $\pi$ denote the quotient map from $V$ onto $V/U$. Prove that there exists $S\in\lmap{V/U, W}$ such that $T = S\circ \pi$ if and only if $U\subseteq \kernel{T}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists $S\in\lmap{V/U, W}$ such that $T = S\circ \pi$.

    Let $u$ be a vector in $U$, then $Tu = S(\pi(u)) = S(0 + U) = 0$. So $u\in \kernel{T}$ for every $u\in U$, which means $U\subseteq \kernel{T}$.

    $(\Leftarrow)$ $U\subseteq \kernel{T}$.

    $v_{1} + U = v_{2} + U$ if and only if $v_{1} - v_{2}\in U$. $v_{1} - v_{2}\in U$ implies $v_{1} - v_{2}\in \kernel{T}$. $v_{1} - v_{2}\in \kernel{T}$ if and only if $v_{1} + \kernel{T} = v_{2} + \kernel{T}$.

    So the map $f: V/U\to V/(\kernel{T})$ where $f(v_{1} + U) = v_{1} + \kernel{T}$ is well-defined. $f$ is also a linear map.

    Let $\widetilde{T}$ be a map from $V/(\kernel{T})$ to $W$ where $\widetilde{T}(v + \kernel{T}) = Tv$. This map is well-defined and linear. Let $S = \widetilde{T}\circ f$, then for every $v\in V$
    \[
        S(v + U) = \widetilde{T}(f(v + U)) = \widetilde{T}(v + \kernel{T}) = Tv.
    \]

    Hence I constructed a linear map $S\in\lmap{V/U, W}$ such that $T = S\circ \pi$.
\end{proof}
\newpage

\section{Duality}

% chapter3:sectionF:exercise1
\begin{exercise}\label{chapter3:sectionF:exercise1}
    Explain why each linear functional is surjective or is the zero map.
\end{exercise}

\begin{proof}
    Let $V$ be a vector space over the field $\mathbb{F}$ and $\varphi$ a linear functional on $V$.

    If $\varphi$ is not the zero map, then there exists a vector $v$ in $V$ such that $\varphi(v) = \lambda_{0}\ne 0$. Then for every $\lambda\in\mathbb{F}$, we have
    \[
        \varphi(\lambda\lambda_{0}^{-1}v) = \lambda\lambda_{0}^{-1}\lambda_{0} = \lambda.
    \]

    Therefore $\varphi$ is surjective. Hence a linear functional is either surjective or is the linear map.
\end{proof}
\newpage

% chapter3:sectionF:exercise2
\begin{exercise}
    Give three distinct examples of linear functionals on $\mathbb{R}^{[0, 1]}$.
\end{exercise}

\begin{proof}
    $T_{1}(f) = 0$ for every $f\in \mathbb{R}^{[0, 1]}$.

    $T_{2}(f) = f(0)$ for every $f\in \mathbb{R}^{[0, 1]}$.

    $T_{3}(f) = -f(0)$ for every $f\in \mathbb{R}^{[0, 1]}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise3
\begin{exercise}
    Suppose $V$ is finite-dimensional and $v \in V$ with $v \ne 0$. Prove that there exists $\varphi\in V'$ such that $\varphi(v) = 1$.
\end{exercise}

\begin{proof}
    If $v\ne 0$ then $v$ is linearly independent. Extend this to create a basis of $V$ and let it be $v, v_{1}, \ldots, v_{n}$.

    Let $\varphi, \varphi_{1}, \ldots, \varphi_{n}$ be the dual basis of $V$, then $\varphi(v) = 1$ due to the definition of dual basis.
\end{proof}
\newpage

% chapter3:sectionF:exercise4
\begin{exercise}
    Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$ such that $U \ne V$. Prove that there exists $\varphi\in V'$ such that $\varphi(u) = 0$ for every $u\in U$ but $\varphi \ne 0$.
\end{exercise}

\begin{proof}
    Let $u_{1}, \ldots u_{m}$ be a basis of $U$. We can extend this list to create a basis of $V$, and let that basis be $u_{1}, \ldots u_{m}, v_{1}, \ldots, v_{n}$. Because $U\ne V$, it follows that $n > 0$.

    Let $\varphi_{1}, \ldots, \varphi_{m}, \varphi_{m+1}, \ldots, \varphi_{m+n}$ be the dual basis of $u_{1}, \ldots u_{m}, v_{1}, \ldots, v_{n}$. Let $\varphi = \varphi_{m+1}$ then $\varphi\ne 0$ and for every $u = a_{1}u_{1} + \cdots + a_{m}u_{m}\in U$,
    \[
        \varphi(u) = \varphi(a_{1}u_{1} + \cdots + a_{m}u_{m}) = \psi_{1}(a_{1}u_{1} + \cdots + a_{m}u_{m}) = \sum^{m}_{k=1}a_{k}\varphi_{m+1}(u_{k}) = 0.
    \]

    So there exists $\varphi\in V'$ such that $\varphi(u) = 0$ for every $u\in U$ but $\varphi\ne 0$.
\end{proof}
\newpage

% chapter3:sectionF:exercise5
\begin{exercise}
    Suppose $T \in \lmap{V, W}$ and $w_{1} , \ldots, w_{m}$ is a basis of $\range{T}$. Hence for each $v\in V$, there exist unique numbers $\varphi_{1} (v), \ldots, \varphi_{m} (v)$ such that
    \[
        Tv = \varphi_{1}(v)w_{1} + \cdots + \varphi_{m}(v)w_{m},
    \]

    thus defining functions $\varphi_{1}, \ldots, \varphi_{m}$ from $V$ to $\mathbb{F}$. Show that each of the functions $\varphi_{1}, \ldots, \varphi_{m}$ is a linear functional on $V$.
\end{exercise}

\begin{proof}
    Let $v, u$ be vectors in $V$ and $\lambda\in\mathbb{F}$
    \begin{align*}
        T(v + u)     & = \varphi_{1}(v + u)w_{1} + \cdots + \varphi_{m}(v + u)w_{m},         \\
        T(\lambda v) & = \varphi_{1}(\lambda v)w_{1} + \cdots + \varphi_{m}(\lambda v)w_{m}.
    \end{align*}

    On the other hand
    \begin{align*}
        T(v + u)     & = Tv + Tu                                                                                                     \\
                     & = (\varphi_{1}(v)w_{1} + \cdots + \varphi_{m}(v)w_{m}) + (\varphi_{1}(u)w_{1} + \cdots + \varphi_{m}(u)w_{m}) \\
                     & = (\varphi_{1}(v) + \varphi_{1}(u))w_{1} + \cdots + (\varphi_{m}(v) + \varphi_{m}(u))w_{m},                   \\
        T(\lambda v) & = \lambda Tv                                                                                                  \\
                     & = \lambda (\varphi_{1}(v)w_{1} + \cdots + \varphi_{m}(v)w_{m})                                                \\
                     & = \lambda\varphi_{1}(v)w_{1} + \cdots + \lambda\varphi_{m}(v)w_{m}
    \end{align*}

    Because $w_{1} , \ldots, w_{m}$ is a basis of $\range{T}$, we can identify the coefficients and conclude that $\varphi_{k}(v + u) = \varphi_{k}(v) + \varphi_{k}(u)$ and $\varphi_{k}(\lambda v) = \lambda \varphi_{k}(v)$. Hence $\varphi_{1}, \ldots, \varphi_{m}$ are linear functionals on $V$.
\end{proof}
\newpage

% chapter3:sectionF:exercise6
\begin{exercise}\label{chapter3:sectionF:exercise6}
    Suppose $\varphi, \beta \in V'$. Prove that $\kernel{\varphi} \subseteq \kernel{\beta}$ if and only if there exists $c \in \mathbb{F}$ such that $\beta = c\varphi$.
\end{exercise}

\begin{proof}[1st Proof]
    According to Exercise~\ref{chapter3:sectionB:exercise25}, there exists a linear map $f$ in $\lmap{\mathbb{F}}$ such that $\beta = f\varphi$ if and only if $\kernel{\varphi}\subseteq\kernel{\beta}$. On the other hand, let $c = f(1)$, then $f(\lambda) = \lambda c$. So $\beta = c\varphi$.
\end{proof}

\begin{proof}[2nd Proof]
    $(\Rightarrow)$ There exists $c \in \mathbb{F}$ such that $\beta = c\varphi$.

    If $v\in \kernel{\varphi}$, then $\beta(v) = c\varphi(v) = 0$, which implies $v\in \kernel{\beta}$. Therefore $\kernel{\varphi} \subseteq \kernel{\beta}$.

    \bigskip

    $(\Leftarrow)$ $\kernel{\varphi} \subseteq \kernel{\beta}$. Due to Exercise~\ref{chapter3:sectionF:exercise1}, we consider the following two possibilities

    $\varphi = 0$. Then $\kernel\varphi = V$, $\kernel{\beta} = V$ and $\varphi(v) = \beta(v) = 0$ for all $v\in V$. So there exists $c\in\mathbb{F}$ such that $\beta = c\varphi$.

    $\varphi$ is surjective. So there exists $v_{0}\in V$ such that $\varphi(v_{0}) = 1$. Let $v$ be a vector in $V$, and let $\varphi(v) = \lambda$.
    \[
        \varphi(v) = \lambda = \lambda\cdot 1 = \lambda \varphi(v_{0}) = \varphi(\lambda v_{0}).
    \]

    So $v - \lambda v_{0}\in \kernel{\varphi}\subseteq \kernel{\beta}$.
    \[
        \beta(v - \lambda v_{0}) = 0 \Longrightarrow \beta(v) = \lambda\beta(v_{0}) = \beta(v_{0})\varphi(v).
    \]

    Let $c = \beta(v_{0})$, we have $\beta(v) = c\varphi(v)$ for every $v\in V$. Hence there exists $c\in\mathbb{F}$ such that $\beta = c\varphi$.
\end{proof}
\newpage

% chapter3:sectionF:exercise7
\begin{exercise}
    Suppose that $V_{1} , \ldots, V_{m}$ are vector spaces. Prove that ${(V_{1} \times \cdots \times V_{m})}'$ and $V_{1}' \times \cdots \times V_{m}'$ are isomorphic vector spaces.
\end{exercise}

\begin{proof}
    Let $\varphi$ be a linear functional on $V_{1} \times \cdots \times V_{m}$. I define maps from $V_{k}$ to $\mathbb{F}$ as follows: $\varphi_{k}(v) = \varphi(\ldots, v, \ldots)$ ($v$ is at the $k$th slot, and other slots are $0$). $\varphi_{k}$ is indeed a linear functional on $V_{k}$. The map $T: \varphi\mapsto (\varphi_{1}, \ldots, \varphi_{m})$ is well-defined and is a linear map.

    Let $(\varphi_{1}, \ldots, \varphi_{m})$ be an element of $V_{1}' \times \cdots \times V_{m}'$. I define a map $S$ from $V_{1}' \times \cdots \times V_{m}'$ to ${(V_{1} \times \cdots \times V_{m})}'$ as follows: $S(\varphi_{1}, \ldots, \varphi_{m}) = \varphi$ where $\varphi(v_{1}, \ldots, v_{m}) = (\varphi_{1}(v_{1}), \ldots, \varphi_{m}(v_{m}))$. The map $S$ is well-defined and is a linear map.

    On the other hand $ST = \operatorname{id}_{{(V_{1} \times \cdots \times V_{m})}'}$ and $TS = \operatorname{id}_{V_{1}' \times \cdots \times V_{m}'}$. Hence $T$, $S$ are isomorphisms. Thus ${(V_{1} \times \cdots \times V_{m})}'$ and $V_{1}' \times \cdots \times V_{m}'$ are isomorphic vector spaces.
\end{proof}
\newpage

% chapter3:sectionF:exercise8
\begin{exercise}
    Suppose $v_{1} , \ldots, v_{n}$ is a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{n}$ is the dual basis of $V'$. Define $\Gamma: V\to \mathbb{F}^{n}$ and $\Lambda: \mathbb{F}^{n}\to V$ by
    \[
        \Gamma(v) = (\varphi_{1}(v), \ldots, \varphi_{n}(v)) \text{  and  } \Lambda(a_{1}, \ldots, a_{n}) = a_{1}v_{1} + \cdots + a_{n}v_{n}.
    \]

    Explain why $\Gamma$ and $\Lambda$ are inverses of each other.
\end{exercise}

\begin{proof}
    \begin{align*}
        (\Gamma\Lambda)(a_{1}, \ldots, a_{n}) & = \Gamma(\Lambda(a_{1}, \ldots, a_{n}))                                                                   \\
                                              & = \Gamma(a_{1}v_{1} + \cdots + a_{n}v_{n})                                                                \\
                                              & = (\varphi_{1}(a_{1}v_{1} + \cdots + a_{n}v_{n}), \ldots, \varphi_{k}(a_{1}v_{1} + \cdots + a_{n}v_{n})).
    \end{align*}

    Because $\varphi_{1}, \ldots, \varphi_{n}$ is the dual basis of $v_{1} , \ldots, v_{n}$, $\varphi_{k}(a_{1}v_{1} + \cdots + a_{n}v_{n}) = a_{k}$. Therefore
    \[
        (\Gamma\Lambda)(a_{1}, \ldots, a_{n}) = (a_{1}, \ldots, a_{n}).
    \]
    \begin{align*}
        (\Lambda\Gamma)(v) & = \Lambda(\Gamma(v))                                  \\
                           & = \Lambda(\varphi_{1}(v), \ldots, \varphi_{n}(v))     \\
                           & = \varphi_{1}(v)v_{1} + \cdots + \varphi_{n}(v)v_{n}.
    \end{align*}

    Besides $\varphi_{k}(v) = \varphi_{k}(a_{1}v_{1} + \cdots + a_{n}v_{n}) = a_{k}$, so
    \[
        (\Lambda\Gamma)(v) = a_{1}v_{1} + \cdots + a_{n}v_{n} = v.
    \]

    Hence $\Gamma$ and $\Lambda$ are inverses of each other.
\end{proof}
\newpage

% chapter3:sectionF:exercise9
\begin{exercise}
    Suppose $m$ is a positive integer. Show that the dual basis of the basis $1, x, \ldots, x^{m}$ of $\mathcal{P}_{m}(\mathbb{R})$ is $\varphi_{0}, \varphi_{1}, \ldots, \varphi_{m}$, where
    \[
        \varphi_{k}(p) = \frac{p^{(k)}(0)}{k!}.
    \]
\end{exercise}

\begin{proof}
    According to the definition of dual basis, $\varphi_{k}(x^{k}) = 1$ and $\varphi_{k}(x^{n}) = 0$ for $n\ne k$. If $p(x) = a_{0} + a_{1}x + \cdots + a_{m}x^{m}$, then
    \[
        \varphi_{k}(p) = \varphi_{k}(a_{k}x^{k}) = a_{k} = \frac{p^{(k)}(0)}{k!}.
    \]

    The last equality is followed from Taylor's expansion at $0$ of $p$.
\end{proof}
\newpage

% chapter3:sectionF:exercise10
\begin{exercise}
    Suppose $m$ is a positive integer.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $1, x - 5, \ldots, {(x-5)}^{m}$ is a basis of $\mathcal{P}_{m}(\mathbb{R})$.
        \item What is the dual basis of the basis in (a)?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item I skip this exercise.
        \item Let $\varphi_{0}, \varphi_{1}, \ldots, \varphi_{m}$ be the dual basis of $1, x - 5, \ldots, {(x - 5)}^{m}$.

              Let $p(x) = a_{0} + a_{1}x + \cdots + a_{m}x^{m}$, then
              \[
                  \varphi_{k}(p) = \varphi_{k}(a_{k}{(x - 5)}^{k}) = a_{k} = \frac{p^{(k)}(5)}{k!}.
              \]

              The last equality is followed from Taylor's expansion at $5$ of $p$.
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise11
\begin{exercise}
    Suppose $v_{1} , \ldots, v_{n}$ is a basis of $V$ and $\varphi_{1} , \ldots, \varphi_{n}$ is the corresponding dual basis of $V'$. Suppose $\psi \in V'$. Prove that
    \[
        \psi = \psi(v_{1})\varphi_{1} + \cdots + \psi(v_{n})\varphi_{n}.
    \]
\end{exercise}

\begin{proof}
    Because $V$ is finite-dimensional, then $\varphi_{1} , \ldots, \varphi_{n}$ is also a basis of $V'$. So there exist $a_{k}$ for $k = 1, \ldots, n$ such that
    \[
        \psi = a_{1}\varphi_{1} + \cdots + a_{n}\varphi_{n}.
    \]

    $\psi(v_{k}) = (a_{1}\varphi_{1} + \cdots + a_{n}\varphi_{n})(v_{k}) = a_{k}\varphi_{k}(v_{k}) = a_{k}$. Therefore
    \[
        \psi = \psi(v_{1})\varphi_{1} + \cdots + \psi(v_{n})\varphi_{n}.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionF:exercise12
\begin{exercise}
    Suppose $S, T\in \lmap{V, W}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that ${(S + T)}' = S' + T'$.
        \item Prove that ${(\lambda T)}' = \lambda T'$ for all $\lambda\in \mathbb{F}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I skip this exercise.
\end{proof}
\newpage

% chapter3:sectionF:exercise13
\begin{exercise}
    Show that the dual map of the identity operator on $V$ is the identity operator
    on $V'$.
\end{exercise}

\begin{proof}
    Let $I$ be the identity operator on $V$ and $I'$ be the dual map of $V$. For every linear functional $\varphi$ on $V$, we have
    \[
        I'(\varphi) = \varphi\circ I.
    \]

    So for every $v\in V$, $(I'(\varphi))(v) = (\varphi\circ I)(v) = \varphi(v)$. Therefore $I'(\varphi) = \varphi$ for every $\varphi\in V'$. Hence $I'$ is the identity operator on $V'$.
\end{proof}
\newpage

% chapter3:sectionF:exercise14
\begin{exercise}
    Define $T: \mathbb{R}^{3} \to \mathbb{R}^{2}$ by
    \[
        T(x, y, z) = (4x + 5y + 6z, 7x + 8y + 9z).
    \]

    Suppose $\varphi_{1}, \varphi_{2}$ denotes the dual basis of the standard basis of $\mathbb{R}^{2}$ and $\psi_{1}, \psi_{2}, \psi_{3}$ denotes the dual basis of the standard basis of $\mathbb{R}^{3}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Describe the linear functionals $T'(\varphi_{1})$ and $T'(\varphi_{2})$.
        \item Write $T'(\varphi_{1})$ and $T'(\varphi_{2})$ as linear combinations of $\psi_{1}, \psi_{2}, \psi_{3}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{align*}
        (T'(\varphi_{1}))(x, y, z) & = (\varphi_{1}\circ T)(x, y, z) = \varphi_{1}(T(x, y, z)) \\
                                   & = \varphi_{1}(4x + 5y + 6z, 7x + 8y + 9z)                 \\
                                   & = \varphi_{1}(4x + 5y + 6z, 0)                            \\
                                   & = 4x + 5y + 6z,                                           \\
        (T'(\varphi_{2}))(x, y, z) & = (\varphi_{2}\circ T)(x, y, z) = \varphi_{2}(T(x, y, z)) \\
                                   & = \varphi_{2}(4x + 5y + 6z, 7x + 8y + 9z)                 \\
                                   & = \varphi_{2}(0, 7x + 8y + 9z)                            \\
                                   & = 7x + 8y + 9z.
    \end{align*}

    $T'(\varphi_{1})$ maps a vector $v\in \mathbb{R}^{3}$ to the 1st slot of $Tv$ and $T'(\varphi_{2})$ maps a vector $v\in \mathbb{R}^{3}$ to the 2nd slot of $Tv$.
    \begin{align*}
        (T'(\varphi_{1}))(x, y, z) & = 4x + 5y + 6z                                                  \\
                                   & = 4\psi_{1}(x, y, z) + 5\psi_{2}(x, y, z) + 6\psi_{2}(x, y, z), \\
        (T'(\varphi_{2}))(x, y, z) & = 7x + 8y + 9z                                                  \\
                                   & = 7\psi_{1}(x, y, z) + 8\psi_{2}(x, y, z) + 9\psi_{2}(x, y, z).
    \end{align*}

    So $T'(\varphi_{1}) = 4\psi_{1} + 5\psi_{2} + 6\psi_{3}$ and $T'(\varphi_{2}) = 7\psi_{1} + 8\psi_{2} + 9\psi_{3}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise15
\begin{exercise}
    Define $T: \mathcal{P}(\mathbb{R})\to \mathcal{P}(\mathbb{R})$ by
    \[
        (Tp)(x) = x^{2}p(x) + p''(x)
    \]

    for each $x\in \mathbb{R}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $\varphi\in\mathcal{P}(\mathbb{R})'$ is defined by $\varphi(p) = p'(4)$. Describe the linear functional $T'(\varphi)$ on $\mathcal{P}(\mathbb{R})$.
        \item Suppose $\varphi\in\mathcal{P}(\mathbb{R})'$ is defined by $\varphi(p) = \int^{1}_{0}p$. Evaluate $(T'(\varphi))(x^{3})$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item
              \begin{align*}
                  (T'(\varphi))(p) & = (\varphi\circ T)(p) = \varphi(T(p))                 \\
                                   & = \varphi(x^{2}p(x) + p''(x))                         \\
                                   & = (2x p(x) + x^{2}p'(x) + p{'}{'}{'}(x))\vert_{x = 4} \\
                                   & = 8p(4) + 16p'(4) + p{'}{'}{'}(4).
              \end{align*}
        \item
              \begin{align*}
                  (T'(\varphi))(x^{3}) & = \varphi(x^{2}x^{3} + 6x)                \\
                                       & = \int^{1}_{0}(x^{5} + 6x)dx              \\
                                       & = \frac{1}{6} + 3 = \frac{19}{6}.\qedhere
              \end{align*}
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise16
\begin{exercise}\label{chapter3:sectionF:exercise16}
    Suppose $W$ is finite-dimensional and $T\in\lmap{V, W}$. Prove that
    \[
        T' = 0 \Longleftrightarrow T = 0.
    \]
\end{exercise}

\begin{proof}
    $W$ is finite-dimensional then $W'$, $\kernel{T'}$, $\range{T}$, ${(\range{T})}^{0}$ are also finite-dimensional.

    In any case, we have $\kernel{T'} = {(\range{T})}^{0}$.
    \begin{align*}
        T' = 0 & \Longleftrightarrow \kernel{T'} = W'                 \\
               & \Longleftrightarrow {(\range{T})}^{0} = W'           \\
               & \Longleftrightarrow \dim{(\range{T})}^{0} = \dim W'  \\
               & \Longleftrightarrow \dim W - \dim \range{T} = \dim W \\
               & \Longleftrightarrow \dim\range{T} = 0                \\
               & \Longleftrightarrow T = 0.\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter3:sectionF:exercise17
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional and $T \in \lmap{V, W}$. Prove that $T$ is invertible if and only if $T' \in \lmap{W', V'}$ is invertible.
\end{exercise}

\begin{proof}
    My reasoning is valid because $V$ and $W$ are finite-dimensional.

    $T$ is injective if and only if $T'$ is surjective. $T$ is surjective if and only if $T'$ is injective.

    $T$ is invertible if and only if $T$ is injective and surjective. $T'$ is invertible if and only if $T'$ is invertible.

    Hence $T$ is invertible if and only if $T'$ is invertible.
\end{proof}
\newpage

% chapter3:sectionF:exercise18
\begin{exercise}
    Suppose $V$ and $W$ are finite-dimensional. Prove that the map that takes $T\in \lmap{V, W}$ to $T'\in \lmap{W', V'}$ is an isomorphism of $\lmap{V, W}$ onto $\lmap{W', V'}$.
\end{exercise}

\begin{proof}
    Because ${(S + T)}' = S' + T'$ and ${(\lambda T)}' = \lambda T'$ for every $S, T\in \lmap{V, W}$ and $\lambda\in\mathbb{F}$, then the map $D$ that takes a linear map in $\lmap{V, W}$ to its dual map in $\lmap{W', V'}$ is another linear map.

    According to Exercise~\ref{chapter3:sectionF:exercise16}, $T' = 0$ if and only if $T = 0$, so $D$ is injective. On the other hand
    \[
        \dim\lmap{V, W} = (\dim V)(\dim W) = (\dim V')(\dim W') = \dim\lmap{W', V'}.
    \]

    So $D$ is also an isomorphism of $\lmap{V, W}$ onto $\lmap{W', V'}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise19
\begin{exercise}\label{chapter3:sectionF:exercise19}
    Suppose $U \subseteq V$. Explain why
    \[
        U^{0} = \{ \varphi\in V': U\subseteq \kernel{\varphi} \}.
    \]
\end{exercise}

\begin{proof}
    If $\psi\in U^{0}$, then $\psi(u) = 0$ for every $u\in U$. On the other hand, $\kernel{\psi}$ is the set of vector $v\in V$ such that $\psi(v) = 0$. Therefore $U\subseteq \kernel{\psi}$, and $U^{0}\subseteq \{ \varphi\in V': U\subseteq \kernel{\varphi} \}$.

    If $\psi\in \{ \varphi\in V': U\subseteq \kernel{\varphi} \}$, then for every $u\in U$, $\psi(u) = 0$ and then $\psi\in U^{0}$. So $\{ \varphi\in V': U\subseteq \kernel{\varphi} \}\subseteq U^{0}$.

    Thus $U^{0} = \{ \varphi\in V': U\subseteq \kernel{\varphi} \}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise20
\begin{exercise}\label{chapter3:sectionF:exercise20}
    Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$. Show that
    \[
        U = \{  v\in V: \varphi(v) = 0 \text{ for every $\varphi \in U^{0}$ } \}.
    \]
\end{exercise}

\begin{proof}
    If $u\in U$, then $\varphi(u) = 0$ for every $\varphi\in U^{0}$ --- This is true, according to the definition of the annihilator of $U$. So
    \[
        U \subseteq \{  v\in V: \varphi(v) = 0 \text{ for every $\varphi \in U^{0}$ } \}.
    \]

    Next, I prove the inclusion in the opposite direction.

    Because $V$ is finite-dimensional, then so is $U$. Let $u_{1}, \ldots, u_{m}$ be a basis of $U$. We extend this list to create a basis of $V$, and let it be
    \[
        u_{1}, \ldots, u_{m}, u_{m+1}, \ldots, u_{m+n}.
    \]

    Let $\varphi_{1}, \ldots, \varphi_{m+n}$ be the dual basis of the above basis of $V$, and $\varphi$ an element of $U^{0}\subseteq V'$. There exist scalars $a_{k}$ for $k = 1, \ldots, m+n$ such that
    \[
        \varphi = a_{1}\varphi_{1} + \cdots + a_{m+n}\varphi_{m+n}.
    \]

    Since $\varphi(u) = 0$ for every $u\in U$, then $\varphi(u_{1}) = \cdots = \varphi(u_{m}) = 0$. Therefore $a_{k}\varphi_{k}(u_{k}) = 0$ for $k = 1,\ldots, m$, so $a_{k} = 0$ for $k = 1,\ldots, m$. It follows that
    \[
        \varphi = a_{m+1}\varphi_{m+1} + \cdots + a_{m+n}\varphi_{m+n}.
    \]

    So $U^{0} = \operatorname{span}(\varphi_{m+1}, \ldots, \varphi_{m+n})$. On the other hand, $\varphi_{m+1}, \ldots, \varphi_{m+n}$ is linearly independent, so $\varphi_{m+1}, \ldots, \varphi_{m+n}$ is also a basis of $U^{0}$.

    If $v = a_{1}u_{1} + \cdots + a_{m+n}u_{m+n}\in \{  v\in V: \varphi(v) = 0 \text{ for every $\varphi \in U^{0}$ } \}$, then $\varphi_{k}(v) = 0$ for $k = m+1, \ldots, m+n$. It follows that $a_{m+1} = \cdots = a_{m+n} = 0$, and $v = a_{1}u_{1} + \cdots + a_{m}u_{m}$. Therefore
    \[
        \{  v\in V: \varphi(v) = 0 \text{ for every $\varphi \in U^{0}$ } \}\subseteq U.
    \]

    Thus $U = \{  v\in V: \varphi(v) = 0 \text{ for every $\varphi \in U^{0}$ } \}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise21
\begin{exercise}
    Suppose $V$ is finite-dimensional and $U$ and $W$ are subspaces of $V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $W^{0}\subseteq U^{0}$ if and only if $U\subseteq W$.
        \item Prove that $W^{0} = U^{0}$ if and only if $U = W$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $(\Rightarrow)$ $W^{0}\subseteq U^{0}$.

              Let $v$ be a vector in $U$ and $\varphi\in W^{0}$. $\varphi$ is also in $U^{0}$ because $W^{0}\subseteq U^{0}$. Due to the definition of annihilator, $\varphi(v) = 0$, and it follows that $v$ is also in $W$. Therefore $U\subseteq W$.

              $(\Leftarrow)$ $U\subseteq W$.

              Let $\varphi$ be a vector in $W^{0}$, and $u\in U$. $u$ is also in $W$ because $U\subseteq W$. So $\varphi(u) = 0$, and it follows that $\varphi\in U^{0}$.

              Hence $W^{0}\subseteq U^{0}$ if and only if $U\subseteq W$.
        \item $W^{0} = U^{0}$ if and only if $W^{0}\subseteq U^{0}$ and $U^{0}\subseteq W^{0}$.

              $U = W$ if and only if $U\subseteq W$ and $W\subseteq U$.

              In combination with (a), we conclude that $W^{0} = U^{0}$ if and only if $U = W$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise22
\begin{exercise}\label{chapter3:sectionF:exercise22}
    Suppose $V$ is finite-dimensional and $U$ and $W$ are subspaces of $V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that ${(U + W)}^{0} = U^{0}\cap W^{0}$.
        \item Show that ${(U\cap W)}^{0} = U^{0} + W^{0}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item If $\varphi\in {(U + W)}^{0}$, then $\varphi(u + w) = 0$ for every $u\in U$ and $w\in W$. So $\varphi(u + 0) = 0$ and $\varphi(0 + w) = 0$  for every $u\in U$ and $w\in W$. Therefore $\varphi\in U^{0}$ and $\varphi\in W^{0}$. So $\varphi\in U^{0}\cap W^{0}$. Hence ${(U + W)}^{0}\subseteq U^{0}\cap W^{0}$.

              If $\varphi\in U^{0}\cap W^{0}$, then $\varphi\in U^{0}$ and $\varphi\in W^{0}$. Let $u\in U$ and $w\in W$, then $\varphi(u) = 0$ and $\varphi(w) = 0$. So $\varphi(u + w) = \varphi(u) + \varphi(w) = 0$. Therefore $U^{0}\cap W^{0}\subseteq {(U + W)}^{0}$.

              Hence ${(U + W)}^{0} = U^{0}\cap W^{0}$.
        \item If $\varphi\in U^{0} + W^{0}$, then there exist $\psi_{1}\in U^{0}$ and $\psi_{2}\in W^{0}$ such that $\varphi = \psi_{1} + \psi_{2}$. For every $v\in U\cap W$, $\varphi(v) = \psi_{1}(v) + \psi_{2}(v) = 0 + 0 = 0$. So $\varphi\in {(U + W)}^{0}$. Hence $U^{0} + W^{0}$ is a subspace of ${(U\cap W)}^{0}$.

              If $\varphi\in {(U\cap W)}^{0}$, I use the condition that $V$ is finite-dimensional.
              \begin{align*}
                  \dim (U^{0} + W^{0}) & = \dim U^{0} + \dim W^{0} - \dim (U^{0} + W^{0})                  \\
                                       & = (\dim V - \dim U) + (\dim V - \dim W) - \dim {(U + W)}^{0}      \\
                                       & \phantom{=} \text{(since ${(U  +W)}^{0} = U^{0}\cap W^{0}$)}      \\
                                       & = (\dim V - \dim U) + (\dim V - \dim W) - (\dim V - \dim (U + W)) \\
                                       & = \dim V - (\dim U + \dim W - \dim (U + W))                       \\
                                       & = \dim V - \dim (U \cap W)                                        \\
                                       & = \dim {(U\cap W)}^{0}.
              \end{align*}

              Thus ${(U\cap W)}^{0} = U^{0} + W^{0}$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise23
\begin{exercise}\label{chapter3:sectionF:exercise23}
    Suppose $V$ is finite-dimensional and $\varphi_{1}, \ldots, \varphi_{m}\in V'$. Prove that the following three sets are equal to each other.
    \begin{enumerate}[label={(\alph*)}]
        \item $\operatorname{span}(\varphi_{1}, \ldots, \varphi_{m})$
        \item ${((\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{m}}))}^{0}$
        \item $\{ \varphi\in V': (\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{m}})\subseteq \kernel{\varphi} \}$
    \end{enumerate}
\end{exercise}

\begin{proof}
    I will use mathematical induction by $m$.

    For $m = 1$. Let $\varphi\in V'$. According to Exercise~\ref{chapter3:sectionF:exercise6}, $\varphi\in\operatorname{span}(\varphi_{1})$ if and only if $\varphi_{1}(v) = 0$ implies $\varphi(v) = 0$. Moreover, $\varphi_{1}(v) = 0$ implies $\varphi(v) = 0$ if and only if $\varphi \in {(\kernel{\varphi_{1}})}^{0}$ (due to the definition of annihilator). Therefore
    \[
        \operatorname{span}(\varphi_{1}) = {(\kernel{\varphi_{1}})}^{0}.
    \]

    Suppose that for $m = k\geq 1$, (a) and (b) are equal. According to Exercise~\ref{chapter3:sectionF:exercise22}
    \[
        {((\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{k+1}}))}^{0} = {((\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{k}}))}^{0} + {(\kernel{\varphi_{k+1}})}^{0}.
    \]

    According to the induction hypothesis and the case $m = 1$,
    \begin{align*}
        {((\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{k}}))}^{0} + {(\kernel{\varphi_{k+1}})}^{0} & = \operatorname{span}(\varphi_{1}, \ldots, \varphi_{k}) + \operatorname{span}(\varphi_{k+1}) \\
                                                                                                               & = \operatorname{span}(\varphi_{1}, \ldots, \varphi_{k+1}).
    \end{align*}

    By the principle of mathematical induction, we conclude that (a) and (b) are equal.

    Due to Exercise~\ref{chapter3:sectionF:exercise19}, (b) and (c) are equal.

    Thus, (a), (b), (c) are equal to each other.
\end{proof}
\newpage

% chapter3:sectionF:exercise24
\begin{exercise}
    Suppose $V$ is finite-dimensional and $v_{1} , \ldots, v_{m} \in V$. Define a linear map $\Gamma: V'\to \mathbb{F}^{m}$ by $\Gamma(\varphi) = (\varphi(v_{1}), \ldots, \varphi(v_{m}))$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $v_{1}, \ldots, v_{m}$ spans $V$ if and only if $\Gamma$ is injective.
        \item Prove that $v_{1}, \ldots, v_{m}$ is linearly independent if and only if $\Gamma$ is surjective.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Let $u_{1}, \ldots, u_{n}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{n}$ be the dual basis.

    \begin{enumerate}[label={(\alph*)}]
        \item $(\Rightarrow)$ $v_{1}, \ldots, v_{m}$ spans $V$.

              Let $\varphi\in\kernel{\Gamma}$, then $\varphi(v_{1}) = \cdots = \varphi(v_{m}) = 0$. $\varphi(v_{1}) = \cdots = \varphi(v_{m}) = 0$ if and only if for all $a_{k}\in\mathbb{F}$ ($k = 1,\ldots,m$), $\varphi(a_{1}v_{1} + \cdots + a_{m}v_{m}) = 0$.

              Because $v_{1}, \ldots, v_{m}$ spans $V$, the above statement means $\varphi(v) = 0$ for all $v\in V$, or equivalently, $\varphi = 0$. Therefore $\kernel{\Gamma} = \{0\}$, and we conclude that $\Gamma$ is injective.

              $(\Leftarrow)$ $\Gamma$ is injective.

              Assume that $\operatorname{span}(v_{1}, \ldots, v_{m}) = U\ne V$. Let $u_{1}, \ldots, u_{n}$ be a basis of $U$ and $u_{1}, \ldots, u_{n}$, $u_{n+1}, \ldots, u_{n+p}$ be a basis of $V$ (because $U\ne V$, then $p > 0$). There exists a linear functional $\psi$ on $V$ such that
              \[
                  \psi(u_{k}) = \begin{cases}
                      0 & \text{if $k = 1, \ldots, n$},    \\
                      1 & \text{if $k = n+1, \ldots, n+p$}
                  \end{cases}
              \]

              By this definition, $\psi(u) = 0$ for all $u\in U$, and $\psi(v_{1}) = \cdots = \psi(v_{m}) = 0$.

              So $\Gamma(\psi) = 0$ by definition of $\psi$ and $\Gamma$. On the other hand, $\psi\ne 0$ because $\psi(u_{n+1})\ne 0$. This contradicts the hypothesis that $\Gamma$ is injective, so the assumption is false.

              Hence $v_{1}, \ldots, v_{m}$ spans $V$.
        \item $(\Rightarrow)$ $v_{1}, \ldots, v_{m}$ is linearly independent.

              Extend the list $v_{1}, \ldots, v_{m}$ to create a basis of $V$ and let it be $v_{1}, \ldots, v_{m}, v_{m+1},\ldots, v_{m+n}$.

              Let $\varphi_{1}, \ldots, \varphi_{m}, \varphi_{m+1}, \ldots, \varphi_{m+n}$ be the dual basis, and $(a_{1}, \ldots, a_{m})$ be an element in $\mathbb{F}^{m}$. I choose $\varphi = a_{1}\varphi_{1} + \cdots + a_{m}\varphi_{m}$.

              It follows from the definition of the dual basis that $\varphi(v_{1}) = a_{1}$, \ldots, $\varphi(v_{m}) = a_{m}$. Therefore
              \[
                  \Gamma(\varphi) = (\varphi(v_{1}), \ldots, \varphi(v_{m})) = (a_{1}, \ldots, a_{m})
              \]

              and we conclude that $\mathbb{F}^{m} = \range{\Gamma}$, or equivalently, $\Gamma$ is surjective.

              $(\Leftarrow)$ $\Gamma$ is surjective.

              Let $a_{1}v_{1} + \cdots + a_{m}v_{m} = 0$ be a linear combination of $0$ in $V$. Because $\Gamma$ is surjective, then there exist $\varphi_{k}\in V'$ for $k = 1,\ldots, m$ such that $\varphi_{k}(v_{i}) = 1$ if $i = k$ and $\varphi_{k}(v_{i}) = 0$ if $i\ne k$. On the other hand, for all $k = 1,\ldots, m$,
              \[
                  a_{k} = \varphi_{k}(a_{1}v_{1} + \cdots + a_{m}v_{m}) = \varphi_{k}(0) = 0.
              \]

              Hence $v_{1}, \ldots, v_{m}$ is linearly independent.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise25
\begin{exercise}
    Suppose $V$ is finite-dimensional and $\varphi_{1} , \ldots, \varphi_{m} \in V'$. Define a linear map $\Gamma: V\to \mathbb{F}^{m}$ by $\Gamma(v) = (\varphi_{1}(v), \ldots, \varphi_{m}(v))$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $\varphi_{1}, \ldots, \varphi_{m}$ spans $V'$ if and only if $\Gamma$ is injective.
        \item Prove that $\varphi_{1}, \ldots, \varphi_{m}$ is linearly independent if and only if $\Gamma$ is surjective.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $(\Rightarrow)$ $\varphi_{1}, \ldots, \varphi_{m}$ spans $V'$.

              Let $v$ be a vector in $\kernel{\Gamma}$, then $\varphi_{1}(v) = \cdots = \varphi_{m}(v) = 0$. Let $\varphi$ be an element of $V'$. Because $\varphi_{1}, \ldots, \varphi_{m}$ spans $V'$, there exist $a_{k}\in \mathbb{F}$ for $k = 1,\ldots,m$ such that
              \[
                  \varphi = a_{1}\varphi_{1} + \cdots + a_{m}\varphi_{m}.
              \]

              It follows that $\varphi(v) = a_{1}\varphi_{1}(v) + \cdots + a_{m}\varphi_{m}(v) = 0$. So $\varphi(v) = 0$ for all $\varphi\in V'$.

              Let $v_{1}, \ldots, v_{n}$ be a basis of $V$, then there exist $\lambda_{k}\in\mathbb{F}$ for $k = 1,\ldots, m$ such that
              \[
                  v = \lambda_{1}v_{1} + \cdots + \lambda_{m}v_{m}.
              \]

              Let $\psi_{1}, \ldots, \psi_{n}$ be the dual basis of $v_{1}, \ldots, v_{n}$, then $\lambda_{k} = \psi_{k}(v) = 0$ for $k = 1,\ldots,n$. Therefore $v = 0$, which means $\kernel{\Gamma} = \{0\}$ and $\Gamma$ is injective.

              $(\Leftarrow)$ $\Gamma$ is injective.

              Let $\psi_{1}, \ldots, \psi_{n}$ be a basis of $\operatorname{span}(\varphi_{1}, \ldots, \varphi_{m})$. Let $\psi_{1}, \ldots, \psi_{n}, \psi_{n+1}, \ldots, \psi_{n+p}$ be a basis of $V'$. Assume that $\operatorname{span}(\varphi_{1}, \ldots, \varphi_{m})\ne V'$, then $p > 0$.

              By Exercise~\ref{chapter3:sectionF:exercise30}, there exists a basis $u_{1}, \ldots, u_{n}, u_{n+1}, \ldots, u_{n+p}$, of which dual basis is $\psi_{1}, \ldots, \psi_{n}, \psi_{n+1}, \ldots, \psi_{n+p}$. It follows from the definition of $\Gamma$ and the dual basis that $\Gamma(u_{n+1}) = (\varphi_{1}(u_{n+1}), \ldots, \varphi_{m}(u_{n+1})) = 0$. This contradicts the hypothesis that $\Gamma$ is injective. So $\operatorname{span}(\varphi_{1}, \ldots, \varphi_{m})\ne V'$.

              Hence $\varphi_{1}, \ldots, \varphi_{m}$ spans $V'$.
        \item $(\Rightarrow)$ $\varphi_{1}, \ldots, \varphi_{m}$ is linearly independent.

              Extend this list to create a basis of $V'$ and let it be $\varphi_{1}, \ldots, \varphi_{m}, \varphi_{m+1}, \ldots, \varphi_{m+n}$.

              By Exercise~\ref{chapter3:sectionF:exercise30}, there exists a basis $u_{1}, \ldots, u_{m}, u_{m+1}, \ldots, u_{m+n}$, of which dual basis is $\varphi_{1}, \ldots, \varphi_{m}, \varphi_{m+1}, \ldots, \varphi_{m+n}$.

              Let $(a_{1}, \ldots, a_{m})$ be an element in $\mathbb{F}^{m}$, then
              \[
                  \Gamma(a_{1}u_{1} + \cdots + a_{m}u_{m}) = (\varphi_{1}(a_{1}u_{1}), \ldots, \varphi_{m}(a_{m}u_{m})) = (a_{1}, \ldots, a_{m}).
              \]

              So $\range{\Gamma} = \mathbb{F}^{m}$, and it follows that $\Gamma$ is surjective.

              $(\Leftarrow)$ $\Gamma$ is surjective.

              Assume $a_{1}\varphi_{1} + \cdots + a_{m}\varphi_{m} = 0$ is a linear combination of $0$ in $V'$. Because $\Gamma$ is surjective, for each $k = 1,\ldots, m$, there exists a vector $v_{k}\in V$ such that $\varphi_{k}(v_{k}) = 1$ and $\varphi_{j}(v_{k}) = 0$ where $j\ne k$. Then
              \[
                  0 = (a_{1}\varphi_{1} + \cdots + a_{m}\varphi_{m})(v_{k}) = a_{k}.
              \]

              So $a_{1} = \cdots = a_{m} = 0$, which implies $\varphi_{1}, \ldots, \varphi_{m}$ is linearly independent.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise26
\begin{exercise}\label{chapter3:sectionF:exercise26}
    Suppose $V$ is finite-dimensional and $\Omega$ is a subspace of $V'$. Prove that
    \[
        \Omega = {\{ v\in V: \varphi(v) = 0 \text{ for every $\varphi\in \Omega$ } \}}^{0}.
    \]
\end{exercise}

\begin{proof}
    Because $V$ is finite-dimensional, then $V'$ and $\Omega$ are also finite-dimensional.

    Let $U = \{ v\in V: \varphi(v) = 0 \text{ for every $\varphi\in \Omega$ } \}$. This definition is equivalent to
    \[
        U = \bigcap_{\varphi\in \Omega} \kernel{\varphi}.
    \]

    Let $\varphi_{1}, \ldots, \varphi_{n}$ be a basis of $\Omega$, then
    \[
        \bigcap_{\varphi\in \Omega} \kernel{\varphi} \subseteq \kernel{\varphi_{1}}\cap \cdots\cap \kernel{\varphi_{n}}.
    \]

    Assume $v$ is a vector in $\kernel{\varphi_{1}}\cap \cdots\cap \kernel{\varphi_{n}}$, then $\varphi_{k}(v) = 0$ for $k = 1, \ldots, n$. Let $\varphi$ be an element of $\Omega$. Because $\varphi_{1}, \ldots, \varphi_{n}$ is a basis of $\Omega$, there exist scalars $a_{k}\in \mathbb{F}$ for $k = 1,\ldots, n$ such that
    \[
        \varphi = a_{1}\varphi_{1} + \cdots + a_{n}\varphi_{n}.
    \]

    Then $\varphi(v) = 0$ as well, which means $v\in \kernel{\varphi}\subseteq \bigcap_{\varphi\in \Omega} \kernel{\varphi}$. So
    \[
        \kernel{\varphi_{1}}\cap \cdots\cap \kernel{\varphi_{n}} \subseteq \bigcap_{\varphi\in \Omega} \kernel{\varphi}.
    \]

    Hence
    \[
        \bigcap_{\varphi\in \Omega} \kernel{\varphi} = \kernel{\varphi_{1}}\cap \cdots\cap \kernel{\varphi_{n}}.
    \]

    It follows that $U = \kernel{\varphi_{1}}\cap\cdots\cap\kernel{\varphi_{n}}$. According to Exercise~\ref{chapter3:sectionF:exercise23},
    \[
        \operatorname{span}(\varphi_{1}, \ldots, \varphi_{n}) = {\left(\kernel{\varphi_{1}}\cap\cdots\cap\kernel{\varphi_{n}}\right)}^{0}.
    \]

    Since $\Omega = \operatorname{span}(\varphi_{1}, \ldots, \varphi_{n})$ and $U = \left(\kernel{\varphi_{1}}\cap\cdots\cap\kernel{\varphi_{n}}\right)$, we conclude that $\Omega = U^{0}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise27
\begin{exercise}\label{chapter3:sectionF:exercise27}
    Suppose $T\in \lmap{\mathcal{P}_{5}(\mathbb{R})}$ and $\kernel{T'} = \operatorname{span}(\varphi)$, where $\varphi$ is the linear functional on $\mathcal{P}_{5}(\mathbb{R})$ defined by $\varphi(p) = p(8)$. Prove that
    \[
        \range{T} = \{ p\in\mathcal{P}_{5}(\mathbb{R}): p(8) = 0 \}    .
    \]
\end{exercise}

\begin{proof}
    Remind that $\kernel{T'} = {(\range{T})}^{0}$. Together with $\kernel{T'} = \operatorname{span}(\varphi)$, it follows that ${(\range{T})}^{0} = \operatorname{span}(\varphi)$. Hence
    \[
        \range{T} = \kernel{\varphi} = \{ p\in\mathcal{P}_{5}(\mathbb{R}): p(8) = 0 \}.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionF:exercise28
\begin{exercise}\label{chapter3:sectionF:exercise28}
    Suppose $V$ is finite-dimensional and $\varphi_{1}, \ldots, \varphi_{m}$ is a linearly independent list in $V'$. Prove that
    \[
        \dim\left( (\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{m}}) \right) = (\dim V) - m.
    \]
\end{exercise}

\begin{proof}
    According to Exercise~\ref{chapter3:sectionF:exercise23}, ${\left((\kernel{\varphi_{0}})\cap\cdots\cap(\kernel{\varphi_{m}})\right)}^{0} = \operatorname{span}(\varphi_{1}, \ldots, \varphi_{m})$, so
    \[
        \dim {\left((\kernel{\varphi_{0}})\cap\cdots\cap(\kernel{\varphi_{m}})\right)}^{0} = \dim  \operatorname{span}(\varphi_{1}, \ldots, \varphi_{m}).
    \]

    Because  $\varphi_{1}, \ldots, \varphi_{m}$ is a linearly independent list in $V'$, $\dim  \operatorname{span}(\varphi_{1}, \ldots, \varphi_{m}) = m$.

    On the other hand,
    \[
        \dim {\left((\kernel{\varphi_{0}})\cap\cdots\cap(\kernel{\varphi_{m}})\right)}^{0} = \dim V -  \dim {\left((\kernel{\varphi_{0}})\cap\cdots\cap(\kernel{\varphi_{m}})\right)}.
    \]

    Hence
    \[
        \dim\left( (\kernel{\varphi_{1}})\cap \cdots \cap (\kernel{\varphi_{m}}) \right) = (\dim V) - m.\qedhere
    \]
\end{proof}
\newpage

% chapter3:sectionF:exercise29
\begin{exercise}\label{chapter3:sectionF:exercise29}
    Suppose $V$ and $W$ are finite-dimensional and $T\in\lmap{V, W}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that if $\varphi\in W'$ and $\kernel{T'} = \operatorname{span}(\varphi)$, then $\range{T} = \kernel{\varphi}$.
        \item Prove that if $\psi\in V'$ and $\range{T'} = \operatorname{span}(\psi)$, then $\kernel{T} = \kernel{\psi}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item For any linear map $T$, we have $\kernel{T'} = {(\range{T})}^{0}$. So ${(\range{T})}^{0} = \operatorname{span}(\varphi)$.

              $w\in \range{T}$ if and only if $\varphi(w) = 0$ (because ${(\range{T})}^{0} = \operatorname{span}(\varphi)$). $\varphi(w) = 0$ if and only if $w\in\kernel{\varphi}$. Hence $\range{T} = \kernel{\varphi}$.

        \item For any linear map $T$ whose domain and codomain are finite-dimensional vector spaces, we have $\range{T'} = {(\kernel{T})}^{0}$. So ${(\kernel{T})}^{0} = \operatorname{span}(\psi)$.

              According to Exercise~\ref{chapter3:sectionF:exercise20},
              \[
                  \kernel{T} = \{ v\in V: \varphi(v) = 0 \text{ for every $\varphi\in{(\kernel T)}^{0}$} \}.
              \]

              Therefore
              \begin{align*}
                  \kernel{T} & = \{ v\in V: \varphi(v) = 0 \text{ for every $\varphi\in\operatorname{span}(\psi)$} \} \\
                             & = \{ v\in V: \psi(v) = 0 \}                                                            \\
                             & = \kernel{\psi}.\qedhere
              \end{align*}
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise30
\begin{exercise}\label{chapter3:sectionF:exercise30}
    Suppose $V$ is finite-dimensional and $\varphi_{1}, \ldots, \varphi_{n}$ is a basis of $V'$. Show that there exists a basis of $V$ whose dual basis is $\varphi_{1}, \ldots, \varphi_{n}$.
\end{exercise}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}$ be a basis of $V$ and $\psi_{1}, \ldots, \psi_{n}$ be the dual basis. There exists an invertible matrix $A$ with $n$ rows, $n$ columns such that
    \begin{align*}
        \varphi_{1} & = A_{1,1}\psi_{1} + \cdots + A_{1,n}\psi_{n}, \\
                    & \ldots                                        \\
        \varphi_{n} & = A_{n,1}\psi_{1} + \cdots + A_{n,n}\psi_{n}.
    \end{align*}

    Let $B$ be the inverse matrix of $C = A^{T}$. I define vectors $u_{1}, \ldots, u_{n}$ of $V$ as follows:
    \begin{align*}
        u_{1} & = B_{1,1}v_{1} + \cdots + B_{1,n}v_{n}, \\
              & \ldots                                  \\
        u_{n} & = B_{n,1}v_{1} + \cdots + B_{n,n}v_{n}.
    \end{align*}

    Since $B$ is invertible, it follows that $u_{1}, \ldots, u_{n}$ is also a basis of $V$. Then
    \[
        \varphi_{i}(u_{j}) = \sum^{n}_{r=1} A_{i,r}\psi_{r}(u_{j}) = \sum^{n}_{r=1} C_{r,i}\psi_{r}(u_{j}) = \sum^{n}_{r=1} C_{r,i}B_{j,r} = {(BC)}_{j,i} = I_{j,i}
    \]

    So $u_{1}, \ldots, u_{n}$ is a basis of $V$ and its dual basis is $\varphi_{1}, \ldots, \varphi_{n}$.
\end{proof}
\newpage

% chapter3:sectionF:exercise31
\begin{exercise}
    Suppose $U$ is a subspace of $V$. Let $i: U\to V$ be the inclusion map defined by $i(u) = u$. Thus $i'\in \lmap{V', U'}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $\kernel{i'} = U^{0}$.
        \item Prove that if $V$ is finite-dimensional, then $\range{i'} = U'$.
        \item Prove that if $V$ is finite-dimensional, then $\tilde{i'}$ is an isomorphism from $V'/U^{0}$ onto $U'$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $\varphi\in \kernel{i'}$ if and only if $i'(\varphi) = 0$. And $i'(\varphi) = 0$ if and only if $\varphi\circ i = 0$.

              $\varphi\circ i = 0$ if and only if $\varphi(i(u)) = 0$ for all $u\in U$. Equivalently, $\varphi(u) = 0$ for all $\varphi\in U^{0}$.

              Hence $\kernel{i'} = U^{0}$.
        \item Because $V$ is finite-dimensional, then so are $U$, $V'$, $U'$ and $U^{0}$.
              \begin{align*}
                  \dim \range{i'} & = \dim V' - \dim\kernel{i'} & \text{(fundamental theorem of linear maps)} \\
                                  & = \dim V - \dim U^{0}       & \text{($\kernel{i'} = U^{0}$)}              \\
                                  & = \dim U = \dim U'.
              \end{align*}

              On the other hand, $\range{i'}$ is a subspace of $U'$, so $\range{i'} = U'$.
        \item According to the first isomorphism theorem, $\tilde{i'}$ is an isomorphism from $V'/(\kernel{i'})$ onto $U'$. On the other hand, according to (a), $\kernel{\tilde{i'}} = U^{0}$, so we conclude that $\tilde{i'}$ is an isomorphism from $V'/U^{0}$ onto $U'$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise32
\begin{exercise}
    The \textit{double dual space} of $V$, denoted by $V''$, is defined to be the dual space of $V'$. In other words, $V'' = {(V')}'$. Define $\Lambda: V\to V''$ by
    \[
        (\Lambda v)(\varphi) = \varphi(v)
    \]

    for each $v\in V$ and each $\varphi\in V'$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $\Lambda$ is a linear map from $V$ to $V''$.
        \item Show that if $T\in \lmap{V}$, then $T''\circ \Lambda = \Lambda\circ T$, where $T'' = {(T')}'$.
        \item Show that if $V$ is finite-dimensional, then $\Lambda$ is an isomorphism from $V$ onto $V''$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item For each $v_{1}, v_{2}\in V$, $\varphi\in V'$, $\lambda_{1}, \lambda_{2}\in \mathbb{F}$
              \begin{align*}
                  (\Lambda (\lambda_{1}v_{1} + \lambda_{2}v_{2}))(\varphi) & = \varphi(\lambda_{1}v_{1} + \lambda_{2}v_{2})                               \\
                                                                           & = \lambda_{1}\varphi(v_{1}) + \lambda_{2}\varphi(v_{2})                      \\
                                                                           & = \lambda_{1}(\Lambda v_{1})(\varphi) + \lambda_{2}(\Lambda v_{2})(\varphi).
              \end{align*}

              Hence $\Lambda$ is a linear map from $V$ to $V''$.
        \item For each $v\in V$ and $\varphi\in V'$, we have
              \begin{align*}
                  ((T''\circ \Lambda)(v))(\varphi) & = (T''(\Lambda v))(\varphi)                                                \\
                                                   & = ((\Lambda v)\circ T')(\varphi)  & \text{(definition of dual map)}        \\
                                                   & = (\Lambda v)(T'(\varphi))                                                 \\
                                                   & = (T'(\varphi))(v)                & \text{(definition of double dual map)} \\
                                                   & = \varphi(Tv)                     & \text{(definition of dual map)}        \\
                                                   & = (\Lambda(Tv))(\varphi)          & \text{(definition of double dual map)} \\
                                                   & = ((\Lambda\circ T)(v))(\varphi).
              \end{align*}

              Therefore $T''\circ \Lambda = \Lambda \circ T$.
        \item Because $V$ is finite-dimensional, then so are $V'$ and $V''$.

              Let $v\in \kernel{\Lambda}$.

              $\Lambda v = 0$ if and only if $(\Lambda v)(\varphi) = 0$ for all $\varphi\in V'$. So $\Lambda v = 0$ if and only if $\varphi(v) = 0$ for all $\varphi\in V'$.

              Let $v_{1}, \ldots, v_{n}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{n}$ be its dual basis. There exist scalars $\lambda_{1}, \ldots, \lambda_{n}$ such that:
              \[
                  v = \lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n}.
              \]

              For each $k = 1,\ldots, n$, $\varphi_{k}(v) = 0$, and this implies $\lambda_{k} = 0$. So $v = 0$. Hence $\kernel{\Lambda} = \{0\}$, and $\Gamma$ is injective.

              On the other hand, $\dim V = \dim V''$, so $\Gamma$ is also an isomorphism.
    \end{enumerate}
\end{proof}
\newpage

% chapter3:sectionF:exercise33
\begin{exercise}
    Suppose $U$ is a subspace of $V$. Let $\pi: V\to V/U$ be the usual quotient map. Thus $\pi'\in \lmap{(V/U)', V'}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $\pi'$ is injective.
        \item Show that $\range \pi' = U^{0}$.
        \item Conclude that $\pi'$ is an isomorphism from ${(V/U)}'$ onto $U^{0}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $\varphi$ be an element of $\kernel{\pi'}$.

              $\varphi\in\kernel{\pi'}$ iff $\pi'(\varphi) = 0$. $\pi'(\varphi) = 0$ iff $\varphi(\pi(v)) = 0$ for all $v\in V$. $\varphi(\pi(v)) = 0$ for all $v\in V$ iff $\varphi\in {(\range{pi})}^{0}$. Therefore $\kernel{\pi'} = {(\range{\pi})}^{0}$.

              On the other hand, $\pi$ is surjective, to ${(\range{\pi})}^{0} = {(V/U)}^{0} = \{0\}$ (the annihilator of the entire codomain). So $\kernel{\pi'} = \{0\}$, and we conclude that $\pi'$ is injective.
        \item Let $\varphi$ be an element of $\range{\pi'}$. Then there exists $\psi\in {(V/U)}'$ such that $\pi'(\psi) = \varphi$. According to the definition of dual map, $\varphi = \psi\circ \pi$. Therefore, for every $u\in U$,
              \[
                  \varphi(u) = \psi(\pi(u)) = \psi(0 + U) = 0.
              \]

              So $\varphi\in U^{0}$, and then $\range{\pi'}\subseteq U^{0}$.

              Now suppose $\varphi$ is an element of $U^{0}$, then $\varphi(u) = 0$ for all $u\in U$. I define $\psi\in {(V/U)}'$ as follows:
              \[
                  \psi(v + U) = \varphi(v).
              \]

              $\psi$ is well-defined, since if $v_{1} + U = v_{2} + U$, then $v_{1} - v_{2}\in U$ and
              \[
                  \varphi(v_{1} + U) - \varphi(v_{2} + U) = \varphi(v_{1}) - \varphi(v_{2}) = \varphi(v_{1} - v_{2}) = 0.
              \]

              Hence $\pi'(\psi) = \psi\circ\pi = \varphi$, which means $\varphi\in \range{\pi'}$ and $U^{0}\subseteq \range{\pi'}$.

              Thus $\range{\pi'} = U^{0}$.
        \item Because $\pi'$ is injective, it follows that ${(V/U)}'$ is isomorphic to $\range{\pi'}$. Moreover, $\range{\pi'} = U^{0}$, so $\pi'$ is an isomorphism from ${(V/U)}'$ onto $U^{0}$.\qedhere
    \end{enumerate}
\end{proof}
\newpage

