\chapter{Multilinear Algebra and Determinants}

\section{Bilinear Forms and Quadratic Forms}

% chapter9:sectionA:exercise1
\begin{exercise}\label{chapter9:sectionA:exercise1}
    Prove that if $\beta$ is a bilinear form on $\mathbb{F}$, then there exists $c\in\mathbb{F}$ such that
    \[
        \beta(x, y) = cxy
    \]

    for all $x, y\in\mathbb{F}$.
\end{exercise}

\begin{proof}
    For all $x, y\in\mathbb{F}$
    \[
        \beta(x, y) = \beta(x\cdot 1, y\cdot 1) = x\beta(1, y\cdot 1) = xy\beta(1,1).
    \]

    Let $c = \beta(1, 1)$, then $\beta(x, y) = cxy$ for every $x, y\in\mathbb{F}$.
\end{proof}
\newpage

% chapter9:sectionA:exercise2
\begin{exercise}\label{chapter9:sectionA:exercise2}
    Let $n = \dim V$. Suppose $\beta$ is a bilinear form on $V$. Prove that there exist $\varphi_{1}, \ldots, \varphi_{n}, \tau_{1}, \ldots, \tau_{n}\in V'$ such that
    \[
        \beta(u, v) = \varphi_{1}(u)\cdot\tau_{1}(v) + \cdots + \varphi_{n}(u)\cdot\tau_{n}(v)
    \]

    for all $u, v\in V$.
\end{exercise}

\begin{quote}
    This exercise shows that if $n = \dim V$, then every bilinear form on V is of
    the form given by the last bullet point of Example 9.2.
\end{quote}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be a basis of $V$. For every vector $v$ in $V$, there exist unique scalars $a_{1}, \ldots, a_{n}$ such that
    \[
        v = a_{1}e_{1} + \cdots + a_{n}e_{n}.
    \]

    For every $k\in\{1,\ldots,n\}$, the mappings
    \begin{align*}
        \tau_{k}    & : v \mapsto a_{k}           \\
        \varphi_{k} & : v \mapsto \beta(v, e_{k})
    \end{align*}

    are linear functionals on $V$. From these, we have
    \begin{align*}
        \beta(u, v) & = \beta(u, \tau_{1}(v)e_{1} + \cdots + \tau_{n}(v)e_{n})                     \\
                    & = \beta(u, e_{1})\cdot\tau_{1}(v) + \cdots + \beta(u, e_{n})\cdot\tau_{n}(v) \\
                    & = \varphi_{1}(u)\cdot\tau_{1}(v) + \cdots + \varphi_{n}(u)\cdot\tau_{n}(v)
    \end{align*}

    for all $u, v\in V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise3
\begin{exercise}\label{chapter9:sectionA:exercise3}
    Suppose $\beta: V\times V\to\mathbb{F}$ is a bilinear form on $V$ and also is a linear functional on $V\times V$. Prove that $\beta = 0$.
\end{exercise}

\begin{proof}
    For every $u, v\in V$
    \begin{align*}
        \beta(u, v) & = \beta(u + 0, v)                                                                   \\
                    & = \beta(u, v) + \beta(0, v)               & \text{($\beta$ is a bilinear form)}     \\
                    & = \beta(u, v + 0) + \beta(0, v)                                                     \\
                    & = \beta(u, v) + \beta(u, 0) + \beta(0, v) & \text{($\beta$ is a bilinear form)}     \\
                    & = \beta(u, v) + \beta(u, v)               & \text{($\beta$ is a linear functional)}
    \end{align*}

    so $\beta(u, v) = \beta(u, v) + (-\beta(u, v)) = 0$. Hence $\beta = 0$.
\end{proof}
\newpage

% chapter9:sectionA:exercise4
\begin{exercise}\label{chapter9:sectionA:exercise4}
    Suppose $V$ is a real inner product space and $\beta$ is a bilinear form on $V$. Show that there exists a unique operator $T\in\lmap{V}$ such that
    \[
        \beta(u, v) = \innerprod{u, Tv}
    \]

    for all $u, v\in V$.
\end{exercise}

\begin{quote}
    This exercise states that if $V$ is a real inner product space, then every bilinear form on $V$ is of the form given by the third bullet point in 9.2.
\end{quote}

\begin{proof}
    $n = \dim V$. Let $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. For every $u, v\in V$
    \begin{align*}
        \beta(u, v) & = \beta(\innerprod{u,e_{1}}e_{1} + \cdots + \innerprod{u,e_{n}}e_{n}, v)            \\
                    & = \innerprod{u,e_{1}}\beta(e_{1}, v) + \cdots + \innerprod{u,e_{n}}\beta(e_{n}, v).
    \end{align*}

    Let's define an operator $T$ on $V$ as follows:
    \[
        Tv = \beta(e_{1}, v)e_{1} + \cdots + \beta(e_{n}, v)e_{n}.
    \]

    Hence for every $u, v\in V$, $\beta(u, v) = \innerprod{u, Tv}$.

    Assume operators $S, T\in\lmap{V}$ satisfy $\beta(u, v) = \innerprod{u, Sv} = \innerprod{u, Tv}$ for every $u, v\in V$. It follows that for every $v\in V$, for every $u\in V$, $\innerprod{u, Sv - Tv} = 0$, so $Sv = Tv$ for every $v\in V$. Therefore $S = T$.

    Thus there exists a unique operator $T$ on $V$ such that $\beta(u, v) = \innerprod{u, Tv}$ for all $u, v\in V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise5
\begin{exercise}\label{chapter9:sectionA:exercise5}
    Suppose $\beta$ is a bilinear form on a real inner product space $V$ and $T$ is the unique operator on $V$ such that $\beta(u, v) = \innerprod{u, Tv}$ for all $u, v \in V$ (see Exercise~\ref{chapter9:sectionA:exercise4}). Show that $\beta$ is an inner product on $V$ if and only if $T$ is an invertible positive operator on $V$.
\end{exercise}

\begin{proof}
    $\beta$ is a bilinear form on a real inner product space $V$. Therefore $\beta$ is also an inner product on $V$ if and only if $\beta$ is conjugate symmetric and positive definite.

    $(\Rightarrow)$ $\beta$ is an inner product on $V$.

    $\beta$ is conjugate symmetric, so $\beta$ is symmetric, since $\mathbb{F} = \mathbb{R}$. For every $u, v\in V$
    \[
        \innerprod{u, Tv} = \beta(u, v) = \beta(v, u) = \innerprod{v, Tu} = \innerprod{Tu, v} = \innerprod{u, T^{*}v}.
    \]

    So $Tv = T^{*}v$ for every $v\in V$, which implies $T$ is self-adjoint.

    $\innerprod{Tv, v} = \innerprod{v, Tv} = \beta(v, v) > 0$ for every nonzero $v\in V$, and $\beta(0, 0) = 0$.

    Hence $T$ is an invertible positive operator on $V$.

    \bigskip
    $(\Leftarrow)$ $T$ is an invertible positive operator on $V$.

    So $T$ is self-adjoint. For every $u, v\in V$
    \[
        \beta(v, u) = \innerprod{v, Tu} = \conj{\innerprod{Tu, v}} = \conj{\innerprod{u, T^{*}v}} = \conj{\innerprod{u, Tv}} = \conj{\beta(u, v)}
    \]

    which means $\beta$ is conjugate symmetric.

    For every $v\in V$
    \[
        \beta(v, v) = \innerprod{v, Tv} = \innerprod{Tv, v} \geq 0
    \]

    because $T$ is positive, so $\beta$ is positive. Moreover, $\beta(v, v) = 0$ if and only if $\innerprod{Tv, v} = 0$. $\innerprod{Tv, v} = 0$ if and only if $v = 0$ because $T$ is an invertible positive operator. Therefore $\beta$ is positive definite.

    Hence $\beta$ is an inner product on $V$.

    \bigskip
    Thus $\beta$ is an inner product on $V$ if and only if $T$ is an invertible positive operator on $V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise6
\begin{exercise}\label{chapter9:sectionA:exercise6}
    Prove or give a counterexample: If $\rho$ is a symmetric bilinear form on $V$, then
    \[
        \{ v\in V : \rho(v, v) = 0 \}
    \]

    is a subspace of $V$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V = \mathbb{R}^{2}$ and
    \[
        \rho((x_{1}, x_{2}), (y_{1}, y_{2})) = x_{1}y_{1} - 2x_{1}y_{2} - 2x_{2}y_{1} + 3x_{2}y_{2}
    \]

    so $\rho$ is a symmetric bilinear form on $\mathbb{R}^{2}$. $(1, 1)$ and $(1, 3)$ are in $\{ v\in V : \rho(v, v) = 0 \}$.
    \[
        \rho((1, 1), (1, 1)) = \rho((3, 1), (3, 1)) = 0.
    \]

    However,
    \[
        \rho((4, 2), (4, 2)) = 4^{2} - 4\cdot 4\cdot 2 + 3\cdot 2^{2} = 3\ne 0
    \]

    so $(4, 2)$ is not in $\{ v\in V : \rho(v, v) = 0 \}$. So $\{ v\in V : \rho(v, v) = 0 \}$ is not closed under addition, which means it is not a subspace of $\mathbb{R}^{2}$.
\end{proof}
\newpage

% chapter9:sectionA:exercise7
\begin{exercise}\label{chapter9:sectionA:exercise7}
    Explain why the proof of 9.13 (diagonalization of a symmetric bilinear form by an orthonormal basis on a real inner product space) fails if the hypothesis that $\mathbb{F} = \mathbb{R}$ is dropped.
\end{exercise}

\begin{proof}
    I give a counterexample where $\mathbb{F} = \mathbb{C}$.

    Let $\rho$ be a symmetric bilinear form on a complex inner product space $\mathbb{C}^{2}$ whose matrix (with respect to the standard basis of $\mathbb{C}^{2}$) is
    \[
        \begin{pmatrix}
            1     & \iota \\
            \iota & 0
        \end{pmatrix}.
    \]

    Let $T$ be an operator on $\mathbb{C}^{2}$ whose matrix with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}1 & \iota \\ \iota & 0 \end{pmatrix}$. The matrix of $T^{*}$ with respect to the standard basis of $\mathbb{C}^{2}$ is
    \[
        \begin{pmatrix}
            1      & -\iota \\
            -\iota & 0
        \end{pmatrix}.
    \]

    However, $T$ is not a normal operator, because
    \[
        \mathcal{M}(T)\mathcal{M}(T^{*}) =
        \begin{pmatrix}
            2 & -\iota \\
            0 & 1
        \end{pmatrix} \ne
        \begin{pmatrix}
            2 & \iota \\
            0 & 1
        \end{pmatrix} = \mathcal{M}(T^{*})\mathcal{M}(T).
    \]

    By the complex spectral theorem, there exists not exist an orthonormal basis of $\mathbb{C}^{2}$ to which $T$ has a diagonal matrix. Hence $\rho$ is not diagonalizable by an orthonormal basis.
\end{proof}
\newpage

% chapter9:sectionA:exercise8
\begin{exercise}\label{chapter9:sectionA:exercise8}
    Find formulas for $\dim V_{\text{sym}}^{(2)}$ and $\dim V_{\text{alt}}^{(2)}$ in terms of $\dim V$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be a basis of $V$.

    $V^{(2)}$ is isomorphic to $\mathbb{F}^{\dim V,\dim V}$, where each bilinear form on $V$ corresponds to its matrix with respect to $e_{1}, \ldots, e_{\dim V}$.

    A symmetric bilinear form corresponds to a symmetric matrix, and the vector space of symmetric matrices with $\dim V$ columns has dimension $(\dim V)\times(\dim V + 1)/2$. Therefore
    \[
        \dim V^{(2)}_{\text{sym}} = \frac{(\dim V) \times (\dim V + 1)}{2}.
    \]

    Moreover, $\dim V^{(2)} = {(\dim V)}^{2}$ and $V^{(2)} = V^{(2)}_{\text{sym}}\oplus V^{(2)}_{\text{alt}}$ so
    \[
        \dim V^{(2)}_{\text{alt}} = {(\dim V)}^{2} - \frac{(\dim V) \times (\dim V + 1)}{2} = \frac{(\dim V)\times (\dim V - 1)}{2}.
    \]

    Thus $\dim V^{(2)}_{\text{sym}} = (\dim V)\times(\dim V + 1)/2$ and $\dim V^{(2)}_{\text{alt}} = (\dim V)\times(\dim V - 1)/2$.
\end{proof}
\newpage

% chapter9:sectionA:exercise9
\begin{exercise}\label{chapter9:sectionA:exercise9}
    Suppose that $n$ is a positive integer and $V = \{ p\in\mathscr{P}_{n}(\mathbb{R}): p(0) = p(1) \}$. Define $\alpha: V\times V\to\mathbb{R}$ by
    \[
        \alpha(p, q) = \int^{1}_{0}pq'.
    \]

    Show that $\alpha$ is an alternating bilinear form on $V$.
\end{exercise}

\begin{proof}
    For every $p_{1}, p_{2}, q \in V$
    \[
        \alpha(p_{1} + p_{2}, q) = \int^{1}_{0}(p_{1} + p_{2})q' = \int^{1}_{0}p_{1}q' + \int^{1}_{0}p_{2}q' = \alpha(p_{1}, q) + \alpha(p_{2}, q).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(\lambda p, q) = \int^{1}_{0}(\lambda p)q' = \lambda\int^{1}_{0}pq' = \lambda\alpha(p, q).
    \]

    For every $p, q_{1}, q_{2}\in V$
    \[
        \alpha(p, q_{1} + q_{2}) = \int^{1}_{0}p(q_{1} + q_{2})' = \int^{1}_{0}pq_{1}' + \int^{1}_{0}pq_{2}' = \alpha(p, q_{1}) + \alpha(p, q_{2}).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(p, \lambda q) = \int^{1}_{0}p(\lambda q)' = \lambda\int^{1}_{0}pq' = \lambda\alpha(p, q).
    \]

    So $\alpha$ is a bilinear form on $V$.

    For every $p\in V$
    \[
        \alpha(p, p) = \int^{1}_{0}pp' = \int^{p(1)}_{p(0)}pdp = \frac{{(p(1))}^{2} - {(p(0))}^{2}}{2} = 0.
    \]

    Hence $\alpha$ is an alternating bilinear form on $V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise10
\begin{exercise}\label{chapter9:sectionA:exercise10}
    Suppose that $n$ is a positive integer and
    \[
        V = \{ p\in\mathscr{P}_{n}(\mathbb{R}): p(0) = p(1) \land p'(0) = p'(1) \}.
    \]

    Define $\rho: V\times V\to\mathbb{R}$ by
    \[
        \rho(p, q) = \int^{1}_{0}pq''.
    \]

    Show that $\rho$ is a symmetric bilinear form on $V$.
\end{exercise}

\begin{proof}
    For every $p_{1}, p_{2}, q \in V$
    \[
        \alpha(p_{1} + p_{2}, q) = \int^{1}_{0}(p_{1} + p_{2})q'' = \int^{1}_{0}p_{1}q'' + \int^{1}_{0}p_{2}q'' = \alpha(p_{1}, q) + \alpha(p_{2}, q).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(\lambda p, q) = \int^{1}_{0}(\lambda p)q'' = \lambda\int^{1}_{0}pq'' = \lambda\alpha(p, q).
    \]

    For every $p, q_{1}, q_{2}\in V$
    \[
        \alpha(p, q_{1} + q_{2}) = \int^{1}_{0}p(q_{1} + q_{2})'' = \int^{1}_{0}pq_{1}'' + \int^{1}_{0}pq_{2}'' = \alpha(p, q_{1}) + \alpha(p, q_{2}).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(p, \lambda q) = \int^{1}_{0}p(\lambda q)'' = \lambda\int^{1}_{0}pq'' = \lambda\alpha(p, q).
    \]

    So $\alpha$ is a bilinear form on $V$.

    For every $p, q\in V$
    \begin{align*}
        \alpha(p, q) & = \int^{1}_{0}pq'' = \int^{1}_{0}p(x)q''(x)dx         \\
                     & = \int^{x=1}_{x=0}p(x)dq'(x)                          \\
                     & = p(x)q'(x)\Big{\vert}^{x=1}_{x=0} - \int^{1}_{0}q'p' \\
                     & = -\int^{1}_{0}q'p'.
    \end{align*}

    Therefore $\alpha(q, p) = -\int^{1}_{0}p'q' = -\int^{1}_{0}q'p' = \alpha(p, q)$.

    Hence $\alpha$ is a symmetric bilinear form on $V$.
\end{proof}
\newpage

\section{Alternating Multilinear Forms}

% chapter9:sectionB:exercise1
\begin{exercise}\label{chapter9:sectionB:exercise1}
    Suppose $m$ is a positive integer. Show that $\dim V^{(m)} = {(\dim V)}^{m}$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{\dim V}$ be its dual basis.

    For each \textit{choices (duplication is allowed)}  of $m$ linear functionals from $\varphi_{1}, \ldots, \varphi_{\dim V}$, we define an $m$-linear form as follows, where $\varphi_{j_{1}}, \ldots, \varphi_{j_{m}}$ is the selected listed, $j_{1}, \ldots, j_{m}$ are from $1, \ldots, \dim V$
    \[
        \alpha_{j_{1},\ldots,j_{m}}: (v_{1}, \ldots, v_{m})\mapsto \varphi_{j_{1}}(v_{1})\cdots \varphi_{j_{m}}(v_{m})
    \]

    Assume that
    \[
        \sum x_{j_{1},\ldots,j_{m}}\alpha_{j_{1},\ldots,j_{m}} = 0
    \]

    for all $v_{1}, \ldots, v_{m}\in V$ and we use all previously defined $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$. Plug $(v_{j_{1}}, \ldots, v_{j_{m}})\in \underbrace{V\times\cdots\times V}_{m}$ in, we obtain
    \[
        x_{j_{1},\ldots, j_{m}} = 0.
    \]

    Hence all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ are linearly independent. Moreover, for every $m$-linear form $\alpha$ on $V$,
    \begin{align*}
        \alpha(v_{1}, \ldots, v_{m}) & = \alpha\left(\sum^{\dim V}_{j=1}\varphi_{j}(v_{1})e_{j}, \ldots, \sum^{\dim V}_{j=1}\varphi_{j}(v_{m})e_{j}\right)                                                     \\
                                     & = \sum^{\dim V}_{j_{m}=1}\left(\cdots\left(\sum^{\dim V}_{j_{1}=1}\alpha(e_{j_{1}}, \ldots, e_{j_{m}})\varphi_{j_{1}}(v_{1})\cdots\varphi_{j_{m}}(v_{m})\right)\right)  \\
                                     & = \sum^{\dim V}_{j_{m}=1}\left(\cdots\left(\sum^{\dim V}_{j_{1}=1}\alpha(e_{j_{1}}, \ldots, e_{j_{m}})\varphi_{j_{1},\ldots,j_{m}}(v_{1}, \ldots, v_{m})\right)\right).
    \end{align*}

    This means all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ spans $V^{(m)}$. Therefore all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ constitute a basis of $V^{(m)}$ and $\dim V^{(m)} = {(\dim V)}^{m}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise2
\begin{exercise}\label{chapter9:sectionB:exercise2}
    Suppose $n\geq 3$ and $\alpha: \mathbb{F}^{n}\times\mathbb{F}^{n}\times\mathbb{F}^{n} \to \mathbb{F}$ is defined by
    \begin{gather*}
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) \\
        = x_{1}y_{2}z_{3} - x_{2}y_{1}z_{3} - x_{3}y_{2}z_{1} - x_{1}y_{3}z_{2} + x_{3}y_{1}z_{2} + x_{2}y_{3}z_{1}.
    \end{gather*}

    Show that $\alpha$ is an alternating $3$-linear form on $\mathbb{F}^{n}$.
\end{exercise}

\begin{proof}
    \begin{align*}
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = x_{1}(y_{2}z_{3} - y_{3}z_{2}) + x_{2}(y_{3}z_{1} - y_{1}z_{3}) + x_{3}(y_{1}z_{2} - y_{2}z_{1}) \\
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = y_{1}(z_{2}x_{3} - z_{3}x_{2}) + y_{2}(z_{3}x_{1} - z_{1}x_{3}) + y_{3}(z_{1}x_{2} - z_{2}x_{1}) \\
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = z_{1}(x_{2}y_{3} - x_{3}y_{2}) + z_{2}(x_{3}y_{1} - x_{1}y_{3}) + z_{3}(x_{1}y_{2} - x_{2}y_{1})
    \end{align*}

    so $\alpha$ is a $3$-linear form on $\mathbb{F}^{n}$ and $\alpha$ is alternating.
\end{proof}
\newpage

% chapter9:sectionB:exercise3
\begin{exercise}\label{chapter9:sectionB:exercise3}
    Suppose $m$ is a positive integer and $\alpha$ is an $m$-linear form on $V$ such that $\alpha(v_{1}, \ldots, v_{m}) = 0$ whenever $v_{1}, \ldots, v_{m}$ is a list of vectors in $V$ with $v_{j} = v_{j+1}$ for some $j\in\{1,\ldots,m-1\}$. Prove that $\alpha$ is an alternating $m$-linear form on $V$.
\end{exercise}

\begin{proof}
    We will show that for every positive integer $k$ less than $m$, $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{j+k}$ for every $j\in\{1,\ldots,m-k\}$.

    The statement is true for $k = 1$ due to the hypothesis.

    Assume the statement is true for all $k < \ell$ where $j + \ell < m$. Suppose that $v_{j} = v_{j + \ell}$.

    Consider the expression $\alpha(\ldots, v_{j} + v_{j+k}, \ldots, v_{j+k} + v_{j}, \ldots)$ where $v_{j} + v_{j+k}$ is at the $j$th slot, and $v_{j+k} + v_{j}$ is at the ${(j+k)}$th slot, $k < \ell$.

    We prove the anti-symmetric property first. By the induction hypothesis, for every
    \[
        (\ldots, w_{j} + w_{j+k}, \ldots, w_{j+k} + w_{j}, \ldots)\in \underbrace{V\times\cdots\times V}_{m}
    \]

    we have
    \begin{align*}
        0 = \alpha(\ldots, w_{j} + w_{j+k}, \ldots, w_{j+k} + w_{j}, \ldots) & = \alpha(\ldots, w_{j}, \ldots, w_{j+k}, \ldots) + \alpha(\ldots, w_{j+k}, \ldots, w_{j}, \ldots).
    \end{align*}

    So $\alpha(\ldots, w_{j}, \ldots, w_{j+k}, \ldots) = -\alpha(\ldots, w_{j+k}, \ldots, w_{j}, \ldots)$. Apply this result, we obtain that, if $(v_{1}, \ldots, v_{m})\in \underbrace{V\times\cdots\times V}_{m}$ such that $v_{j} = v_{j+\ell}$, then
    \[
        \alpha(\ldots, v_{j}, v_{j+1}, \ldots, v_{j+\ell}, \ldots) = -\alpha(\ldots, v_{j+1}, v_{j}, \ldots, v_{j+\ell}, \ldots).
    \]

    By the induction hypothesis, $\alpha(\ldots, v_{j+1}, v_{j}, \ldots, v_{j+\ell}, \ldots) = 0$ because $v_{j}$ is at the $(j+1)$th slot and $v_{j+\ell}$ is at the $(j+\ell)$th slot. Therefore $\alpha(\ldots, v_{j}, v_{j+1}, \ldots, v_{j+\ell}, \ldots) = 0$.

    Due to the principle of mathematical induction, for every positive integer $k < m$, for every positive integer $j\in\{1,\ldots,m-k\}$, $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{j + k}$.

    Thus $\alpha$ is an alternating $m$-linear form on $V$.
\end{proof}
\newpage

% chapter9:sectionB:exercise4
\begin{exercise}\label{chapter9:sectionB:exercise4}
    Prove or give a counterexample: If $\alpha\in V^{(4)}_{\text{alt}}$, then
    \[
        \{ (v_{1}, v_{2}, v_{3}, v_{4})\in V^{4}: \alpha(v_{1}, v_{2}, v_{3}, v_{4}) = 0 \}
    \]

    is a subspace of $V^{4}$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V$ is a vector space with dimension $4$. Let $e_{1}, e_{2}, e_{3}, e_{4}$ be a basis of $V$, and $\alpha$ a nonzero alternating $4$-linear form on $V$. Because $\alpha$ is alternating,
    \[
        \alpha(e_{1}, e_{1}, e_{3}, 0) = \alpha(0, e_{2}, e_{2}, e_{4}) = 0.
    \]

    However $(e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4}) = (e_{1}, e_{1}, e_{3}, 0) + (0, e_{2}, e_{2}, e_{4})$ and
    \[
        \alpha(e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4})\ne 0
    \]

    because $e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4}$. Therefore the set
    \[
        \{ (v_{1}, v_{2}, v_{3}, v_{4})\in V^{4}: \alpha(v_{1}, v_{2}, v_{3}, v_{4}) = 0 \}
    \]

    is not closed under addition, which implies it is not a subspace of $V^{4}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise5
\begin{exercise}\label{chapter9:sectionB:exercise5}
    Suppose $m$ is a positive integer and $\beta$ is an $m$-linear form on $V$. Define an $m$-linear form $\alpha$ on $V$ by
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}(\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    for $v_{1}, \ldots, v_{m}\in V$. Explain why $\alpha\in V^{(m)}_{\text{alt}}$.
\end{exercise}

\begin{proof}
    For each $(j_{1}, \ldots, j_{m})\in \operatorname{perm}m$, the map
    \[
        (v_{1}, \ldots, v_{m})\mapsto (\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    is an $m$-linear form on $V$. Therefore
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}(\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    is an $m$-linear form on $V$.

    If $v_{j} = v_{k}$ for some $j\ne k$ then for each permutation $(x_{1}, \ldots, x_{m})$ in $\operatorname{perm}m$, there exists a unique permutation $(y_{1}, \ldots, y_{m})$ in $\operatorname{perm}m$ where these two differ by a swap $j\leftrightarrow k$, hence one has $+1$ sign and the other has $-1$ sign. Moreover,
    \[
        \beta(v_{x_{1}}, \ldots, v_{x_{m}}) = \beta(v_{y_{1}}, \ldots, v_{y_{m}}).
    \]

    Therefore $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{k}$ for some $j\ne k$. Hence $\alpha$ is alternating.

    Thus $\alpha\in V^{(m)}_{\text{alt}}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise6
\begin{exercise}\label{chapter9:sectionB:exercise6}
    Suppose $m$ is a positive integer and $\beta$ is an $m$-linear form on $V$. Define an $m$-linear form $\alpha$ on $V$ by
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    for $v_{1}, \ldots, v_{m}\in V$. Explain why
    \[
        \alpha(v_{k_{1}}, \ldots, v_{k_{m}}) = \alpha(v_{1}, \ldots, v_{m})
    \]

    for all $v_{1}, \ldots, v_{m}\in V$ and all $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$.
\end{exercise}

\begin{proof}
    For each $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$, $\alpha(v_{k_{1}}, \ldots, v_{k_{m}})$ is the sum of all $\beta(v_{i_{1}}, \ldots, v_{i_{m}})$ where each $(i_{1}, \ldots, i_{m})$ is a permutation of $(k_{1}, \ldots, k_{m})$. On the other hand, each $(i_{1}, \ldots, i_{m})$ is also a permutation of $(1, \ldots, m)$. Therefore $\alpha(v_{k_{1}}, \ldots, v_{k_{m}}) = \alpha(v_{1}, \ldots, v_{m})$ for all $v_{1}, \ldots, v_{m}\in V$ and all $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$.
\end{proof}
\newpage

% chapter9:sectionB:exercise7
\begin{exercise}\label{chapter9:sectionB:exercise7}
    Give an example of a nonzero alternating $2$-linear form $\alpha$ on $\mathbb{R}^{3}$ and a linearly independent list $v_{1}, v_{2}$ in $\mathbb{R}^{3}$ such that $\alpha(v_{1}, v_{2}) = 0$.
\end{exercise}

\begin{quote}
    This exercise shows that 9.39 can fail if the hypothesis that $n = \dim V$ is deleted.
\end{quote}

\begin{proof}
    Here is an example.
    \[
        \alpha((x_{1}, x_{2}, x_{3}), (y_{1}, y_{2}, y_{3})) = x_{1}y_{2} - x_{2}y_{1}
    \]

    $\alpha\in {(\mathbb{R}^{3})}^{(2)}_{\text{alt}}$. However,
    \[
        \alpha((1, 0, 0), (1, 0, 1)) = 0
    \]

    although $(1, 0, 0), (1, 0, 1)$ is a linearly independent list.
\end{proof}
\newpage

\section{Determinants}

% chapter9:sectionC:exercise1
\begin{exercise}\label{chapter9:sectionC:exercise1}
    Prove or give a counterexample: $S, T\in\lmap{V}\implies \det(S + T) = \det S + \det T$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    Let $V$ be a real finite-dimensional vector space of dimension greater than $1$, then
    \[
        \det(I + I) = 2^{\dim V} \ne 1 + 1 = \det I + \det I.
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise2
\begin{exercise}\label{chapter9:sectionC:exercise2}
    Suppose the first column of a square matrix $A$ consists of all zeros except possibly the first entry $A_{1,1}$. Let $B$ be the matrix obtained from $A$ by deleting the first row and the first column of $A$. Show that $\det A = A_{1,1} \det B$.
\end{exercise}

\begin{proof}
    Let $n$ be the number of columns of $A$. By the Leibniz's determinant formula
    \begin{align*}
        \det A & = \sum_{(j_{1}, \ldots, j_{n})\in\operatorname{perm}n}(\operatorname{sign}(j_{1}, \ldots, j_{n}))A_{j_{1},1}\cdots A_{j_{n},n}
    \end{align*}

    If $j_{1}\ne 1$, then $A_{j_{1},1}\cdots A_{j_{n},n} = 0$ because $A_{i,1} = 0$ for all $i\ne 1$. Therefore
    \begin{align*}
        \det A & = A_{1,1}\sum_{(j_{2}, \ldots, j_{n})\in\operatorname{perm}(2,\ldots,n)}\operatorname{sign}(j_{2}, \ldots, j_{n})A_{j_{2},2}\cdots A_{j_{n},n} \\
               & = A_{1,1}\sum_{(i_{1},\ldots, i_{n-1})\in\operatorname{perm}(n-1)}B_{i_{1},1}\cdots B_{i_{n-1},n-1}                                            \\
               & = A_{1,1}\det B
    \end{align*}

    where $\operatorname{sign}(j_{2}, \ldots, j_{n}) = \operatorname{sign}(1, j_{2}, \ldots, j_{n})$ because $1 < j_{2}, \ldots, j_{n}$.

    Thus $\det A = A_{1,1}\det B$.
\end{proof}
\newpage

% chapter9:sectionC:exercise3
\begin{exercise}\label{chapter9:sectionC:exercise3}
    Suppose $T\in\lmap{V}$ is nilpotent. Prove that $\det(I + T) = 1$.
\end{exercise}

\begin{proof}
    Because $T$ is nilpotent, then there exists a basis $e_{1}, \ldots, e_{\dim V}$ of $V$ to which $T$ has an upper-triangular matrix whose entries on the diagonal are $0$. Therefore the matrix of $I + T$ with respect to the basis $e_{1}, \ldots, e_{\dim V}$ of $V$ is an upper-triangular matrix whose entries on the diagonal are $1$. Thus $\det(I + T) = 1$.
\end{proof}
\newpage

% chapter9:sectionC:exercise4
\begin{exercise}\label{chapter9:sectionC:exercise4}
    Suppose $S\in\lmap{V}$. Prove that $S$ is unitary if and only if $\abs{\det S} = \norm{S} = 1$.
\end{exercise}

\begin{proof}
    Let $s_{1}\geq s_{2}\geq \cdots \geq s_{\dim V}$ be the singular values of $S$, then
    \[
        \abs{\det S} = s_{1}\cdots s_{\dim V}
    \]

    and
    \[
        \norm{S} = s_{1}.
    \]

    Therefore $\abs{\det S} = \norm{S} = 1$ if and only if all singular values of $S$ are equal to $1$. Moreover, all singular values of $S$ are equal to $1$ if and only if $S$ is unitary.

    Thus $S$ is unitary if and only if $\abs{\det S} = \norm{S} = 1$.
\end{proof}
\newpage

% chapter9:sectionC:exercise5
\begin{exercise}\label{chapter9:sectionC:exercise5}
    Suppose $A$ is a block upper-triangular matrix
    \[
        A = \begin{pmatrix}
            A_{1} &        & *     \\
                  & \ddots &       \\
            0     &        & A_{m}
        \end{pmatrix},
    \]

    where each $A_{k}$ along the diagonal is a square matrix. Prove that
    \[
        \det A = (\det A_{1})\cdots (\det A_{m}).
    \]
\end{exercise}

\begin{proof}
    Firstly, for brevity (I don't want to use lengthy indices), I will prove that if
    \[
        C = \begin{pmatrix}
            A & * \\
            0 & B
        \end{pmatrix}
    \]

    then $\det C = (\det A)(\det B)$.

    Let the numbers of columns of $A, B$ be $m, n$, respectively. Let $T$ be the operator on $V = \mathbb{F}^{m+n}$ whose matrix with respect to the standard basis of $V = \mathbb{F}^{m+n}$ is $C$.

    Let $\alpha$ be a nonzero alternating $(m+n)$-linear form in $V^{(m)}_{\text{alt}}$.
    \begin{align*}
        (\det T)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}) & = \alpha(Te_{1}, \ldots, Te_{m}, Te_{m+1}, \ldots, Te_{m+n})
    \end{align*}

    Let $U = \operatorname{span}(e_{1}, \ldots, e_{n})$, then $U$ is invariant under $T$. Moreover, the matrix of $T\vert_{U}$ with respect to $e_{1}, \ldots, e_{n}$ is $A$. The following map
    \[
        (v_{1}, \ldots, v_{m})\mapsto \alpha(Tv_{1}, \ldots, Tv_{m}, Te_{m+1}, \ldots, Te_{m+n})
    \]

    is an alternating $m$-linear form on $U$. Therefore
    \begin{align*}
        \alpha(Te_{1}, \ldots, Te_{m}, Te_{m+1}, \ldots, Te_{m+n}) & = (\det T\vert_U)\alpha(e_{1}, \ldots, e_{m}, Te_{m+1}, \ldots, Te_{m+n}) \\
                                                                   & = (\det A)\alpha(e_{1}, \ldots, e_{m}, Te_{m+1}, \ldots, Te_{m+n})
    \end{align*}

    Moreover, for each $j\in\{1,\ldots,n\}$
    \begin{align*}
        Te_{m+j} & = (C_{1,m+j}e_{1} + \cdots + C_{m, m+j}e_{m}) + (C_{m+1,j}e_{m+1} + \cdots + C_{m+n,j}e_{m+n}) \\
                 & = (C_{1,m+j}e_{1} + \cdots + C_{m, m+j}e_{m}) + (B_{1,j}e_{m+1} + \cdots + B_{n,j}e_{m+n})
    \end{align*}

    and $\alpha$ is an alternating $(m+n)$-linear form on $V$, so we have
    \[
        \alpha(e_{1}, \ldots, e_{m}, Te_{m+1}, \ldots, Te_{m+n}) = \alpha(e_{1}, \ldots, e_{m}, \sum^{n}_{i=1}B_{i,1}e_{m+i}, \ldots, \sum^{n}_{i=1}B_{i,n}e_{m+i})
    \]

    Let $S$ be the operator on $W = \operatorname{span}(e_{m+1}, \ldots, e_{m+n})$ whose matrix with respect to the basis $e_{m+1}, \ldots, e_{m+n}$ is $B$, then
    \[
        \alpha(e_{1}, \ldots, e_{m}, \sum^{n}_{i=1}B_{i,1}e_{m+i}, \ldots, \sum^{n}_{i=1}B_{i,n}e_{m+i}) = \alpha(e_{1}, \ldots, e_{m}, Se_{m+1}, \ldots, Se_{m+n}).
    \]

    The following map
    \[
        (v_{1}, \ldots, v_{n})\mapsto \alpha(e_{1}, \ldots, e_{m}, Sv_{1}, \ldots, Sv_{n})
    \]

    is an alternating $n$-linear form on $W$, so
    \begin{align*}
        \alpha(e_{1}, \ldots, e_{m}, Se_{m+1}, \ldots, Se_{m+n}) & = (\det S)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n})  \\
                                                                 & = (\det B)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}).
    \end{align*}

    Combine every so far together, we obtain
    \begin{align*}
        (\det C)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}) & = \alpha(Te_{1}, \ldots, Te_{m}, Te_{m+1}, \ldots, Te_{m+n})              \\
                                                                       & = (\det A)(\det B)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}).
    \end{align*}

    Hence $\det C = (\det A)(\det B)$.

    \bigskip
    Back to the original problem. I give a proof using mathematical induction on $m$.

    The statement is true for $m = 1$.

    Assume the statement is true for $m = n - 1$. Now suppose $A$ is a block upper-triangular matrix has $n$ blocks. Let $B$ be the block upper-triangular matrix obtained by removing the rows and columns of $A$ containing entries of $A_{n}$. By the previously proved result,
    \[
        \det A = (\det B)(\det A_{n}).
    \]

    According to the induction hypothesis, $\det B = (\det A_{1})\cdots (\det A_{n-1})$. Therefore $\det A = (\det A_{1})\cdots (\det A_{n})$.

    By the principle of mathematical induction, we conclude that
    \[
        \det A = (\det A_{1})\cdots (\det A_{m}).\qedhere
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise6
\begin{exercise}\label{chapter9:sectionC:exercise6}
    Suppose $A = \begin{pmatrix}v_{1} & \cdots & v_{n}\end{pmatrix}$ is an $n$-by-$n$ matrix, with $v_{k}$ denoting $k$th column of $A$. Show that if $(m_{1}, \ldots, m_{n})\in \operatorname{perm}n$, then
    \[
        \det\begin{pmatrix}v_{m_{1}} & \cdots & v_{m_{n}}\end{pmatrix} = (\operatorname{sign}(m_{1}, \ldots, m_{n}))\det A.
    \]
\end{exercise}

\begin{proof}
    Because $(u_{1}, \ldots, u_{n})\mapsto \det\begin{pmatrix}u_{1} & \cdots & u_{n}\end{pmatrix}$ is an alternating $n$-linear form on $\mathbb{F}^{n}$, then
    \begin{align*}
        \det\begin{pmatrix}v_{m_{1}} & \cdots & v_{m_{n}}\end{pmatrix} & = (\operatorname{sign}(m_{1}, \ldots, m_{n}))\det\begin{pmatrix}v_{1} & \cdots & v_{n}\end{pmatrix} \\
                                                                                                 & = (\operatorname{sign}(m_{1}, \ldots, m_{n}))\det A.\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter9:sectionC:exercise7
\begin{exercise}\label{chapter9:sectionC:exercise7}
    Suppose $T\in\lmap{V}$ is invertible. Let $p$ denote the characteristic polynomial of $T$ and let $q$ denote the characteristic polynomial of $T^{-1}$. Prove that
    \[
        q(z) = \frac{1}{p(0)}z^{\dim V}p\left(\frac{1}{z}\right)
    \]

    for all nonzero $z\in\mathbb{F}$.
\end{exercise}

\begin{proof}
    For all nonzero $z\in\mathbb{F}$,
    \begin{align*}
        q(z) & = \det(zI - T^{-1})                                                         & \text{(definition of characteristic polynomial)} \\
             & = \det(T^{-1}zT - T^{-1})                                                                                                      \\
             & = \det(T^{-1})\det(zT - I)                                                  & (\det(AB) = (\det A)(\det B))                    \\
             & = \frac{1}{\det T}\det(zT - I)                                              & (1 = (\det T^{-1})(\det T))                      \\
             & = \frac{1}{\det T}{(-z)}^{\dim V}\det\left(\frac{1}{z}I - T\right)          & (\det\lambda T = \lambda^{\dim V}\det T)         \\
             & = \frac{{(-1)}^{\dim V}}{\det T}z^{\dim V}\det\left(\frac{1}{z}I - T\right)                                                    \\
             & = \frac{1}{p(0)}z^{\dim V}p\left(\frac{1}{z}\right).
    \end{align*}
\end{proof}
\newpage

% chapter9:sectionC:exercise8
\begin{exercise}\label{chapter9:sectionC:exercise8}
    Suppose $T \in \lmap{V}$ is an operator with no eigenvalues (which implies that $\mathbb{F} = \mathbb{R}$). Prove that $\det T > 0$.
\end{exercise}

\begin{proof}
    Let $p$ be the characteristic polynomial of $T$. Because $T$ has no eigenvalue, then $p$ has no root. Therefore $p(x)\ne 0$ for every $x\in\mathbb{R}$.

    Assume there exist $a, b\in\mathbb{R}$ such that $p(a) < 0$ and $p(b) > 0$. Because of the intermediate value theorem and $p$ is continuous on $\mathbb{R}$, there exists $c\in\mathbb{R}$ and $a < c < b$ such that $p(c) = 0$. This is a contradiction because $p$ has no root. Therefore, either $p(x) > 0$ for every $x\in\mathbb{R}$ or $p(x) < 0$ for every $x\in\mathbb{R}$.

    On the other hand $p$ is a monic polynomial with real coefficient, so
    \[
        \lim\limits_{x\to+\infty}\frac{p(x)}{x^{\dim V}} = 1
    \]

    which implies $\lim\limits_{x\to+\infty}p(x) = +\infty$. Hence there exists a real number $x_{0}$ such that $p(x_{0}) > 0$. Since either $p(x) > 0$ for every $x\in\mathbb{R}$ or $p(x) < 0$ for every $x\in\mathbb{R}$, we conclude that $p(x) > 0$ for all $x\in\mathbb{R}$.

    $\dim V$ is an even number, because if $\dim V$ is an odd number, then $p$ has a root. Therefore the constant term of $p$ is ${(-1)}^{\dim V}\det T = \det T$. So $\det T = p(0) > 0$.
\end{proof}
\newpage

% chapter9:sectionC:exercise9
\begin{exercise}\label{chapter9:sectionC:exercise9}
    Suppose that $V$ is a real vector space of even dimension, $T \in \lmap{V}$, and $\det T < 0$. Prove that $T$ has at least two distinct eigenvalues.
\end{exercise}

\begin{proof}
    Let $p$ be the characteristic polynomial of $T$. Because $\dim V$ is an even number, the constant term of $p$ is ${(-1)}^{\dim V} = \det T$.

    We have
    \[
        \lim\limits_{x\to{\color{red}{+\infty}}}\frac{p(x)}{x^{\dim V}} = \lim\limits_{x\to{\color{blue}{-\infty}}}\frac{p(x)}{x^{\dim V}} = 1.
    \]

    So $\lim\limits_{x\to{\color{red}{+\infty}}}p(x) = \lim\limits_{x\to{\color{blue}{-\infty}}}p(x) = +\infty$. Therefore, there are positive number $a$ and negative number $b$ such that $p(a) > 0$ and $p(b) > 0$.

    Since $\det T < 0$ and $\det T = p(0)$, it follows that $p(0) < 0$. By the intermediate value theorem, there is a positive number $c_{1}$ less than $a$ such that $p(c_{1}) = 0$ and there is a negative number $c_{2}$ larger than $b$ such that $p(c_{2}) = 0$. Moreover, $c_{1}$ and $c_{2}$ are distinct and are also eigenvalues of $T$.

    Thus $T$ has at least two distinct eigenvalues.
\end{proof}
\newpage

% chapter9:sectionC:exercise10
\begin{exercise}\label{chapter9:sectionC:exercise10}
    Suppose $V$ is a real vector space of odd dimension and $T \in \lmap{V}$. Without using the minimal polynomial, prove that $T$ has an eigenvalue.
\end{exercise}

\begin{quote}
    This result was previously proved without using determinants or the characteristic polynomial $-$ see 5.34.
\end{quote}

\begin{proof}
    $V$ is a real vector space of odd dimension, so the characteristic polynomial of $T$ has real coefficients and odd degree. Therefore the characteristic polynomial of $T$ has a root, which implies $T$ has an eigenvalue.
\end{proof}
\newpage

% chapter9:sectionC:exercise11
\begin{exercise}\label{chapter9:sectionC:exercise11}
    Prove or give a counterexample: If $\mathbb{F} = \mathbb{R}$, $T\in\lmap{V}$, and $\det T > 0$, then $T$ has a square root.
\end{exercise}

\begin{quote}
    If $\mathbb{F} = \mathbb{C}$, $T\in\lmap{V}$, and $\det T\ne 0$, then $T$ has a square root (see 8.41).
\end{quote}

\begin{proof}
    Here is a counterexample.

    On $V = \mathbb{R}^{2}$, let $T(x, y) = (-x, x - y)$. The matrix of $T$ with respect to $(1, 0), (0, 1)$ is
    \[
        A = \begin{pmatrix}
            -1 & 1  \\
            0  & -1
        \end{pmatrix}.
    \]

    $\det T = \det A = 1 > 0$. Assume $T$ has a square root $S\in\lmap{\mathbb{R}^{2}}$, let the matrix of $S$ with respect to $(1, 0), (0, 1)$ be $B$. Because $T = S^{2}$, so $B^{2} = A$, and we obtain the following system of equations
    \[
        \begin{cases}
            B_{1,1}^{2} + B_{1,2}B_{2,1} = -1, \\
            B_{2,2}^{2} + B_{1,2}B_{2,1} = -1, \\
            B_{1,2}(B_{1,1} + B_{2,2}) = 1,    \\
            B_{2,1}(B_{1,1} + B_{2,2}) = 0.
        \end{cases}
    \]

    From the last two equations, we deduce that $B_{1,1} + B_{2,2}\ne 0$ and $B_{2,1} = 0$. Therefore $-1 = B_{1,1}^{2} + B_{1,2}B_{2,1} = B_{1,1}^{2}$, which is impossible because $B_{1,1}$ is a real number.

    Hence the chosen operator $T$ does not have a square root.
\end{proof}
\newpage

% chapter9:sectionC:exercise12
\begin{exercise}\label{chapter9:sectionC:exercise12}
    Suppose $S, T\in\lmap{V}$ and $S$ is invertible. Define $p: \mathbb{F}\to\mathbb{F}$ by
    \[
        p(z) = \det(zS - T).
    \]

    Prove that $p$ is a polynomial of degree $\dim V$ and that the coefficient of $z^{\dim V}$ in this polynomial is $\det S$.
\end{exercise}

\begin{proof}
    \[
        p(z) = \det(zS - T) = \det(zIS - TS^{-1}S) = \det(zI - TS^{-1})\det(S)
    \]

    so the coefficient of $z^{\dim V}$ in this polynomial is $\det S$.
\end{proof}
\newpage

% chapter9:sectionC:exercise13
\begin{exercise}\label{chapter9:sectionC:exercise13}
    Suppose $\mathbb{F} = \mathbb{C}$, $T\in\lmap{V}$, and $n = \dim V > 2$. Let $\lambda_{1}, \ldots, \lambda_{n}$ denote the eigenvalues of $T$, with each eigenvalue included as many times as its multiplicity.
    \begin{enumerate}[label={(\alph*)}]
        \item Find a formula for the coefficient of $z^{n-2}$ in the characteristic polynomial of $T$ in terms of $\lambda_{1}, \ldots, \lambda_{n}$.
        \item Find a formula for the coefficient of $z$ in the characteristic polynomial of $T$ in terms of $\lambda_{1}, \ldots, \lambda_{n}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    The characteristic polynomial of $T$ is $(z - \lambda_{1})\cdots (z - \lambda_{n})$.

    \begin{enumerate}[label={(\alph*)}]
        \item The coefficient of $z^{n-2}$ in the characteristic polynomial of $T$ is
              \[
                  \sum_{1\leq i < j\leq n}\lambda_{i}\lambda_{j}.
              \]
        \item The coefficient of $z$ in the characteristic polynomial of $T$ is
              \[
                  {(-1)}^{n-1}\sum^{n}_{i=1}\left(\prod^{n}_{\stackrel{j=1}{j\ne i}}\lambda_{j}\right).
              \]
    \end{enumerate}
\end{proof}
\newpage

% chapter9:sectionC:exercise14
\begin{exercise}\label{chapter9:sectionC:exercise14}
    Suppose $V$ is an inner product space and $T$ is a positive operator on $V$. Prove that
    \[
        \det\sqrt{T} = \sqrt{\det T}.
    \]
\end{exercise}

\begin{proof}
    $T$ is a positive operator so $T$ is self-adjoint and hence normal. By the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{\dim V}$ of $V$ which consists of eigenvectors of $T$. Let the eigenvalue of $T$ corresponding to $e_{k}$ be $\lambda_{k}$, for every $k\in\{1,\ldots,\dim V\}$.

    Because $T$ is a positive operator, all eigenvalues of $T$ are nonnegative, and $T$ has a unique positive square root $\sqrt{T}$. Moreover
    \[
        \sqrt{T}e_{k} = \sqrt{\lambda_{k}}e_{k}
    \]

    for every $k\in\{1,\ldots,\dim V\}$. Hence
    \[
        \det\sqrt{T} = \sqrt{\lambda_{1}}\cdots\sqrt{\lambda_{\dim V}} = \sqrt{\lambda_{1}\cdots\lambda_{\dim V}} = \sqrt{\det T}.
    \]

    Thus $\det\sqrt{T} = \sqrt{\det T}$.
\end{proof}
\newpage

% chapter9:sectionC:exercise15
\begin{exercise}\label{chapter9:sectionC:exercise15}
    Suppose $V$ is an inner product space and $T\in\lmap{V}$. Use the polar decomposition to give a proof that
    \[
        \abs{\det T} = \sqrt{\det(T^{*}T)}
    \]

    that is different from the proof given earlier (see 9.60).
\end{exercise}

\begin{proof}
    By the polar decomposition, there exists an unitary operator $S\in\lmap{V}$ such that $T = S\sqrt{T^{*}T}$. Since $\abs{\det S} = 1$, we have
    \[
        \abs{\det T} = \abs{\det S\sqrt{T^{*}T}} = \abs{(\det S)(\det\sqrt{T^{*}T})} = \abs{\det S}\abs{\det\sqrt{T^{*}T}} = \abs{\det\sqrt{T^{*}T}}.
    \]

    $T^{*}T$ is a positive operator on $V$. By Exercise~\ref{chapter9:sectionC:exercise14}, $\det\sqrt{T^{*}T} = \sqrt{\det(T^{*}T)}$, we obtain
    \[
        \abs{\det T} = \sqrt{\det(T^{*}T)}.
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise16
\begin{exercise}\label{chapter9:sectionC:exercise16}
    Suppose $T\in\lmap{V}$. Define $g: \mathbb{F}\to\mathbb{F}$ by $g(x) = \det(I + xT)$. Show that $g'(0) = \operatorname{tr}T$.
\end{exercise}

\begin{quote}
    Look for a clean solution to this exercise, without using the explicit but complicated formula for the determinant of a matrix.
\end{quote}

\begin{proof}
    $g$ is a polynomial function, so $g$ is differentiable.

    Let $n = \dim V$ and $e_{1}, \ldots, e_{n}$ be a basis of $V$. Let $\alpha$ be an alternating multilinear form in $V^{(\dim V)}_{\text{alt}}$ such that $\alpha(e_{1}, \ldots, e_{n}) = 1$.
    \begin{align*}
        \det(I + xT) & = (\det(I + xT))\alpha(e_{1}, \ldots, e_{n})                                                                        \\
                     & = \alpha(e_{1} + xTe_{1}, \ldots, e_{n} + xTe_{n})                                                                  \\
                     & = \alpha(e_{1}, \ldots, e_{n}) + x\alpha_{1}(e_{1}, \ldots, e_{n}) + \cdots + x^{n}\alpha_{n}(e_{1}, \ldots, e_{n}) \\
                     & = 1 + x\alpha_{1}(e_{1}, \ldots, e_{n}) + \cdots + x^{n}\alpha_{n}(e_{1}, \ldots, e_{n})
    \end{align*}

    where $\alpha_{k}(e_{1}, \ldots, e_{n})$ is defined as follows: $\operatorname{comb}(n, k)$ is the set of subsets of $k$ elements from $\{1,\ldots,n\}$
    \[
        \alpha_{k}(e_{1}, \ldots, e_{n}) = \sum_{(i_{1}, \ldots, i_{k})\in\operatorname{comb}(n,k)}\alpha(v_{1}, \ldots, v_{n})
    \]

    where $v_{j} = Te_{j}$ if $j\in\{ i_{1}, \ldots, i_{k} \}$, and otherwise, $v_{j} = e_{j}$.

    Therefore
    \begin{multline*}
        \det(I + xT) - 1 \\
        = x(\alpha(Te_{1}, \ldots, e_{n}) + \cdots + \alpha(e_{1}, \ldots, Te_{n})) + x^{2}\alpha_{2}(e_{1}, \ldots, e_{n}) + \cdots + x^{n}\alpha_{n}(e_{1}, \ldots, e_{n}).
    \end{multline*}

    Let $A$ be the matrix of $T$ with respect to $e_{1}, \ldots, e_{n}$, then
    \[
        Te_{j} = A_{1,j}e_{1} + \cdots + A_{n,j}e_{n}.
    \]

    Because $\alpha$ is an alternating multilinear form,
    \begin{align*}
        \alpha(Te_{1}, \ldots, e_{n}) + \cdots + \alpha(e_{1}, \ldots, Te_{n}) & = \alpha(A_{1,1}e_{1}, \ldots, e_{n}) + \cdots + \alpha(e_{1}, \ldots, A_{n,n}e_{n}) \\
                                                                               & = (A_{1,1} + \cdots + A_{n,n})\alpha(e_{1}, \ldots, e_{n})                           \\
                                                                               & = \operatorname{tr}T.
    \end{align*}

    Therefore
    \[
        \frac{\det(I + xT) - 1}{x} = \operatorname{tr}T + x\alpha_{2}(e_{1}, \ldots, e_{n}) + \cdots + x^{n-1}\alpha_{n}(e_{1}, \ldots,e_{n})
    \]

    so
    \[
        \lim\limits_{x\to 0}\frac{\det(I + xT) - 1}{x} = \operatorname{tr}T.
    \]

    Equivalently, $g'(0) = \operatorname{tr}T$.
\end{proof}
\newpage

% chapter9:sectionC:exercise17
\begin{exercise}\label{chapter9:sectionC:exercise17}
    Suppose $a, b, c$ are positive numbers. Find the volume of the ellipsoid
    \[
        \left\{ (x,y,z)\in\mathbb{R}^{3}: \frac{x^{2}}{a^{2}} + \frac{y^{2}}{b^{2}} + \frac{z^{2}}{c^{2}} < 1 \right\}
    \]

    by finding a set $\Omega\subseteq\mathbb{R}^{3}$ whose volume you know and an operator $T$ on $\mathbb{R}^{3}$ such that $T(\Omega)$ equals the ellipsoid above.
\end{exercise}

\begin{proof}
    Let $T$ be the operator on $\mathbb{R}^{3}$ defined by $T(x, y, z) = \left(\frac{x}{a}, \frac{y}{b}, \frac{z}{c}\right)$.

    \[
        (x, y, z)\in \left\{ (x,y,z)\in\mathbb{R}^{3}: \frac{x^{2}}{a^{2}} + \frac{y^{2}}{b^{2}} + \frac{z^{2}}{c^{2}} < 1 \right\}
    \]

    if and only if
    \[
        T(x, y, z) = \left(\frac{x}{a}, \frac{y}{b}, \frac{z}{c}\right)\in \Omega = \left\{ (x,y,z)\in\mathbb{R}^{3}: x^{2} + y^{2} + z^{2} < 1 \right\}.
    \]

    The volume of the ellipsoid (in fact, this is a sphere)
    \[
        \Omega = \left\{ (x,y,z)\in\mathbb{R}^{3}: x^{2} + y^{2} + z^{2} < 1 \right\}
    \]

    is $\frac{4}{3}\pi$. On the other hand
    \[
        \abs{T^{-1}(\Omega)} = \abs{\det T^{-1}}\abs{\Omega}
    \]

    and $\abs{\Omega} = \frac{4}{3}\pi$, $\det T^{-1} = abc$. Thus the volume of the given ellipsoid is $\frac{4}{3}\pi abc$.
\end{proof}
\newpage

% chapter9:sectionC:exercise18
\begin{exercise}\label{chapter9:sectionC:exercise18}
    Suppose that $A$ is an invertible square matrix. Prove that Hadamard's inequality (9.66) is an equality if and only if each column of $A$ is orthogonal to the other columns.
\end{exercise}

\begin{proof}
    I rewrite the proof of the Hadamard's inequality here.

    Let $n$ be the number of columns of $A$. $v_{1}, \ldots, v_{n} \in \mathbb{F}^{n}$ are the columns of $A$.

    If $A$ is not invertible, then $\abs{\det A} = 0 \leq \norm{v_{1}}\cdots \norm{v_{n}}$. Suppose $A$ is invertible. By the QR decomposition, there exists an unitary matrix $Q$ and an upper-triangular matrix $R$ whose entries on the diagonal are positive numbers such that $A = QR$.
    \begin{align*}
        \abs{\det A} & = \abs{\det Q}\abs{\det R}             \\
                     & = \abs{\det R}                         \\
                     & = \prod^{n}_{k=1}R_{k,k}               \\
                     & \leq \prod^{n}_{k=1}\norm{R_{\cdot,k}} \\
                     & = \prod^{n}_{k=1}\norm{QR_{\cdot,k}}   \\
                     & = \prod^{n}_{k=1}\norm{v_{k}}.
    \end{align*}

    Assume that $\abs{\det A} = \prod^{n}_{k=1}\norm{v_{k}}$.

    If $A$ is not invertible, then $\det A = 0$ and $\prod^{n}_{k=1}\norm{v_{k}} = 0$. $\prod^{n}_{k=1}\norm{v_{k}} = 0$ if and only if $v_{1} = \cdots = v_{n} = 0$, which implies $A = 0$. Therefore the columns of $A$ are pairwise orthogonal.

    If $A$ is invertible, the proof implies
    \[
        \prod^{n}_{k=1}R_{k,k} = \prod^{n}_{k=1}\norm{R_{\cdot,k}}.
    \]

    This happens if and only if $R$ is a diagonal matrix. Since $R$ is a diagonal matrix, the $k$th column of $A$ is $Q_{\cdot,k}R_{k,k}$. Because the columns of $Q$ are pairwise orthogonal, then $Q_{\cdot,1}R_{1,1}, \ldots, Q_{\cdot,n}R_{n,n}$ are pairwise orthogonal. Thus the columns of $A$ are pairwise orthogonal.

    \bigskip
    Assume that the columns of $A$ are pairwise orthogonal.

    If $A$ is not invertible, then the columns of $A$ are linearly dependent. Moreover, the columns of $A$ are pairwise orthogonal so there must be at least one column consisting of all $0$ (because otherwise, they are linearly independent). Therefore $\abs{\det A} = 0 = \prod^{n}_{k=1}\norm{v_{k}}$.

    If $A$ is invertible, all columns of $A$ are nonzero vectors of $\mathbb{F}^{n}$, so
    \[
        A = \prod^{n}_{k=1}\norm{v_{k}}\begin{pmatrix}
            A_{1,1}/\norm{v_{1}} & \cdots & A_{1,n}/\norm{v_{n}} \\
            \vdots               &        & \vdots               \\
            A_{n,1}/\norm{v_{1}} & \cdots & A_{n,n}/\norm{v_{n}}
        \end{pmatrix}
    \]

    where the columns of
    \[
        \begin{pmatrix}
            A_{1,1}/\norm{v_{1}} & \cdots & A_{1,n}/\norm{v_{n}} \\
            \vdots               &        & \vdots               \\
            A_{n,1}/\norm{v_{1}} & \cdots & A_{n,n}/\norm{v_{n}}
        \end{pmatrix}
    \]

    constitute an orthonormal basis of $\mathbb{F}^{n}$, which means this matrix is unitary. Therefore $\abs{\det A} = \prod^{n}_{k=1}\norm{v_{k}}$.

    Thus the Hadamard's inequality is an equality if and only if the columns of $A$ are pairwise orthogonal.
\end{proof}
\newpage

% chapter9:sectionC:exercise19
\begin{exercise}\label{chapter9:sectionC:exercise19}
    Suppose $V$ is an inner product space, $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$, and $T\in\lmap{V}$ is a positive operator.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $\det T\leq \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}$.
        \item Prove that if $T$ is invertible, then the inequality in (a) is an equality if and only if $e_{k}$ is an eigenvector of $T$ for each $k = 1,\ldots,n$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $A$ be the matrix of $\sqrt{T}$ with respect to the basis $e_{1}, \ldots, e_{n}$ of $V$, then the $k$th column $v_{k}$ of $A$ is the coordinates of $\sqrt{T}e_{k}$ with respect to $e_{1}, \ldots, e_{n}$. By the Pythagorean theorem and the Parseval's identity (we use the standard inner products on $\mathbb{F}^{n}$ and the inner product on $V$),
              \begin{align*}
                  \norm{v_{k}}^{2} = \abs{A_{1,k}}^{2} + \cdots + \abs{A_{n,k}}^{2} = \abs{\innerprod{\sqrt{T}e_{k}, e_{1}}}^{2} + \cdots +\abs{\innerprod{\sqrt{T}e_{k}, e_{n}}}^{2} = \norm{\sqrt{T}e_{k}}^{2}.
              \end{align*}

              for every $k\in\{1,\ldots,n\}$.
              \begin{align*}
                  \det T & = {(\det \sqrt{T})}^{2}                                   & \text{(Exercise~\ref{chapter9:sectionC:exercise14})} \\
                         & \leq \prod^{n}_{k=1}\norm{v_{k}}^{2}                      & \text{(Hadamard's inequality)}                       \\
                         & = \prod^{n}_{k=1}\norm{v_{k}}^{2}                                                                                \\
                         & = \prod^{n}_{k=1}\innerprod{\sqrt{T}e_{k}, \sqrt{T}e_{k}}                                                        \\
                         & = \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}                & \text{($T$ is a positive operator)}
              \end{align*}

              Thus $\det T\leq \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}$.
        \item According to part (a) and the equality condition of Hadamard's inequality (Exercise~\ref{chapter9:sectionC:exercise18}), when $T$ is invertible, the inequality in (a) is an equality if and only if the columns of $\mathcal{M}(\sqrt{T}, (e_{1}, \ldots, e_{n}))$ are pairwise orthogonal.


              Suppose (a) is an equality and $T$ is invertible.

              The columns of $\mathcal{M}(\sqrt{T}, (e_{1}, \ldots, e_{n}))$ are pairwise orthogonal if and only if $\innerprod{\sqrt{T}e_{k}, \sqrt{T}e_{j}} = 0$ for all $j\ne k$. Equivalently, $\innerprod{Te_{k}, e_{j}} = 0$ for all $j\ne k$, this means the matrix of $T$ with respect to $e_{1}, \ldots, e_{n}$ is a diagonal matrix. Therefore $e_{1}, \ldots, e_{n}$ are eigenvectors of $T$.

              Suppose $e_{1}, \ldots, e_{n}$ are eigenvectors of $T$. Let $\lambda_{k}$ be the eigenvalue of $T$ corresponding to $e_{k}$ for every $k\in\{1,\ldots,n\}$, then
              \[
                  \det T = \prod^{n}_{k=1}\lambda_{k} = \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}.
              \]

              Thus if $T$ is invertible, the inequality in (a) is an equality if and only if $e_{1}, \ldots, e_{n}$ are eigenvectors of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter9:sectionC:exercise20
\begin{exercise}\label{chapter9:sectionC:exercise20}
    Suppose $A$ is an $n$-by-$n$ matrix, and suppose $c$ is such that $\abs{A_{j,k}}\leq c$ for all $j, k\in\{1,\ldots,n\}$. Prove that
    \[
        \abs{\det A}\leq c^{n}n^{n/2}.
    \]
\end{exercise}

\begin{quote}
    The formula for the determinant of a matrix (9.46) shows that $\abs{\det A}\leq c^{n}n{!}$. However, the estimate given by this exercise is much better. For example, if $c = 1$ and $n = 100$, then $c^{n}n! \approx 10^{158}$, but the estimate given by this exercise is the much smaller number $10^{100}$. If $n$ is an integer power of $2$, then the inequality above is sharp and cannot be improved.
\end{quote}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}\in\mathbb{F}^{n}$ be the columns of $A$. By the Hadamard's inequality
    \[
        \abs{\det A}\leq \prod^{n}_{k=1}\norm{v_{k}}.
    \]

    For every $k\in\{1,\ldots,n\}$
    \[
        \norm{v_{k}}^{2} = \abs{A_{1,k}}^{2} + \cdots + \abs{A_{n,k}}^{2} \leq nc^{2}.
    \]

    Therefore
    \[
        \abs{\det A}\leq {(c\sqrt{n})}^{n} = c^{n}n^{n/2}.
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise21
\begin{exercise}\label{chapter9:sectionC:exercise21}
    Suppose $n$ is a positive integer and $\delta: \mathbb{C}^{n,n}\to \mathbb{C}$ is a function such that
    \[
        \delta(AB) = \delta(A)\cdot\delta(B)
    \]

    for all $A, B\in\mathbb{C}^{n,n}$ and $\delta(A)$ equals the product of the diagonal entries of $A$ for each diagonal matrix $A\in\mathbb{C}^{n,n}$. Prove that
    \[
        \delta(A) = \det A
    \]

    for all $A\in\mathbb{C}^{n,n}$.
\end{exercise}

\begin{quote}
    Recall that $\mathbb{C}^{n,n}$ denotes set of $n$-by-$n$ matrices with entries in $\mathbb{C}$. This exercise shows that the determinant is the unique function defined on square matrices that is multiplicative and has the desired behavior on diagonal matrices. This result is analogous to Exercise~\ref{chapter8:sectionD:exercise10}, which shows that the trace is uniquely determined by its algebraic properties.
\end{quote}

\begin{proof}
    If $A$ is invertible, then $\delta(A)\delta(A^{-1}) = \delta(I) = 1\ne 0$, so $\delta(A)\ne 0$.

    If $A$ is not invertible, then there exists $v_{1}\in\mathbb{C}^{n,1}$ and $v_{1}\ne 0$ such that $Av_{1} = 0$. Extend $v_{1}$ to a basis $v_{1}, \ldots, v_{n}$ of $\mathbb{C}^{n,1}$, let $B$ be a matrix whose $k$th column is $v_{k}$, then
    \[
        AB = \begin{pmatrix}
            0      & *      & \cdots & *      \\
            0      & *      & \cdots & *      \\
            \vdots & \vdots &        & \vdots \\
            0      & *      & \cdots & *
        \end{pmatrix}
        =
        \begin{pmatrix}
            0      & *      & \cdots & *      \\
            0      & *      & \cdots & *      \\
            \vdots & \vdots &        & \vdots \\
            0      & *      & \cdots & *
        \end{pmatrix}
        \begin{pmatrix}
            0      & 0      & \cdots & 0      \\
            0      & 1      & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & 1
        \end{pmatrix}.
    \]

    Therefore $\delta(A)\delta(B) = \delta(AB) = 0$. Because $\delta(B)\ne 0$ (since $B$ is invertible), it follows that $\delta(A) = 0$. Hence $\delta(A) = 0$ if $A$ is not invertible.

    By the Schur's decomposition theorem, for each $A\in\mathbb{C}^{n,n}$ there exists an unitary matrix $Q$ such that $Q^{*}AQ$ is an upper-triangular matrix.
    \[
        \delta(Q^{*}AQ) = \delta(Q^{*})\delta(A)\delta(Q) = \delta(Q^{*})\delta(Q)\delta(A) = \delta(Q^{*}Q)\delta(A) = \delta(I)\delta(A) = \delta(A).
    \]

    Suppose $A$ is not invertible, then $\delta(A) = 0 = \det A$.

    Suppose $A$ is invertible, then all entries on the diagonal of $Q^{*}AQ$ are nonzero. Let $U = Q^{*}AQ$ and choose nonzero complex numbers $\lambda_{1}, \ldots, \lambda_{n}$ such that $U_{1,1}/\lambda_{1}, \ldots, U_{n,n}/\lambda_{n}$ are pairwise distinct, then
    \[
        \begin{pmatrix}
            U_{1,1} & U_{1,2} & \cdots & U_{1,n} \\
            0       & U_{2,2} & \cdots & U_{2,n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & 0       & \cdots & U_{n,n}
        \end{pmatrix} =
        \begin{pmatrix}
            U_{1,1}/\lambda_{1} & U_{1,2}/\lambda_{2} & \cdots & U_{1,n}/\lambda_{n} \\
            0                   & U_{2,2}/\lambda_{2} & \cdots & U_{2,n}/\lambda_{n} \\
            \vdots              & \vdots              &        & \vdots              \\
            0                   & 0                   & \cdots & U_{n,n}/\lambda_{n}
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_{1} & 0           & \cdots & 0           \\
            0           & \lambda_{2} & \cdots & 0           \\
            \vdots      & \vdots      &        & \vdots      \\
            0           & 0           & \cdots & \lambda_{n}
        \end{pmatrix}.
    \]

    The upper-triangular matrix
    \[
        W = \begin{pmatrix}
            U_{1,1}/\lambda_{1} & U_{1,2}/\lambda_{2} & \cdots & U_{1,n}/\lambda_{n} \\
            0                   & U_{2,2}/\lambda_{2} & \cdots & U_{2,n}/\lambda_{n} \\
            \vdots              & \vdots              &        & \vdots              \\
            0                   & 0                   & \cdots & U_{n,n}/\lambda_{n}
        \end{pmatrix}
    \]

    has distinct entries on the diagonal, so it has distinct $n$ eigenvalues (which are precisely the entries on the diagonal). Therefore it is diagonalizable, hence there exists an invertible matrix $C$ such that $C^{-1}WC$ is a diagonal matrix.
    \[
        \delta(C^{-1}WC) = \delta(C^{-1})\delta(W)\delta(C) = \delta(C^{-1})\delta(C)\delta(W) = \delta(C^{-1}C)\delta(W) = \delta(W).
    \]

    Hence $\delta(U) = \delta(W)\lambda_{1}\cdots\lambda_{n} = U_{1,1}\cdots U_{n,n}$. Moreover, $U_{1,1}, \ldots, U_{n,n}$ are also eigenvalues of $U$, each appears as many times as its multiplicity, so $\det U =  U_{1,1}\cdots U_{n,n}$. Therefore
    \[
        \delta(A) = \delta(Q^{*}AQ) = \delta(U) = U_{1,1}\cdots U_{n,n} = \det U = \det (Q^{*}AQ) = \det A.
    \]

    Thus for all $A\in\mathbb{C}^{n,n}$, $\delta(A) = \det A$.
\end{proof}
\newpage

\section{Tensor Products}

% chapter9:sectionD:exercise1
\begin{exercise}\label{chapter9:sectionD:exercise1}
    Suppose $v\in V$ and $w\in W$. Prove that $v\otimes w = 0$ if and only if $v = 0$ or $w = 0$.
\end{exercise}

\begin{proof}
    Suppose $v = 0$ or $w = 0$.

    If $v = 0$ then for all $\varphi\in V'$, for all $\tau\in W'$,
    \[
        (v\otimes w)(\varphi, \tau) = \varphi(v)\tau(w) = 0\tau(w) = 0.
    \]

    Therefore $v\otimes w = 0$.

    If $w = 0$ then for all $\varphi\in V'$, for all $\tau\in W'$,
    \[
        (v\otimes w)(\varphi, \tau) = \varphi(v)\tau(w) = \varphi(v)0 = 0.
    \]

    Therefore $v\otimes w = 0$.

    \bigskip
    Suppose $v\otimes w = 0$.

    If $v\ne 0$, then there exists $\varphi\in V'$ such that $\varphi(v)\ne 0$. For all $\tau\in W'$,
    \[
        \varphi(v)\tau(w) = (v\otimes w)(\varphi, \tau) = 0
    \]

    because $v\otimes w = 0$. So $\varphi(v)\tau(w) = 0$. Since $\varphi(v) = 0$ then $\tau(w) = 0$ (for all $\tau\in W'$). Therefore $w = 0$.

    Hence $v = 0$ or $w = 0$.
\end{proof}
\newpage

% chapter9:sectionD:exercise2
\begin{exercise}\label{chapter9:sectionD:exercise2}
    Give an example of six distinct vectors $v_{1}, v_{2}, v_{3}, w_{1}, w_{2}, w_{3}$ in $\mathbb{R}^{3}$ such that
    \[
        v_{1}\otimes w_{1} + v_{2}\otimes w_{2} + v_{3}\otimes w_{3} = 0
    \]

    but none of $v_{1}\otimes w_{1}, v_{2}\otimes w_{2}, v_{3}\otimes w_{3}$ is a scalar multiple of another element of this list.
\end{exercise}

\begin{proof}
    Let $e_{1} = (1, 0, 0), e_{2} = (0, 1, 0), e_{3} = (0, 0, 1)$.

    I choose
    \[
        \begin{split}
            v_{1} = (1, 0, 0)\qquad w_{1} = (1, 0, 0) \\
            v_{2} = (0, 1, 0)\qquad w_{2} = (1, 0, 0) \\
            v_{3} = (-1, -1, 0)\qquad w_{3} = (1, 0, 0)
        \end{split}
    \]

    so
    \begin{align*}
        v_{1}\otimes w_{1} & = e_{1}\otimes e_{1}          \\
        v_{2}\otimes w_{2} & = e_{2}\otimes e_{1}          \\
        v_{3}\otimes w_{3} & = (-e_{1}-e_{2})\otimes e_{1}
    \end{align*}

    By Exercise~\ref{chapter9:sectionD:exercise1}
    \begin{align*}
        a_{1}(v_{1}\otimes w_{1}) + a_{2}(v_{2}\otimes w_{2}) = 0 & \Longleftrightarrow (a_{1}e_{1} + a_{2}e_{2})\otimes e_{1} = 0          \\
                                                                  & \Longleftrightarrow a_{1}e_{1} + a_{2}e_{2} = 0                         \\
                                                                  & \Longleftrightarrow a_{1} = a_{2} = 0,                                  \\
        a_{2}(v_{2}\otimes w_{2}) + a_{3}(v_{3}\otimes w_{3}) = 0 & \Longleftrightarrow (a_{2}e_{2} + a_{3}(-e_{1}-e_{2}))\otimes e_{1} = 0 \\
                                                                  & \Longleftrightarrow a_{2}e_{2} + a_{3}(-e_{1}-e_{2}) = 0                \\
                                                                  & \Longleftrightarrow a_{2} = a_{3} = 0,                                  \\
        a_{3}(v_{3}\otimes w_{3}) + a_{1}(v_{1}\otimes w_{1}) = 0 & \Longleftrightarrow (a_{3}(-e_{1}-e_{2}) + a_{1}e_{1})\otimes e_{1} = 0 \\
                                                                  & \Longleftrightarrow a_{3}(-e_{1}-e_{2}) + a_{1}e_{1} = 0                \\
                                                                  & \Longleftrightarrow a_{3} = a_{1} = 0,
    \end{align*}

    so $v_{1}\times w_{1}, v_{2}\otimes w_{2}, v_{3}\otimes w_{3}$ are pairwise linearly independent, which implies none of them is a scalar multiple of another. However,
    \[
        v_{1}\otimes w_{1} + v_{2}\otimes w_{2} + v_{3}\otimes w_{3} = (e_{1} + e_{2} - e_{1} - e_{2})\otimes e_{1} = 0\otimes e_{1} = 0.
    \]
\end{proof}
\newpage

% chapter9:sectionD:exercise3
\begin{exercise}\label{chapter9:sectionD:exercise3}
    Suppose that $v_{1}, \ldots, v_{m}$ is a linearly independent list in $V$. Suppose also that $w_{1}, \ldots, w_{m}$ is a list in $W$ such that
    \[
        v_{1}\otimes w_{1} + \cdots + v_{m}\otimes w_{m} = 0.
    \]

    Prove that $w_{1} = \cdots = w_{m} = 0$.
\end{exercise}

\begin{proof}
    Let $\widehat{V} = \operatorname{span}(v_{1}, \ldots, v_{m})$ and $\varphi_{1}, \ldots, \varphi_{m}\in (\widehat{V})'$ be the dual basis of $v_{1}, \ldots, v_{m}$.

    For every $k\in\{1,\ldots,m\}$, for all $\tau\in W'$,
    \[
        0 = (v_{1}\otimes w_{1} + \cdots + v_{m}\otimes w_{m})(\varphi_{k}, \tau) = \varphi_{k}(v_{k})\tau(w_{k}) = \tau(w_{k})
    \]

    so $w_{k} = 0$ for every $k\in\{1,\ldots,m\}$.
\end{proof}
\newpage

% chapter9:sectionD:exercise4
\begin{exercise}\label{chapter9:sectionD:exercise4}
    Suppose $\dim V > 1$ and $\dim W > 1$. Prove that
    \[
        \{ v\otimes w: (v, w)\in V\times W \}
    \]

    is not a subspace of $V\otimes W$.
\end{exercise}

\begin{quote}
    This exercise implies that if $\dim V > 1$ and $\dim W > 1$, then
    \[
        \{ v\otimes w: (v, w)\in V\times W \} \ne V\otimes W.
    \]
\end{quote}

\begin{proof}
    Denote the set $\{ v\otimes w: (v, w)\in V\times W \}$ by $X$.

    Because $\dim V > 1$ and $\dim W > 1$, there exist $v_{1}, v_{2}\in V$ and $w_{1}, w_{2}\in W$ such that $v_{1}, v_{2}$ is a linearly independent list and $w_{1}, w_{2}$ is a linearly independent list.

    Assume $v_{1}\otimes w_{1} + v_{2}\otimes w_{2}\in X$, then there exist $v\in V$ and $w\in W$ such that $v_{1}\otimes w_{1} + v_{2}\otimes w_{2} = v\otimes w$.

    If $v$ is a linear combination of $v_{1}, v_{2}$ then there exist $\lambda_{1}, \lambda_{2}\in\mathbb{F}$ such that $v = \lambda_{1}v_{1} + \lambda_{2}v_{2}$.
    \begin{align*}
        0 & = v_{1}\otimes w_{1} + v_{2}\otimes w_{2} - (\lambda_{1}v_{1} + \lambda_{2}v_{2})\otimes w            \\
          & = v_{1}\otimes w_{1} + v_{2}\otimes w_{2} - \lambda_{1}(v_{1}\otimes w) - \lambda_{2}(v_{2}\otimes w) \\
          & = v_{1}\otimes w_{1} + v_{2}\otimes w_{2} - v_{1}\otimes (\lambda_{1}w) - v_{2}\otimes (\lambda_{2}w) \\
          & = v_{1}\otimes (w_{1} - \lambda_{1}w) + v_{2}\otimes (w_{2} - \lambda_{2}w).
    \end{align*}

    By Exercise~\ref{chapter9:sectionD:exercise3}, $w_{1} - \lambda_{1}w = w_{2} - \lambda_{2}w = 0$ so $w_{1} = \lambda_{1}w$, $w_{2} = \lambda_{2}w$. $\lambda_{1}$ and $\lambda_{2}$ are not equal to $0$ because $w_{1}, w_{2}$ is a linearly independent list. Therefore $\lambda_{1}^{-1}w_{1} - \lambda_{2}^{-1}w_{2} = 0$, which implies $w_{1}, w_{2}$ is a linearly dependent list. So $v$ is not a linear combination of $v_{1}, v_{2}$.

    Since $v$ is not a linear combination of $v_{1}, v_{2}$ and $v_{1}, v_{2}$ is a linearly independent list, then $v, v_{1}, v_{2}$ is a linearly independent list. By Exercise~\ref{chapter9:sectionD:exercise3} and $v_{1}\otimes w_{1} + v_{2}\otimes w_{2} - v\otimes w = 0$, we conclude that $w_{1} = w_{2} = w = 0$, which is a contradiction because $w_{1}, w_{2}$ is a linearly independent list.

    Hence the initial assumption is false, so $X$ is not closed under addition. Thus $X$ is not a subspace of $V\otimes W$.
\end{proof}
\newpage

% chapter9:sectionD:exercise5
\begin{exercise}\label{chapter9:sectionD:exercise5}
    Suppose $m$ and $n$ are positive integers. For $v\in\mathbb{F}^{m}$ and $w\in\mathbb{F}^{n}$, identify $v\otimes w$ with an $m$-by-$n$ matrix as in Example 9.76. With that identification, show that the set
    \[
        \{ v\otimes w: v\in\mathbb{F}^{m} \land w\in\mathbb{F}^{n} \}
    \]

    is the set of $m$-by-$n$ matrices (with entries in $\mathbb{F}$) that have rank at most one.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{m}$ be the standard basis of $\mathbb{F}^{m}$ and $f_{1}, \ldots, f_{n}$ be the standard basis of $\mathbb{F}^{n}$. There exist scalars $v_{1}, \ldots, v_{m}; w_{1}, \ldots, w_{n}$ such that
    \begin{align*}
        v & = v_{1}e_{1} + \cdots + v_{m}e_{m}, \\
        w & = w_{1}f_{1} + \cdots + w_{n}e_{n}.
    \end{align*}

    $v\otimes w$ is identified with the $m$-by-$n$ matrix $A$ where $A_{j,k} = v_{j}w_{k}$.
    \[
        A = \begin{pmatrix}
            v_{1}w_{1} & \cdots & v_{1}w_{n} \\
            \vdots     &        & \vdots     \\
            v_{m}w_{1} & \cdots & v_{m}w_{n}
        \end{pmatrix} =
        \begin{pmatrix}
            v_{1} \\ \vdots \\ v_{m}
        \end{pmatrix}
        \begin{pmatrix}
            w_{1} & \cdots & w_{n}
        \end{pmatrix}.
    \]

    By Exercise~\ref{chapter3:sectionB:exercise23}, the definition of (column) rank of a matrix, and the fact the column rank and row rank of a matrix are equal, we have
    \[
        \operatorname{rank} A \leq \operatorname{rank}\begin{pmatrix}
            v_{1} \\ \vdots \\ v_{m}
        \end{pmatrix} \leq 1.
    \]

    Now suppose $B$ is an $m$-by-$n$ matrix of rank at most one.

    If $\operatorname{rank} B = 0$, then $B = 0$, so $0\otimes 0$ is identified with $B$.

    Otherwise, $\operatorname{rank} B = 1$, then there exists a nonzero column of $B$ such that other columns are multiple of this nonzero column. Suppose the $k$ column $B_{\cdot,k}$ of $B$ is nonzero and $B_{\cdot,i} = \lambda_{i}B_{\cdot,k}$ for every $i\in\{1,\ldots,n\}$ where $\lambda_{k} = 1$. Hence
    \[
        B = \begin{pmatrix}B_{1,k} \\ \vdots \\ B_{m,k}\end{pmatrix}\begin{pmatrix}\lambda_{1} & \cdots & \lambda_{n}\end{pmatrix}
    \]

    which means $B = (B_{1,k}, \ldots, B_{m,k})\otimes (\lambda_{1}, \ldots, \lambda_{n})$.

    Thus the set $\{ v\otimes w: v\in\mathbb{F}^{m}\land w\in\mathbb{F}^{n} \}$ is (identified with) the set of $m$-by-$n$ matrices (with entries in $\mathbb{F}$) that have rank at most one.
\end{proof}
\newpage

% chapter9:sectionD:exercise6
\begin{exercise}\label{chapter9:sectionD:exercise6}
    Suppose $m$ and $n$ are positive integers. Give a description, analogous to Exercise~\ref{chapter9:sectionD:exercise5}, of the set of $m$-by-$n$ matrices (with entries in $\mathbb{F}$) that have rank at most two.
\end{exercise}

\begin{proof}
    Let $m, n$ be positive integers greater than $1$. For $v\in\mathbb{F}^{m}$ and $w\in\mathbb{F}^{n}$, identify $v\otimes w$ with the $m$-by-$n$ matrix
    \[
        \begin{pmatrix}
            v_{1} \\ \vdots \\ v_{m}
        \end{pmatrix}
        \begin{pmatrix}
            w_{1} & \cdots & w_{n}
        \end{pmatrix}.
    \]

    The set $\{ v_{1}\otimes w_{1} + v_{2}\otimes w_{2}: v_{1}, v_{2}\in \mathbb{F}^{m}\land w_{1}, w_{2}\in \mathbb{F}^{n} \}$ is the set of matrices that have rank at most two.

    Let $e_{1}, \ldots, e_{m}$ be the standard basis of $\mathbb{F}^{m}$ and $f_{1}, \ldots, f_{n}$ be the standard basis of $\mathbb{F}^{n}$.

    Let $v_{1}, v_{2}$ be vectors in $\mathbb{F}^{m}$ and $w_{1}, w_{2}$ be vectors in $\mathbb{F}^{n}$ and
    \begin{align*}
        v_{1} & = v_{1}^{1}e_{1} + \cdots + v_{1}^{m}e_{m}  \\
        v_{2} & = v_{2}^{1}e_{1} + \cdots + v_{2}^{m}e_{m}  \\
        w_{1} & = w_{1}^{1}f_{1} + \cdots + w_{1}^{n}f_{n}  \\
        w_{2} & = w_{2}^{1}f_{1} + \cdots + w_{2}^{n}f_{n}.
    \end{align*}
    \begin{multline*}
        \operatorname{rank}\left(\begin{pmatrix}v_{1}^{1} \\ \vdots \\ v_{1}^{m}\end{pmatrix}\begin{pmatrix}w_{1}^{1} & \cdots & w_{1}^{n}\end{pmatrix} + \begin{pmatrix}v_{2}^{1} \\ \vdots \\ v_{2}^{m}\end{pmatrix}\begin{pmatrix}w_{2}^{1} & \cdots & w_{2}^{n}\end{pmatrix}\right) \\
        \leq \operatorname{rank}\begin{pmatrix}v_{1}^{1} \\ \vdots \\ v_{1}^{m}\end{pmatrix}\begin{pmatrix}w_{1}^{1} & \cdots & w_{1}^{n}\end{pmatrix} + \operatorname{rank}\begin{pmatrix}v_{2}^{1} \\ \vdots \\ v_{2}^{m}\end{pmatrix}\begin{pmatrix}w_{2}^{1} & \cdots & w_{2}^{n}\end{pmatrix} \\
        \leq 1 + 1 = 2.
    \end{multline*}

    Therefore the set $\{ v_{1}\otimes w_{1} + v_{2}\otimes w_{2}: v_{1}, v_{2}\in \mathbb{F}^{m}\land w_{1}, w_{2}\in \mathbb{F}^{n} \}$ is a subset of the set of matrices that have rank at most two.

    Let $A$ be a set of rank at most two.

    If $\operatorname{rank}A = 0$ then $A = 0$. Therefore $0\otimes 0$ is identified with $A$.

    If $\operatorname{rank}A = 1$ then there exist $v\in\mathbb{F}^{m}$ and $w\in\mathbb{F}^{n}$ such that $v\otimes w + 0\otimes 0$ is identified with $A$ (due to Exercise~\ref{chapter9:sectionD:exercise5}).

    If $\operatorname{rank}A = 2$, then there exist two columns $A_{\cdot,k_{1}}, A_{\cdot,k_{2}}$ of $A$ such that other columns of $A$ is a linear combination of these two. Assume $A_{\cdot,k} = \lambda_{k}^{1}A_{\cdot,k_{1}} + \lambda_{k}^{2}A_{\cdot,k_{2}}$ for every $k\in\{1,\ldots,n\}$, where $\lambda_{k_{1}}^{1} = 1, \lambda_{k_{1}}^{2} = 0$ and $\lambda_{k_{2}}^{1} = 0, \lambda_{k_{2}}^{2} = 1$. Hence
    \[
        A = \begin{pmatrix}A_{1,k_{1}} \\ \vdots \\ A_{m,k_{1}}\end{pmatrix}\begin{pmatrix}\lambda_{1}^{1} & \cdots \lambda_{n}^{1}\end{pmatrix} + \begin{pmatrix}A_{1,k_{2}} \\ \vdots \\ A_{m,k_{2}}\end{pmatrix}\begin{pmatrix}\lambda_{1}^{2} & \cdots \lambda_{n}^{2}\end{pmatrix}
    \]

    so
    \[
        (A_{1,k_{1}}, \ldots, A_{m,k_{1}})\otimes (\lambda_{1}^{1}, \ldots, \lambda_{n}^{1}) + (A_{1,k_{2}}, \ldots, A_{m,k_{2}})\otimes (\lambda_{1}^{2}, \ldots, \lambda_{n}^{2})
    \]

    is identified with $A$. Therefore the set of $m$-by-$n$ matrices with entries in $\mathbb{F}$ that have rank at most two is a subset of $\{ v_{1}\otimes w_{1} + v_{2}\otimes w_{2}: v_{1}, v_{2}\in \mathbb{F}^{m}\land w_{1}, w_{2}\in \mathbb{F}^{n} \}$.

    Thus the set $\{ v_{1}\otimes w_{1} + v_{2}\otimes w_{2}: v_{1}, v_{2}\in\mathbb{F}^{m}\land w_{1}, w_{2}\in\mathbb{F}^{n} \}$ is the set of $m$-by-$n$ matrices with entries in $\mathbb{F}$ that have rank at most two.
\end{proof}
\newpage

% chapter9:sectionD:exercise7
\begin{exercise}\label{chapter9:sectionD:exercise7}
    Suppose $\dim V > 2$ and $\dim W > 2$. Prove that
    \[
        \{ v_{1}\otimes w_{1} + v_{2}\otimes w_{2}: v_{1}, v_{2}\in V \land w_{1}, w_{2}\in W \}\ne V\otimes W.
    \]
\end{exercise}

\begin{proof}
    Denote $\{ v_{1}\otimes w_{1} + v_{2}\otimes w_{2}: v_{1}, v_{2}\in V \land w_{1}, w_{2}\in W \}$ by $X$.

    Let $m = \dim V$ and $n = \dim W$.

    Let $e_{1}, \ldots, e_{m}$ be a basis of $V$, $f_{1}, \ldots, f_{n}$ be a basis of $W$.

    Define $f: V\otimes W\to \mathbb{F}^{m,n}$ as follows: $f(\beta)$ is the $m$-by-$n$ matrix $A$ where $A_{j,k} = \beta(e_{j}, f_{k})$. $f$ is an isomorphism from $V\otimes W$ onto $\mathbb{F}^{m,n}$.

    Assume $X$ is a subspace of $V\otimes W$, then the range of $X$ under $f$ is a subspace of $\mathbb{F}^{m,n}$. The range of $X$ under $f$ is the set of matrices that have rank at most two, according to Exercise~\ref{chapter9:sectionD:exercise6}. On the other hand, because $m > 2$ and $n > 2$, there are matrices that have rank at most two but their sum has rank greater than two, for example
    \[
        \begin{pmatrix}
            1      & 0      & \cdots & 0      \\
            0      & 1      & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & 0
        \end{pmatrix},
        \begin{pmatrix}
            0      & 0      & \cdots & 0      \\
            0      & 0      & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & 1
        \end{pmatrix}
    \]

    where the first matrix has rank two, where only the entries at 1st row, 1st column and 2nd row, 2nd column are nonzero; the second matrix has rank two, where only the entries at $m$th row, $n$th column is nonzero. The sum of these two matrices is a matrix of rank three. Therefore the range of $X$ under $f$ is not a subspace of $\mathbb{F}^{m,n}$, which is a contradiction.

    Hence $X$ is not a subspace of $V\otimes W$. Thus $X\ne V\otimes W$.
\end{proof}
\newpage

% chapter9:sectionD:exercise8
\begin{exercise}\label{chapter9:sectionD:exercise8}
    Suppose $v_{1}, \ldots, v_{m}\in V$ and $w_{1}, \ldots, w_{m}\in W$ are such that
    \[
        v_{1}\otimes w_{1} + \cdots + v_{m}\otimes w_{m} = 0.
    \]

    Suppose that $U$ is a vector space and $\Gamma: V\times W\to U$ is a bilinear map. Show that
    \[
        \Gamma(v_{1}, w_{1}) + \cdots + \Gamma(v_{m}, w_{m}) = 0.
    \]
\end{exercise}

\begin{proof}
    $\Gamma: V\times W\to U$ is a bilinear map so there exists a unique linear map $\widehat{\Gamma}: V\otimes W\to U$ such that
    \[
        \widehat{\Gamma}(v\otimes w) = \Gamma(v, w).
    \]

    Therefore
    \begin{align*}
        \Gamma(v_{1}, w_{1}) + \cdots + \Gamma(v_{m}, w_{m}) & = \widehat{\Gamma}(v_{1}\otimes w_{1}) + \cdots + \widehat{\Gamma}(v_{m}\otimes w_{m}) \\
                                                             & = \widehat{\Gamma}(v_{1}\otimes w_{1} + \cdots + v_{m}\otimes w_{m})                   \\
                                                             & = \widehat{\Gamma}(0)                                                                  \\
                                                             & = 0.\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter9:sectionD:exercise9
\begin{exercise}\label{chapter9:sectionD:exercise9}
    Suppose $S\in\lmap{V}$ and $T\in\lmap{W}$. Prove that there exists a unique operator on $V\otimes W$ that takes $v\otimes w$ to $Sv\otimes Tw$ for all $v\in V$ and $w\in W$.
\end{exercise}

\begin{quote}
    In an abuse of notation, the operator on $V \otimes W$ given by this exercise is often called $S\otimes T$.
\end{quote}

\begin{proof}
    Let $F$ be a map from $V\times W$ to $V\otimes W$ defined by
    \[
        F(v, w) = Sv\otimes Tw
    \]

    then $F$ is a bilinear map on $V\times W$. So there exists a unique linear map from $V\otimes W$ to $V\otimes W$ such that
    \[
        \widehat{F}(v\otimes w) = F(v, w).
    \]

    So $\widehat{F}(v\otimes w) = F(v, w) = Sv\otimes Tw$ for every $v\in V, w\in W$.

    Assume that $\widehat{F}$ and $\widehat{G}$ are operators on $V\otimes W$ that takes $v\otimes w$ to $Sv\otimes Tw$ for all $v\in V$ and $w\in W$.

    Let $e_{1}, \ldots, e_{m}$ be a basis of $V$ and $f_{1}, \ldots, f_{n}$ be a basis of $W$, then ${\{ e_{j}\otimes f_{k} \}}_{j=1,\ldots,m;k=1,\ldots,n}$ is a basis of $V\otimes W$. The image of this basis are the same under $\widehat{F}$ and $\widehat{G}$ because $\widehat{F}(e_{j}\otimes f_{k}) = Se_{j}\otimes Tf_{k} = \widehat{G}(e_{j}, f_{k})$. Therefore $\widehat{F} = \widehat{G}$.

    Thus there exists a unique operator on $V\otimes W$ that takes $v\otimes w$ to $Sv\otimes Tw$ for all $v\in V$ and $w\in W$.
\end{proof}
\newpage

% chapter9:sectionD:exercise10
\begin{exercise}\label{chapter9:sectionD:exercise10}
    Suppose $S\in\lmap{V}$ and $T\in\lmap{W}$. Prove that $S\otimes T$ is an invertible operator on $V\otimes W$ if and only if both $S$ and $T$ are invertible operators. Also, prove that if both $S$ and $T$ are invertible operators, then ${(S\otimes T)}^{-1} = S^{-1}\otimes T^{-1}$, where we are using the notation from the comment after Exercise~\ref{chapter9:sectionD:exercise9}.
\end{exercise}

\begin{quote}[Additional notes]
    In this problem, we need $\dim V > 0$ and $\dim W > 0$.
\end{quote}

\begin{proof}
    Let $e_{1}, \ldots, e_{m}$ be a basis of $V$, $f_{1}, \ldots, f_{n}$ be a basis of $W$, then ${\{e_{j}\otimes f_{k}\}}_{j=1,\ldots,m;k=1,\ldots,n}$ is a basis of $V\otimes W$.

    $(\Rightarrow)$ $S\otimes T$ is an invertible operator on $V\otimes W$.

    Suppose $Sv = 0$, then for every $w\in W$, $(S\otimes T)(v\otimes w) = Sv\otimes Tw = 0\otimes Tw = 0$. Because $S\otimes T$ is invertible, $v\otimes w = 0$ for every $w\in W$. By Exercise~\ref{chapter9:sectionD:exercise1}, $v = 0$. Hence $S$ is injective, which implies $S$ is invertible, because $V$ is a finite-dimensional vector space.

    Analogously, $T$ is also invertible. Thus $S$ and $T$ are invertible operators.

    \bigskip
    $(\Leftarrow)$ $S$ and $T$ are invertible operators.
    \begin{align*}
        (S\otimes T)(S^{-1}\otimes T^{-1})\left(\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}e_{j}\otimes f_{k}\right) & = (S\otimes T)\left(\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}S^{-1}e_{j}\otimes T^{-1}f_{k}\right) \\
                                                                                                             & = \sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k} (SS^{-1})e_{j}\otimes (TT^{-1})f_{k}                   \\
                                                                                                             & = \sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}e_{j}\otimes f_{k},                                     \\
        (S^{-1}\otimes T^{-1})(S\otimes T)\left(\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}e_{j}\otimes f_{k}\right) & = (S^{-1}\otimes T^{-1})\left(\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}Se_{j}\otimes Tf_{k}\right) \\
                                                                                                             & = \sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(S^{-1}S)e_{j}\otimes (T^{-1}T)f_{k}                    \\
                                                                                                             & = \sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}e_{j}\otimes f_{k}.
    \end{align*}

    So $(S\otimes T)(S^{-1}\otimes T^{-1}) = (S^{-1}\otimes T^{-1})(S\otimes T) = I$. Hence $S\otimes T$ is invertible and ${(S\otimes T)}^{-1} = S^{-1}\otimes T^{-1}$.
\end{proof}
\newpage

% chapter9:sectionD:exercise11
\begin{exercise}\label{chapter9:sectionD:exercise11}
    Suppose $V$ and $W$ are inner product spaces. Prove that if $S\in\lmap{V}$ and $T\in\lmap{W}$, then ${(S\otimes T)}^{*} = S^{*}\otimes T^{*}$, where we are using the notation from the comment after Exercise~\ref{chapter9:sectionD:exercise11}.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{m}$ be a basis of $V$ and $f_{1}, \ldots, f_{n}$ be a basis of $W$, then
    \[
        {\{e_{j}\otimes f_{k}\}}_{j=1,\ldots,m; k=1,\ldots,n}
    \]

    is a basis of $V\otimes W$.

    For every
    \[
        \sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(e_{j}\otimes f_{k})\quad\text{and}\quad\sum^{n}_{s=1}\sum^{m}_{r=1}b_{r,s}(e_{r}\otimes f_{s})
    \]

    in $V\otimes W$,
    \begingroup
    \allowdisplaybreaks{}
    \begin{align*}
          & \innerprod{\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(e_{j}\otimes f_{k}), {(S\otimes T)}^{*}\left(\sum^{n}_{s=1}\sum^{m}_{r=1}b_{r,s}(e_{r}\otimes f_{s})\right)}    \\
        = & \innerprod{(S\otimes T)\left(\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(e_{j}\otimes f_{k})\right), \sum^{n}_{s=1}\sum^{m}_{r=1}b_{r,s}(e_{r}\otimes f_{s})}          \\
        = & \innerprod{\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(Se_{j}\otimes Tf_{k}), \sum^{n}_{s=1}\sum^{m}_{r=1}b_{r,s}(e_{r}\otimes f_{s})}                                 \\
        = & \sum^{n}_{k=1}\sum^{m}_{j=1}\sum^{n}_{s=1}\sum^{m}_{r=1}a_{j,k}\conj{b_{r,s}}\innerprod{Se_{j}\otimes Tf_{k}, e_{r}\otimes f_{s}}                              \\
        = & \sum^{n}_{k=1}\sum^{m}_{j=1}\sum^{n}_{s=1}\sum^{m}_{r=1}a_{j,k}\conj{b_{r,s}}\innerprod{Se_{j}, e_{r}}\innerprod{Tf_{k}, f_{s}}                                \\
        = & \sum^{n}_{k=1}\sum^{m}_{j=1}\sum^{n}_{s=1}\sum^{m}_{r=1}a_{j,k}\conj{b_{r,s}}\innerprod{e_{j}, S^{*}e_{r}}\innerprod{f_{k}, T^{*}f_{s}}                        \\
        = & \sum^{n}_{k=1}\sum^{m}_{j=1}\sum^{n}_{s=1}\sum^{m}_{r=1}\innerprod{a_{j,k}e_{j}, S^{*}e_{r}}\innerprod{f_{k}, b_{r,s}T^{*}f_{s}}                               \\
        = & \sum^{n}_{k=1}\sum^{m}_{j=1}\sum^{n}_{s=1}\sum^{m}_{r=1}\innerprod{a_{j,k}e_{j}\otimes f_{k}, S^{*}e_{r}\otimes T^{*}f_{s}}                                    \\
        = & \innerprod{\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(e_{j}\otimes f_{k}), \sum^{n}_{s=1}\sum^{m}_{r=1}b_{r,s}(S^{*}e_{r}\otimes T^{*}f_{s})}                         \\
        = & \innerprod{\sum^{n}_{k=1}\sum^{m}_{j=1}a_{j,k}(e_{j}\otimes f_{k}), (S^{*}\otimes T^{*})\left(\sum^{n}_{s=1}\sum^{m}_{r=1}b_{r,s}(e_{r}\otimes f_{s})\right)}.
    \end{align*}
    \endgroup

    The last equality shows that ${(S\otimes T)}^{*} = S^{*}\otimes T^{*}$.
\end{proof}
\newpage

% chapter9:sectionD:exercise12
\begin{exercise}\label{chapter9:sectionD:exercise12}
    Suppose that $V_{1}, \ldots, V_{m}$ are finite-dimensional inner product spaces. Prove that there is a unique inner product on $V_{1}\otimes \cdots\otimes V_{m}$ such that
    \[
        \innerprod{v_{1}\otimes \cdots\otimes v_{m}, u_{1}\otimes \cdots\otimes u_{m}} = \innerprod{v_{1}, u_{1}}\cdots\innerprod{v_{m}, u_{m}}
    \]

    for all $(v_{1}, \ldots, v_{m})$ and $(u_{1}, \ldots, u_{m})$ in $V_{1}\times\cdots\times V_{m}$.
\end{exercise}

\begin{quote}
    Note that the equation above implies that
    \[
        \norm{v_{1}\otimes \cdots\otimes v_{m}} = \norm{v_{1}}\times\cdots\times\norm{v_{m}}
    \]

    for all $(v_{1}, \ldots, v_{m})\in V_{1}\times\cdots\times V_{m}$.
\end{quote}

\begin{proof}
    Let
    \begin{itemize}
        \item $e_{1}^{1}, \ldots, e_{n_{1}}^{1}$ be an orthonormal basis of $V_{1}$,
        \item \ldots
        \item $e_{1}^{m}, \ldots, e_{n_{m}}^{m}$ be an orthonormal basis of $V_{m}$
    \end{itemize}

    then
    \[
        {\{ e_{j_{1}}^{1}\otimes \cdots \otimes e_{j_{m}}^{m} \}}_{j_{1}=1,\ldots,n_{1}; \ldots; j_{m}=1,\ldots,n_{m}}
    \]

    is a basis of $V_{1}\otimes\cdots \otimes V_{m}$. I define an inner product on $V_{1}\otimes \cdots \otimes V_{m}$ as follows:
    \begin{multline*}
        \innerprod{\sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}a_{j_{1},\ldots,j_{m}}e_{j_{1}}\otimes\cdots\otimes e_{j_{m}}, \sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}b_{j_{1},\ldots,j_{m}}e_{j_{1}}\otimes\cdots\otimes e_{j_{m}}} \\
        = \sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}a_{j_{1},\ldots,j_{m}}\conj{b_{j_{1},\ldots,j_{m}}}.
    \end{multline*}

    Let
    \begin{align*}
        v_{k} & = v_{1}^{k}e_{1}^{k} + \cdots + v_{n_{k}}^{k}e_{n_{k}}^{k} \\
        u_{k} & = u_{1}^{k}e_{1}^{k} + \cdots + u_{n_{k}}^{k}e_{n_{k}}^{k}
    \end{align*}

    for every $k\in\{1,\ldots,m\}$.
    \begin{align*}
          & \innerprod{v_{1}\otimes\cdots\otimes v_{m}, u_{1}\otimes\cdots\otimes u_{m}}                                                                                                                                                                                                         \\
        = & \innerprod{\sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}v_{j_{1}}^{1}\cdots v_{j_{m}}^{m}e_{j_{1}}^{1}\otimes \cdots\otimes e_{j_{m}}^{m}, \sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}u_{j_{1}}^{1}\cdots u_{j_{m}}^{m}e_{j_{1}}^{1}\otimes \cdots\otimes e_{j_{m}}^{m}} \\
        = & \sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}v_{j_{1}}^{1}\cdots v_{j_{m}}^{m}\conj{u_{j_{1}}^{1}\cdots u_{j_{m}}^{m}}                                                                                                                                                          \\
        = & \sum^{n_{1}}_{j_{1}=1}\cdots\sum^{n_{m}}_{j_{m}=1}v_{j_{1}}^{1}\conj{u_{j_{1}}^{1}}\cdots v_{j_{m}}^{m}\conj{u_{j_{m}}^{m}}                                                                                                                                                          \\
        = & \innerprod{v_{1}, u_{1}}\cdots\innerprod{v_{m}, u_{m}}.
    \end{align*}

    If an inner product on $V_{1}\otimes \cdots \otimes V_{m}$ satisfies
    \[
        \innerprod{v_{1}\otimes \cdots\otimes v_{m}, u_{1}\otimes \cdots\otimes u_{m}} = \innerprod{v_{1}, u_{1}}\cdots\innerprod{v_{m}, u_{m}}
    \]

    for all $(v_{1}, \ldots, v_{m})$ and $(u_{1}, \ldots, u_{m})$ in $V_{1}\times\cdots\times V_{m}$ then it is identical to the defined inner product, because every element of $V_{1}\otimes \cdots \otimes V_{m}$ is a linear combination of the basis
    \[
        {\{ e_{j_{1}}^{1}\otimes \cdots \otimes e_{j_{m}}^{m} \}}_{j_{1}=1,\ldots,n_{1}; \ldots; j_{m}=1,\ldots,n_{m}}.
    \]

    Thus there exists uniquely such an inner product on $V_{1}\otimes \cdots\otimes V_{m}$.
\end{proof}
\newpage

% chapter9:sectionD:exercise13
\begin{exercise}\label{chapter9:sectionD:exercise13}
    Suppose that $V_{1}, \ldots, V_{m}$ are finite-dimensional inner product spaces and $V_{1}\otimes\cdots\otimes V_{m}$ is made into an inner product space using the inner product from Exercise~\ref{chapter9:sectionD:exercise12}. Suppose $e_{1}^{k}, \ldots, e_{n_{k}}^{k}$ is an orthonormal basis of $V_{k}$ for each $k = 1,\ldots,m$. Show that the list
    \[
        {\{ e_{j_{1}}^{1}\otimes \cdots\otimes e_{j_{m}}^{m} \}}_{j_{1}=1,\ldots,n_{1}; \ldots ; j_{m}=1,\ldots,n_{m}}
    \]

    is an orthonormal basis of $V_{1}\otimes\cdots\otimes V_{m}$.
\end{exercise}

\begin{proof}
    Firstly,
    \[
        {\{ e_{j_{1}}^{1}\otimes \cdots\otimes e_{j_{m}}^{m} \}}_{j_{1}=1,\ldots,n_{1}; \ldots ; j_{m}=1,\ldots,n_{m}}
    \]

    is a basis of $V_{1}\otimes\cdots\otimes V_{m}$. Moreover
    \begin{align*}
        \innerprod{e_{j_{1}}^{1}\otimes\cdots\otimes e_{j_{m}}^{m}, e_{k_{1}}^{1}\otimes\cdots\otimes e_{k_{m}}^{m}} = \innerprod{e_{j_{1}}^{1}, e_{k_{1}}^{1}}\cdots\innerprod{e_{j_{m}}^{m}, e_{k_{m}}^{m}}
    \end{align*}

    so
    \[
        \innerprod{e_{j_{1}}^{1}\otimes\cdots\otimes e_{j_{m}}^{m}, e_{k_{1}}^{1}\otimes\cdots\otimes e_{k_{m}}^{m}} =
        \begin{cases}
            1 & \text{if $j_{1} = k_{1}, \ldots, j_{m} = k_{m}$} \\
            0 & \text{otherwise}
        \end{cases}.
    \]

    Thus the list
    \[
        {\{ e_{j_{1}}^{1}\otimes \cdots\otimes e_{j_{m}}^{m} \}}_{j_{1}=1,\ldots,n_{1}; \ldots ; j_{m}=1,\ldots,n_{m}}
    \]

    is an orthonormal basis of $V_{1}\otimes\cdots\otimes V_{m}$.
\end{proof}
\newpage
