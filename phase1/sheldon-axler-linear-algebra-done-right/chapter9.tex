\chapter{Multilinear Algebra and Determinants}

\section{Bilinear Forms and Quadratic Forms}

% chapter9:sectionA:exercise1
\begin{exercise}\label{chapter9:sectionA:exercise1}
    Prove that if $\beta$ is a bilinear form on $\mathbb{F}$, then there exists $c\in\mathbb{F}$ such that
    \[
        \beta(x, y) = cxy
    \]

    for all $x, y\in\mathbb{F}$.
\end{exercise}

\begin{proof}
    For all $x, y\in\mathbb{F}$
    \[
        \beta(x, y) = \beta(x\cdot 1, y\cdot 1) = x\beta(1, y\cdot 1) = xy\beta(1,1).
    \]

    Let $c = \beta(1, 1)$, then $\beta(x, y) = cxy$ for every $x, y\in\mathbb{F}$.
\end{proof}
\newpage

% chapter9:sectionA:exercise2
\begin{exercise}\label{chapter9:sectionA:exercise2}
    Let $n = \dim V$. Suppose $\beta$ is a bilinear form on $V$. Prove that there exist $\varphi_{1}, \ldots, \varphi_{n}, \tau_{1}, \ldots, \tau_{n}\in V'$ such that
    \[
        \beta(u, v) = \varphi_{1}(u)\cdot\tau_{1}(v) + \cdots + \varphi_{n}(u)\cdot\tau_{n}(v)
    \]

    for all $u, v\in V$.
\end{exercise}

\begin{quote}
    This exercise shows that if $n = \dim V$, then every bilinear form on V is of
    the form given by the last bullet point of Example 9.2.
\end{quote}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be a basis of $V$. For every vector $v$ in $V$, there exist unique scalars $a_{1}, \ldots, a_{n}$ such that
    \[
        v = a_{1}e_{1} + \cdots + a_{n}e_{n}.
    \]

    For every $k\in\{1,\ldots,n\}$, the mappings
    \begin{align*}
        \tau_{k}    & : v \mapsto a_{k}           \\
        \varphi_{k} & : v \mapsto \beta(v, e_{k})
    \end{align*}

    are linear functionals on $V$. From these, we have
    \begin{align*}
        \beta(u, v) & = \beta(u, \tau_{1}(v)e_{1} + \cdots + \tau_{n}(v)e_{n})                     \\
                    & = \beta(u, e_{1})\cdot\tau_{1}(v) + \cdots + \beta(u, e_{n})\cdot\tau_{n}(v) \\
                    & = \varphi_{1}(u)\cdot\tau_{1}(v) + \cdots + \varphi_{n}(u)\cdot\tau_{n}(v)
    \end{align*}

    for all $u, v\in V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise3
\begin{exercise}\label{chapter9:sectionA:exercise3}
    Suppose $\beta: V\times V\to\mathbb{F}$ is a bilinear form on $V$ and also is a linear functional on $V\times V$. Prove that $\beta = 0$.
\end{exercise}

\begin{proof}
    For every $u, v\in V$
    \begin{align*}
        \beta(u, v) & = \beta(u + 0, v)                                                                   \\
                    & = \beta(u, v) + \beta(0, v)               & \text{($\beta$ is a bilinear form)}     \\
                    & = \beta(u, v + 0) + \beta(0, v)                                                     \\
                    & = \beta(u, v) + \beta(u, 0) + \beta(0, v) & \text{($\beta$ is a bilinear form)}     \\
                    & = \beta(u, v) + \beta(u, v)               & \text{($\beta$ is a linear functional)}
    \end{align*}

    so $\beta(u, v) = \beta(u, v) + (-\beta(u, v)) = 0$. Hence $\beta = 0$.
\end{proof}
\newpage

% chapter9:sectionA:exercise4
\begin{exercise}\label{chapter9:sectionA:exercise4}
    Suppose $V$ is a real inner product space and $\beta$ is a bilinear form on $V$. Show that there exists a unique operator $T\in\lmap{V}$ such that
    \[
        \beta(u, v) = \innerprod{u, Tv}
    \]

    for all $u, v\in V$.
\end{exercise}

\begin{quote}
    This exercise states that if $V$ is a real inner product space, then every bilinear form on $V$ is of the form given by the third bullet point in 9.2.
\end{quote}

\begin{proof}
    $n = \dim V$. Let $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. For every $u, v\in V$
    \begin{align*}
        \beta(u, v) & = \beta(\innerprod{u,e_{1}}e_{1} + \cdots + \innerprod{u,e_{n}}e_{n}, v)            \\
                    & = \innerprod{u,e_{1}}\beta(e_{1}, v) + \cdots + \innerprod{u,e_{n}}\beta(e_{n}, v).
    \end{align*}

    Let's define an operator $T$ on $V$ as follows:
    \[
        Tv = \beta(e_{1}, v)e_{1} + \cdots + \beta(e_{n}, v)e_{n}.
    \]

    Hence for every $u, v\in V$, $\beta(u, v) = \innerprod{u, Tv}$.

    Assume operators $S, T\in\lmap{V}$ satisfy $\beta(u, v) = \innerprod{u, Sv} = \innerprod{u, Tv}$ for every $u, v\in V$. It follows that for every $v\in V$, for every $u\in V$, $\innerprod{u, Sv - Tv} = 0$, so $Sv = Tv$ for every $v\in V$. Therefore $S = T$.

    Thus there exists a unique operator $T$ on $V$ such that $\beta(u, v) = \innerprod{u, Tv}$ for all $u, v\in V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise5
\begin{exercise}\label{chapter9:sectionA:exercise5}
    Suppose $\beta$ is a bilinear form on a real inner product space $V$ and $T$ is the unique operator on $V$ such that $\beta(u, v) = \innerprod{u, Tv}$ for all $u, v \in V$ (see Exercise~\ref{chapter9:sectionA:exercise4}). Show that $\beta$ is an inner product on $V$ if and only if $T$ is an invertible positive operator on $V$.
\end{exercise}

\begin{proof}
    $\beta$ is a bilinear form on a real inner product space $V$. Therefore $\beta$ is also an inner product on $V$ if and only if $\beta$ is conjugate symmetric and positive definite.

    $(\Rightarrow)$ $\beta$ is an inner product on $V$.

    $\beta$ is conjugate symmetric, so $\beta$ is symmetric, since $\mathbb{F} = \mathbb{R}$. For every $u, v\in V$
    \[
        \innerprod{u, Tv} = \beta(u, v) = \beta(v, u) = \innerprod{v, Tu} = \innerprod{Tu, v} = \innerprod{u, T^{*}v}.
    \]

    So $Tv = T^{*}v$ for every $v\in V$, which implies $T$ is self-adjoint.

    $\innerprod{Tv, v} = \innerprod{v, Tv} = \beta(v, v) > 0$ for every nonzero $v\in V$, and $\beta(0, 0) = 0$.

    Hence $T$ is an invertible positive operator on $V$.

    \bigskip
    $(\Leftarrow)$ $T$ is an invertible positive operator on $V$.

    So $T$ is self-adjoint. For every $u, v\in V$
    \[
        \beta(v, u) = \innerprod{v, Tu} = \conj{\innerprod{Tu, v}} = \conj{\innerprod{u, T^{*}v}} = \conj{\innerprod{u, Tv}} = \conj{\beta(u, v)}
    \]

    which means $\beta$ is conjugate symmetric.

    For every $v\in V$
    \[
        \beta(v, v) = \innerprod{v, Tv} = \innerprod{Tv, v} \geq 0
    \]

    because $T$ is positive, so $\beta$ is positive. Moreover, $\beta(v, v) = 0$ if and only if $\innerprod{Tv, v} = 0$. $\innerprod{Tv, v} = 0$ if and only if $v = 0$ because $T$ is an invertible positive operator. Therefore $\beta$ is positive definite.

    Hence $\beta$ is an inner product on $V$.

    \bigskip
    Thus $\beta$ is an inner product on $V$ if and only if $T$ is an invertible positive operator on $V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise6
\begin{exercise}\label{chapter9:sectionA:exercise6}
    Prove or give a counterexample: If $\rho$ is a symmetric bilinear form on $V$, then
    \[
        \{ v\in V : \rho(v, v) = 0 \}
    \]

    is a subspace of $V$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V = \mathbb{R}^{2}$ and
    \[
        \rho((x_{1}, x_{2}), (y_{1}, y_{2})) = x_{1}y_{1} - 2x_{1}y_{2} - 2x_{2}y_{1} + 3x_{2}y_{2}
    \]

    so $\rho$ is a symmetric bilinear form on $\mathbb{R}^{2}$. $(1, 1)$ and $(1, 3)$ are in $\{ v\in V : \rho(v, v) = 0 \}$.
    \[
        \rho((1, 1), (1, 1)) = \rho((3, 1), (3, 1)) = 0.
    \]

    However,
    \[
        \rho((4, 2), (4, 2)) = 4^{2} - 4\cdot 4\cdot 2 + 3\cdot 2^{2} = 3\ne 0
    \]

    so $(4, 2)$ is not in $\{ v\in V : \rho(v, v) = 0 \}$. So $\{ v\in V : \rho(v, v) = 0 \}$ is not closed under addition, which means it is not a subspace of $\mathbb{R}^{2}$.
\end{proof}
\newpage

% chapter9:sectionA:exercise7
\begin{exercise}\label{chapter9:sectionA:exercise7}
    Explain why the proof of 9.13 (diagonalization of a symmetric bilinear form by an orthonormal basis on a real inner product space) fails if the hypothesis that $\mathbb{F} = \mathbb{R}$ is dropped.
\end{exercise}

\begin{proof}
    I give a counterexample where $\mathbb{F} = \mathbb{C}$.

    Let $\rho$ be a symmetric bilinear form on a complex inner product space $\mathbb{C}^{2}$ whose matrix (with respect to the standard basis of $\mathbb{C}^{2}$) is
    \[
        \begin{pmatrix}
            1     & \iota \\
            \iota & 0
        \end{pmatrix}.
    \]

    Let $T$ be an operator on $\mathbb{C}^{2}$ whose matrix with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}1 & \iota \\ \iota & 0 \end{pmatrix}$. The matrix of $T^{*}$ with respect to the standard basis of $\mathbb{C}^{2}$ is
    \[
        \begin{pmatrix}
            1      & -\iota \\
            -\iota & 0
        \end{pmatrix}.
    \]

    However, $T$ is not a normal operator, because
    \[
        \mathcal{M}(T)\mathcal{M}(T^{*}) =
        \begin{pmatrix}
            2 & -\iota \\
            0 & 1
        \end{pmatrix} \ne
        \begin{pmatrix}
            2 & \iota \\
            0 & 1
        \end{pmatrix} = \mathcal{M}(T^{*})\mathcal{M}(T).
    \]

    By the complex spectral theorem, there exists not exist an orthonormal basis of $\mathbb{C}^{2}$ to which $T$ has a diagonal matrix. Hence $\rho$ is not diagonalizable by an orthonormal basis.
\end{proof}
\newpage

% chapter9:sectionA:exercise8
\begin{exercise}\label{chapter9:sectionA:exercise8}
    Find formulas for $\dim V_{\text{sym}}^{(2)}$ and $\dim V_{\text{alt}}^{(2)}$ in terms of $\dim V$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be a basis of $V$.

    $V^{(2)}$ is isomorphic to $\mathbb{F}^{\dim V,\dim V}$, where each bilinear form on $V$ corresponds to its matrix with respect to $e_{1}, \ldots, e_{\dim V}$.

    A symmetric bilinear form corresponds to a symmetric matrix, and the vector space of symmetric matrices with $\dim V$ columns has dimension $(\dim V)\times(\dim V + 1)/2$. Therefore
    \[
        \dim V^{(2)}_{\text{sym}} = \frac{(\dim V) \times (\dim V + 1)}{2}.
    \]

    Moreover, $\dim V^{(2)} = {(\dim V)}^{2}$ and $V^{(2)} = V^{(2)}_{\text{sym}}\oplus V^{(2)}_{\text{alt}}$ so
    \[
        \dim V^{(2)}_{\text{alt}} = {(\dim V)}^{2} - \frac{(\dim V) \times (\dim V + 1)}{2} = \frac{(\dim V)\times (\dim V - 1)}{2}.
    \]

    Thus $\dim V^{(2)}_{\text{sym}} = (\dim V)\times(\dim V + 1)/2$ and $\dim V^{(2)}_{\text{alt}} = (\dim V)\times(\dim V - 1)/2$.
\end{proof}
\newpage

% chapter9:sectionA:exercise9
\begin{exercise}\label{chapter9:sectionA:exercise9}
    Suppose that $n$ is a positive integer and $V = \{ p\in\mathscr{P}_{n}(\mathbb{R}): p(0) = p(1) \}$. Define $\alpha: V\times V\to\mathbb{R}$ by
    \[
        \alpha(p, q) = \int^{1}_{0}pq'.
    \]

    Show that $\alpha$ is an alternating bilinear form on $V$.
\end{exercise}

\begin{proof}
    For every $p_{1}, p_{2}, q \in V$
    \[
        \alpha(p_{1} + p_{2}, q) = \int^{1}_{0}(p_{1} + p_{2})q' = \int^{1}_{0}p_{1}q' + \int^{1}_{0}p_{2}q' = \alpha(p_{1}, q) + \alpha(p_{2}, q).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(\lambda p, q) = \int^{1}_{0}(\lambda p)q' = \lambda\int^{1}_{0}pq' = \lambda\alpha(p, q).
    \]

    For every $p, q_{1}, q_{2}\in V$
    \[
        \alpha(p, q_{1} + q_{2}) = \int^{1}_{0}p(q_{1} + q_{2})' = \int^{1}_{0}pq_{1}' + \int^{1}_{0}pq_{2}' = \alpha(p, q_{1}) + \alpha(p, q_{2}).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(p, \lambda q) = \int^{1}_{0}p(\lambda q)' = \lambda\int^{1}_{0}pq' = \lambda\alpha(p, q).
    \]

    So $\alpha$ is a bilinear form on $V$.

    For every $p\in V$
    \[
        \alpha(p, p) = \int^{1}_{0}pp' = \int^{p(1)}_{p(0)}pdp = \frac{{(p(1))}^{2} - {(p(0))}^{2}}{2} = 0.
    \]

    Hence $\alpha$ is an alternating bilinear form on $V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise10
\begin{exercise}\label{chapter9:sectionA:exercise10}
    Suppose that $n$ is a positive integer and
    \[
        V = \{ p\in\mathscr{P}_{n}(\mathbb{R}): p(0) = p(1) \land p'(0) = p'(1) \}.
    \]

    Define $\rho: V\times V\to\mathbb{R}$ by
    \[
        \rho(p, q) = \int^{1}_{0}pq''.
    \]

    Show that $\rho$ is a symmetric bilinear form on $V$.
\end{exercise}

\begin{proof}
    For every $p_{1}, p_{2}, q \in V$
    \[
        \alpha(p_{1} + p_{2}, q) = \int^{1}_{0}(p_{1} + p_{2})q'' = \int^{1}_{0}p_{1}q'' + \int^{1}_{0}p_{2}q'' = \alpha(p_{1}, q) + \alpha(p_{2}, q).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(\lambda p, q) = \int^{1}_{0}(\lambda p)q'' = \lambda\int^{1}_{0}pq'' = \lambda\alpha(p, q).
    \]

    For every $p, q_{1}, q_{2}\in V$
    \[
        \alpha(p, q_{1} + q_{2}) = \int^{1}_{0}p(q_{1} + q_{2})'' = \int^{1}_{0}pq_{1}'' + \int^{1}_{0}pq_{2}'' = \alpha(p, q_{1}) + \alpha(p, q_{2}).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(p, \lambda q) = \int^{1}_{0}p(\lambda q)'' = \lambda\int^{1}_{0}pq'' = \lambda\alpha(p, q).
    \]

    So $\alpha$ is a bilinear form on $V$.

    For every $p, q\in V$
    \begin{align*}
        \alpha(p, q) & = \int^{1}_{0}pq'' = \int^{1}_{0}p(x)q''(x)dx         \\
                     & = \int^{x=1}_{x=0}p(x)dq'(x)                          \\
                     & = p(x)q'(x)\Big{\vert}^{x=1}_{x=0} - \int^{1}_{0}q'p' \\
                     & = -\int^{1}_{0}q'p'.
    \end{align*}

    Therefore $\alpha(q, p) = -\int^{1}_{0}p'q' = -\int^{1}_{0}q'p' = \alpha(p, q)$.

    Hence $\alpha$ is a symmetric bilinear form on $V$.
\end{proof}
\newpage

\section{Alternating Multilinear Forms}

% chapter9:sectionB:exercise1
\begin{exercise}\label{chapter9:sectionB:exercise1}
    Suppose $m$ is a positive integer. Show that $\dim V^{(m)} = {(\dim V)}^{m}$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{\dim V}$ be its dual basis.

    For each \textit{choices (duplication is allowed)}  of $m$ linear functionals from $\varphi_{1}, \ldots, \varphi_{\dim V}$, we define an $m$-linear form as follows, where $\varphi_{j_{1}}, \ldots, \varphi_{j_{m}}$ is the selected listed, $j_{1}, \ldots, j_{m}$ are from $1, \ldots, \dim V$
    \[
        \alpha_{j_{1},\ldots,j_{m}}: (v_{1}, \ldots, v_{m})\mapsto \varphi_{j_{1}}(v_{1})\cdots \varphi_{j_{m}}(v_{m})
    \]

    Assume that
    \[
        \sum x_{j_{1},\ldots,j_{m}}\alpha_{j_{1},\ldots,j_{m}} = 0
    \]

    for all $v_{1}, \ldots, v_{m}\in V$ and we use all previously defined $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$. Plug $(v_{j_{1}}, \ldots, v_{j_{m}})\in \underbrace{V\times\cdots\times V}_{m}$ in, we obtain
    \[
        x_{j_{1},\ldots, j_{m}} = 0.
    \]

    Hence all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ are linearly independent. Moreover, for every $m$-linear form $\alpha$ on $V$,
    \begin{align*}
        \alpha(v_{1}, \ldots, v_{m}) & = \alpha\left(\sum^{\dim V}_{j=1}\varphi_{j}(v_{1})e_{j}, \ldots, \sum^{\dim V}_{j=1}\varphi_{j}(v_{m})e_{j}\right)                                                     \\
                                     & = \sum^{\dim V}_{j_{m}=1}\left(\cdots\left(\sum^{\dim V}_{j_{1}=1}\alpha(e_{j_{1}}, \ldots, e_{j_{m}})\varphi_{j_{1}}(v_{1})\cdots\varphi_{j_{m}}(v_{m})\right)\right)  \\
                                     & = \sum^{\dim V}_{j_{m}=1}\left(\cdots\left(\sum^{\dim V}_{j_{1}=1}\alpha(e_{j_{1}}, \ldots, e_{j_{m}})\varphi_{j_{1},\ldots,j_{m}}(v_{1}, \ldots, v_{m})\right)\right).
    \end{align*}

    This means all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ spans $V^{(m)}$. Therefore all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ constitute a basis of $V^{(m)}$ and $\dim V^{(m)} = {(\dim V)}^{m}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise2
\begin{exercise}\label{chapter9:sectionB:exercise2}
    Suppose $n\geq 3$ and $\alpha: \mathbb{F}^{n}\times\mathbb{F}^{n}\times\mathbb{F}^{n} \to \mathbb{F}$ is defined by
    \begin{gather*}
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) \\
        = x_{1}y_{2}z_{3} - x_{2}y_{1}z_{3} - x_{3}y_{2}z_{1} - x_{1}y_{3}z_{2} + x_{3}y_{1}z_{2} + x_{2}y_{3}z_{1}.
    \end{gather*}

    Show that $\alpha$ is an alternating $3$-linear form on $\mathbb{F}^{n}$.
\end{exercise}

\begin{proof}
    \begin{align*}
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = x_{1}(y_{2}z_{3} - y_{3}z_{2}) + x_{2}(y_{3}z_{1} - y_{1}z_{3}) + x_{3}(y_{1}z_{2} - y_{2}z_{1}) \\
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = y_{1}(z_{2}x_{3} - z_{3}x_{2}) + y_{2}(z_{3}x_{1} - z_{1}x_{3}) + y_{3}(z_{1}x_{2} - z_{2}x_{1}) \\
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = z_{1}(x_{2}y_{3} - x_{3}y_{2}) + z_{2}(x_{3}y_{1} - x_{1}y_{3}) + z_{3}(x_{1}y_{2} - x_{2}y_{1})
    \end{align*}

    so $\alpha$ is a $3$-linear form on $\mathbb{F}^{n}$ and $\alpha$ is alternating.
\end{proof}
\newpage

% chapter9:sectionB:exercise3
\begin{exercise}\label{chapter9:sectionB:exercise3}
    Suppose $m$ is a positive integer and $\alpha$ is an $m$-linear form on $V$ such that $\alpha(v_{1}, \ldots, v_{m}) = 0$ whenever $v_{1}, \ldots, v_{m}$ is a list of vectors in $V$ with $v_{j} = v_{j+1}$ for some $j\in\{1,\ldots,m-1\}$. Prove that $\alpha$ is an alternating $m$-linear form on $V$.
\end{exercise}

\begin{proof}
    We will show that for every positive integer $k$ less than $m$, $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{j+k}$ for every $j\in\{1,\ldots,m-k\}$.

    The statement is true for $k = 1$ due to the hypothesis.

    Assume the statement is true for all $k < \ell$ where $j + \ell < m$. Suppose that $v_{j} = v_{j + \ell}$.

    Consider the expression $\alpha(\ldots, v_{j} + v_{j+k}, \ldots, v_{j+k} + v_{j}, \ldots)$ where $v_{j} + v_{j+k}$ is at the $j$th slot, and $v_{j+k} + v_{j}$ is at the ${(j+k)}$th slot, $k < \ell$.

    We prove the anti-symmetric property first. By the induction hypothesis, for every
    \[
        (\ldots, w_{j} + w_{j+k}, \ldots, w_{j+k} + w_{j}, \ldots)\in \underbrace{V\times\cdots\times V}_{m}
    \]

    we have
    \begin{align*}
        0 = \alpha(\ldots, w_{j} + w_{j+k}, \ldots, w_{j+k} + w_{j}, \ldots) & = \alpha(\ldots, w_{j}, \ldots, w_{j+k}, \ldots) + \alpha(\ldots, w_{j+k}, \ldots, w_{j}, \ldots).
    \end{align*}

    So $\alpha(\ldots, w_{j}, \ldots, w_{j+k}, \ldots) = -\alpha(\ldots, w_{j+k}, \ldots, w_{j}, \ldots)$. Apply this result, we obtain that, if $(v_{1}, \ldots, v_{m})\in \underbrace{V\times\cdots\times V}_{m}$ such that $v_{j} = v_{j+\ell}$, then
    \[
        \alpha(\ldots, v_{j}, v_{j+1}, \ldots, v_{j+\ell}, \ldots) = -\alpha(\ldots, v_{j+1}, v_{j}, \ldots, v_{j+\ell}, \ldots).
    \]

    By the induction hypothesis, $\alpha(\ldots, v_{j+1}, v_{j}, \ldots, v_{j+\ell}, \ldots) = 0$ because $v_{j}$ is at the $(j+1)$th slot and $v_{j+\ell}$ is at the $(j+\ell)$th slot. Therefore $\alpha(\ldots, v_{j}, v_{j+1}, \ldots, v_{j+\ell}, \ldots) = 0$.

    Due to the principle of mathematical induction, for every positive integer $k < m$, for every positive integer $j\in\{1,\ldots,m-k\}$, $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{j + k}$.

    Thus $\alpha$ is an alternating $m$-linear form on $V$.
\end{proof}
\newpage

% chapter9:sectionB:exercise4
\begin{exercise}\label{chapter9:sectionB:exercise4}
    Prove or give a counterexample: If $\alpha\in V^{(4)}_{\text{alt}}$, then
    \[
        \{ (v_{1}, v_{2}, v_{3}, v_{4})\in V^{4}: \alpha(v_{1}, v_{2}, v_{3}, v_{4}) = 0 \}
    \]

    is a subspace of $V^{4}$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V$ is a vector space with dimension $4$. Let $e_{1}, e_{2}, e_{3}, e_{4}$ be a basis of $V$, and $\alpha$ a nonzero alternating $4$-linear form on $V$. Because $\alpha$ is alternating,
    \[
        \alpha(e_{1}, e_{1}, e_{3}, 0) = \alpha(0, e_{2}, e_{2}, e_{4}) = 0.
    \]

    However $(e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4}) = (e_{1}, e_{1}, e_{3}, 0) + (0, e_{2}, e_{2}, e_{4})$ and
    \[
        \alpha(e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4})\ne 0
    \]

    because $e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4}$. Therefore the set
    \[
        \{ (v_{1}, v_{2}, v_{3}, v_{4})\in V^{4}: \alpha(v_{1}, v_{2}, v_{3}, v_{4}) = 0 \}
    \]

    is not closed under addition, which implies it is not a subspace of $V^{4}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise5
\begin{exercise}\label{chapter9:sectionB:exercise5}
    Suppose $m$ is a positive integer and $\beta$ is an $m$-linear form on $V$. Define an $m$-linear form $\alpha$ on $V$ by
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}(\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    for $v_{1}, \ldots, v_{m}\in V$. Explain why $\alpha\in V^{(m)}_{\text{alt}}$.
\end{exercise}

\begin{proof}
    For each $(j_{1}, \ldots, j_{m})\in \operatorname{perm}m$, the map
    \[
        (v_{1}, \ldots, v_{m})\mapsto (\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    is an $m$-linear form on $V$. Therefore
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}(\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    is an $m$-linear form on $V$.

    If $v_{j} = v_{k}$ for some $j\ne k$ then for each permutation $(x_{1}, \ldots, x_{m})$ in $\operatorname{perm}m$, there exists a unique permutation $(y_{1}, \ldots, y_{m})$ in $\operatorname{perm}m$ where these two differ by a swap $j\leftrightarrow k$, hence one has $+1$ sign and the other has $-1$ sign. Moreover,
    \[
        \beta(v_{x_{1}}, \ldots, v_{x_{m}}) = \beta(v_{y_{1}}, \ldots, v_{y_{m}}).
    \]

    Therefore $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{k}$ for some $j\ne k$. Hence $\alpha$ is alternating.

    Thus $\alpha\in V^{(m)}_{\text{alt}}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise6
\begin{exercise}\label{chapter9:sectionB:exercise6}
    Suppose $m$ is a positive integer and $\beta$ is an $m$-linear form on $V$. Define an $m$-linear form $\alpha$ on $V$ by
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    for $v_{1}, \ldots, v_{m}\in V$. Explain why
    \[
        \alpha(v_{k_{1}}, \ldots, v_{k_{m}}) = \alpha(v_{1}, \ldots, v_{m})
    \]

    for all $v_{1}, \ldots, v_{m}\in V$ and all $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$.
\end{exercise}

\begin{proof}
    For each $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$, $\alpha(v_{k_{1}}, \ldots, v_{k_{m}})$ is the sum of all $\beta(v_{i_{1}}, \ldots, v_{i_{m}})$ where each $(i_{1}, \ldots, i_{m})$ is a permutation of $(k_{1}, \ldots, k_{m})$. On the other hand, each $(i_{1}, \ldots, i_{m})$ is also a permutation of $(1, \ldots, m)$. Therefore $\alpha(v_{k_{1}}, \ldots, v_{k_{m}}) = \alpha(v_{1}, \ldots, v_{m})$ for all $v_{1}, \ldots, v_{m}\in V$ and all $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$.
\end{proof}
\newpage

% chapter9:sectionB:exercise7
\begin{exercise}\label{chapter9:sectionB:exercise7}
    Give an example of a nonzero alternating $2$-linear form $\alpha$ on $\mathbb{R}^{3}$ and a linearly independent list $v_{1}, v_{2}$ in $\mathbb{R}^{3}$ such that $\alpha(v_{1}, v_{2}) = 0$.
\end{exercise}

\begin{quote}
    This exercise shows that 9.39 can fail if the hypothesis that $n = \dim V$ is deleted.
\end{quote}

\begin{proof}
    Here is an example.
    \[
        \alpha((x_{1}, x_{2}, x_{3}), (y_{1}, y_{2}, y_{3})) = x_{1}y_{2} - x_{2}y_{1}
    \]

    $\alpha\in {(\mathbb{R}^{3})}^{(2)}_{\text{alt}}$. However,
    \[
        \alpha((1, 0, 0), (1, 0, 1)) = 0
    \]

    although $(1, 0, 0), (1, 0, 1)$ is a linearly independent list.
\end{proof}
\newpage

\section{Determinants}

% chapter9:sectionC:exercise1
\begin{exercise}\label{chapter9:sectionC:exercise1}
    Prove or give a counterexample: $S, T\in\lmap{V}\implies \det(S + T) = \det S + \det T$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    Let $V$ be a real finite-dimensional vector space of dimension greater than $1$, then
    \[
        \det(I + I) = 2^{\dim V} \ne 1 + 1 = \det I + \det I.
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise2
\begin{exercise}\label{chapter9:sectionC:exercise2}
    Suppose the first column of a square matrix $A$ consists of all zeros except possibly the first entry $A_{1,1}$. Let $B$ be the matrix obtained from $A$ by deleting the first row and the first column of $A$. Show that $\det A = A_{1,1} \det B$.
\end{exercise}

\begin{proof}
    Let $n$ be the number of columns of $A$. By the Leibniz's determinant formula
    \begin{align*}
        \det A & = \sum_{(j_{1}, \ldots, j_{n})\in\operatorname{perm}n}(\operatorname{sign}(j_{1}, \ldots, j_{n}))A_{j_{1},1}\cdots A_{j_{n},n}
    \end{align*}

    If $j_{1}\ne 1$, then $A_{j_{1},1}\cdots A_{j_{n},n} = 0$ because $A_{i,1} = 0$ for all $i\ne 1$. Therefore
    \begin{align*}
        \det A & = A_{1,1}\sum_{(j_{2}, \ldots, j_{n})\in\operatorname{perm}(2,\ldots,n)}\operatorname{sign}(j_{2}, \ldots, j_{n})A_{j_{2},2}\cdots A_{j_{n},n} \\
               & = A_{1,1}\sum_{(i_{1},\ldots, i_{n-1})\in\operatorname{perm}(n-1)}B_{i_{1},1}\cdots B_{i_{n-1},n-1}                                            \\
               & = A_{1,1}\det B
    \end{align*}

    where $\operatorname{sign}(j_{2}, \ldots, j_{n}) = \operatorname{sign}(1, j_{2}, \ldots, j_{n})$ because $1 < j_{2}, \ldots, j_{n}$.

    Thus $\det A = A_{1,1}\det B$.
\end{proof}
\newpage

% chapter9:sectionC:exercise3
\begin{exercise}\label{chapter9:sectionC:exercise3}
    Suppose $T\in\lmap{V}$ is nilpotent. Prove that $\det(I + T) = 1$.
\end{exercise}

\begin{proof}
    Because $T$ is nilpotent, then there exists a basis $e_{1}, \ldots, e_{\dim V}$ of $V$ to which $T$ has an upper-triangular matrix whose entries on the diagonal are $0$. Therefore the matrix of $I + T$ with respect to the basis $e_{1}, \ldots, e_{\dim V}$ of $V$ is an upper-triangular matrix whose entries on the diagonal are $1$. Thus $\det(I + T) = 1$.
\end{proof}
\newpage

% chapter9:sectionC:exercise4
\begin{exercise}\label{chapter9:sectionC:exercise4}
    Suppose $S\in\lmap{V}$. Prove that $S$ is unitary if and only if $\abs{\det S} = \norm{S} = 1$.
\end{exercise}

\begin{proof}
    Let $s_{1}\geq s_{2}\geq \cdots \geq s_{\dim V}$ be the singular values of $S$, then
    \[
        \abs{\det S} = s_{1}\cdots s_{\dim V}
    \]

    and
    \[
        \norm{S} = s_{1}.
    \]

    Therefore $\abs{\det S} = \norm{S} = 1$ if and only if all singular values of $S$ are equal to $1$. Moreover, all singular values of $S$ are equal to $1$ if and only if $S$ is unitary.

    Thus $S$ is unitary if and only if $\abs{\det S} = \norm{S} = 1$.
\end{proof}
\newpage

% chapter9:sectionC:exercise5
\begin{exercise}\label{chapter9:sectionC:exercise5}
    Suppose $A$ is a block upper-triangular matrix
    \[
        A = \begin{pmatrix}
            A_{1} &        & *     \\
                  & \ddots &       \\
            0     &        & A_{m}
        \end{pmatrix},
    \]

    where each $A_{k}$ along the diagonal is a square matrix. Prove that
    \[
        \det A = (\det A_{1})\cdots (\det A_{m}).
    \]
\end{exercise}

\begin{proof}
    Firstly, for brevity (I don't want to use lengthy indices), I will prove that if
    \[
        C = \begin{pmatrix}
            A & * \\
            0 & B
        \end{pmatrix}
    \]

    then $\det C = (\det A)(\det B)$.

    Let the numbers of columns of $A, B$ be $m, n$, respectively. Let $T$ be the operator on $V = \mathbb{F}^{m+n}$ whose matrix with respect to the standard basis of $V = \mathbb{F}^{m+n}$ is $C$.

    Let $\alpha$ be a nonzero alternating $(m+n)$-linear form in $V^{(m)}_{\text{alt}}$.
    \begin{align*}
        (\det T)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}) & = \alpha(Te_{1}, \ldots, Te_{m}, Te_{m+1}, \ldots, Te_{m+n})
    \end{align*}

    Let $U = \operatorname{span}(e_{1}, \ldots, e_{n})$, then $U$ is invariant under $T$. Moreover, the matrix of $T\vert_{U}$ with respect to $e_{1}, \ldots, e_{n}$ is $A$. The following map
    \[
        (v_{1}, \ldots, v_{m})\mapsto \alpha(Tv_{1}, \ldots, Tv_{m}, Te_{m+1}, \ldots, Te_{m+n})
    \]

    is an alternating $m$-linear form on $U$. Therefore
    \begin{align*}
        \alpha(Te_{1}, \ldots, Te_{m}, Te_{m+1}, \ldots, Te_{m+n}) & = (\det T\vert_U)\alpha(e_{1}, \ldots, e_{m}, Te_{m+1}, \ldots, Te_{m+n}) \\
                                                                   & = (\det A)\alpha(e_{1}, \ldots, e_{m}, Te_{m+1}, \ldots, Te_{m+n})
    \end{align*}

    Moreover, for each $j\in\{1,\ldots,n\}$
    \begin{align*}
        Te_{m+j} & = (C_{1,m+j}e_{1} + \cdots + C_{m, m+j}e_{m}) + (C_{m+1,j}e_{m+1} + \cdots + C_{m+n,j}e_{m+n}) \\
                 & = (C_{1,m+j}e_{1} + \cdots + C_{m, m+j}e_{m}) + (B_{1,j}e_{m+1} + \cdots + B_{n,j}e_{m+n})
    \end{align*}

    and $\alpha$ is an alternating $(m+n)$-linear form on $V$, so we have
    \[
        \alpha(e_{1}, \ldots, e_{m}, Te_{m+1}, \ldots, Te_{m+n}) = \alpha(e_{1}, \ldots, e_{m}, \sum^{n}_{i=1}B_{i,1}e_{m+i}, \ldots, \sum^{n}_{i=1}B_{i,n}e_{m+i})
    \]

    Let $S$ be the operator on $W = \operatorname{span}(e_{m+1}, \ldots, e_{m+n})$ whose matrix with respect to the basis $e_{m+1}, \ldots, e_{m+n}$ is $B$, then
    \[
        \alpha(e_{1}, \ldots, e_{m}, \sum^{n}_{i=1}B_{i,1}e_{m+i}, \ldots, \sum^{n}_{i=1}B_{i,n}e_{m+i}) = \alpha(e_{1}, \ldots, e_{m}, Se_{m+1}, \ldots, Se_{m+n}).
    \]

    The following map
    \[
        (v_{1}, \ldots, v_{n})\mapsto \alpha(e_{1}, \ldots, e_{m}, Sv_{1}, \ldots, Sv_{n})
    \]

    is an alternating $n$-linear form on $W$, so
    \begin{align*}
        \alpha(e_{1}, \ldots, e_{m}, Se_{m+1}, \ldots, Se_{m+n}) & = (\det S)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n})  \\
                                                                 & = (\det B)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}).
    \end{align*}

    Combine every so far together, we obtain
    \begin{align*}
        (\det C)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}) & = \alpha(Te_{1}, \ldots, Te_{m}, Te_{m+1}, \ldots, Te_{m+n})              \\
                                                                       & = (\det A)(\det B)\alpha(e_{1}, \ldots, e_{m}, e_{m+1}, \ldots, e_{m+n}).
    \end{align*}

    Hence $\det C = (\det A)(\det B)$.

    \bigskip
    Back to the original problem. I give a proof using mathematical induction on $m$.

    The statement is true for $m = 1$.

    Assume the statement is true for $m = n - 1$. Now suppose $A$ is a block upper-triangular matrix has $n$ blocks. Let $B$ be the block upper-triangular matrix obtained by removing the rows and columns of $A$ containing entries of $A_{n}$. By the previously proved result,
    \[
        \det A = (\det B)(\det A_{n}).
    \]

    According to the induction hypothesis, $\det B = (\det A_{1})\cdots (\det A_{n-1})$. Therefore $\det A = (\det A_{1})\cdots (\det A_{n})$.

    By the principle of mathematical induction, we conclude that
    \[
        \det A = (\det A_{1})\cdots (\det A_{m}).\qedhere
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise6
\begin{exercise}\label{chapter9:sectionC:exercise6}
    Suppose $A = \begin{pmatrix}v_{1} & \cdots & v_{n}\end{pmatrix}$ is an $n$-by-$n$ matrix, with $v_{k}$ denoting $k$th column of $A$. Show that if $(m_{1}, \ldots, m_{n})\in \operatorname{perm}n$, then
    \[
        \det\begin{pmatrix}v_{m_{1}} & \cdots & v_{m_{n}}\end{pmatrix} = (\operatorname{sign}(m_{1}, \ldots, m_{n}))\det A.
    \]
\end{exercise}

\begin{proof}
    Because $(u_{1}, \ldots, u_{n})\mapsto \det\begin{pmatrix}u_{1} & \cdots & u_{n}\end{pmatrix}$ is an alternating $n$-linear form on $\mathbb{F}^{n}$, then
    \begin{align*}
        \det\begin{pmatrix}v_{m_{1}} & \cdots & v_{m_{n}}\end{pmatrix} & = (\operatorname{sign}(m_{1}, \ldots, m_{n}))\det\begin{pmatrix}v_{1} & \cdots & v_{n}\end{pmatrix} \\
                                                                                                 & = (\operatorname{sign}(m_{1}, \ldots, m_{n}))\det A.\qedhere
    \end{align*}
\end{proof}
\newpage

% chapter9:sectionC:exercise7
\begin{exercise}\label{chapter9:sectionC:exercise7}
    Suppose $T\in\lmap{V}$ is invertible. Let $p$ denote the characteristic polynomial of $T$ and let $q$ denote the characteristic polynomial of $T^{-1}$. Prove that
    \[
        q(z) = \frac{1}{p(0)}z^{\dim V}p\left(\frac{1}{z}\right)
    \]

    for all nonzero $z\in\mathbb{F}$.
\end{exercise}

\begin{proof}
    For all nonzero $z\in\mathbb{F}$,
    \begin{align*}
        q(z) & = \det(zI - T^{-1})                                                         & \text{(definition of characteristic polynomial)} \\
             & = \det(T^{-1}zT - T^{-1})                                                                                                      \\
             & = \det(T^{-1})\det(zT - I)                                                  & (\det(AB) = (\det A)(\det B))                    \\
             & = \frac{1}{\det T}\det(zT - I)                                              & (1 = (\det T^{-1})(\det T))                      \\
             & = \frac{1}{\det T}{(-z)}^{\dim V}\det\left(\frac{1}{z}I - T\right)          & (\det\lambda T = \lambda^{\dim V}\det T)         \\
             & = \frac{{(-1)}^{\dim V}}{\det T}z^{\dim V}\det\left(\frac{1}{z}I - T\right)                                                    \\
             & = \frac{1}{p(0)}z^{\dim V}p\left(\frac{1}{z}\right).
    \end{align*}
\end{proof}
\newpage

% chapter9:sectionC:exercise8
\begin{exercise}\label{chapter9:sectionC:exercise8}
    Suppose $T \in \lmap{V}$ is an operator with no eigenvalues (which implies that $\mathbb{F} = \mathbb{R}$). Prove that $\det T > 0$.
\end{exercise}

\begin{proof}
    Let $p$ be the characteristic polynomial of $T$. Because $T$ has no eigenvalue, then $p$ has no root. Therefore $p(x)\ne 0$ for every $x\in\mathbb{R}$.

    Assume there exist $a, b\in\mathbb{R}$ such that $p(a) < 0$ and $p(b) > 0$. Because of the intermediate value theorem and $p$ is continuous on $\mathbb{R}$, there exists $c\in\mathbb{R}$ and $a < c < b$ such that $p(c) = 0$. This is a contradiction because $p$ has no root. Therefore, either $p(x) > 0$ for every $x\in\mathbb{R}$ or $p(x) < 0$ for every $x\in\mathbb{R}$.

    On the other hand $p$ is a monic polynomial with real coefficient, so
    \[
        \lim\limits_{x\to+\infty}\frac{p(x)}{x^{\dim V}} = 1
    \]

    which implies $\lim\limits_{x\to+\infty}p(x) = +\infty$. Hence there exists a real number $x_{0}$ such that $p(x_{0}) > 0$. Since either $p(x) > 0$ for every $x\in\mathbb{R}$ or $p(x) < 0$ for every $x\in\mathbb{R}$, we conclude that $p(x) > 0$ for all $x\in\mathbb{R}$.

    $\dim V$ is an even number, because if $\dim V$ is an odd number, then $p$ has a root. Therefore the constant term of $p$ is ${(-1)}^{\dim V}\det T = \det T$. So $\det T = p(0) > 0$.
\end{proof}
\newpage

% chapter9:sectionC:exercise9
\begin{exercise}\label{chapter9:sectionC:exercise9}
    Suppose that $V$ is a real vector space of even dimension, $T \in \lmap{V}$, and $\det T < 0$. Prove that $T$ has at least two distinct eigenvalues.
\end{exercise}

\begin{proof}
    Let $p$ be the characteristic polynomial of $T$. Because $\dim V$ is an even number, the constant term of $p$ is ${(-1)}^{\dim V} = \det T$.

    We have
    \[
        \lim\limits_{x\to{\color{red}{+\infty}}}\frac{p(x)}{x^{\dim V}} = \lim\limits_{x\to{\color{blue}{-\infty}}}\frac{p(x)}{x^{\dim V}} = 1.
    \]

    So $\lim\limits_{x\to{\color{red}{+\infty}}}p(x) = \lim\limits_{x\to{\color{blue}{-\infty}}}p(x) = +\infty$. Therefore, there are positive number $a$ and negative number $b$ such that $p(a) > 0$ and $p(b) > 0$.

    Since $\det T < 0$ and $\det T = p(0)$, it follows that $p(0) < 0$. By the intermediate value theorem, there is a positive number $c_{1}$ less than $a$ such that $p(c_{1}) = 0$ and there is a negative number $c_{2}$ larger than $b$ such that $p(c_{2}) = 0$. Moreover, $c_{1}$ and $c_{2}$ are distinct and are also eigenvalues of $T$.

    Thus $T$ has at least two distinct eigenvalues.
\end{proof}
\newpage

% chapter9:sectionC:exercise10
\begin{exercise}\label{chapter9:sectionC:exercise10}
    Suppose $V$ is a real vector space of odd dimension and $T \in \lmap{V}$. Without using the minimal polynomial, prove that $T$ has an eigenvalue.
\end{exercise}

\begin{quote}
    This result was previously proved without using determinants or the characteristic polynomial $-$ see 5.34.
\end{quote}

\begin{proof}
    $V$ is a real vector space of odd dimension, so the characteristic polynomial of $T$ has real coefficients and odd degree. Therefore the characteristic polynomial of $T$ has a root, which implies $T$ has an eigenvalue.
\end{proof}
\newpage

% chapter9:sectionC:exercise11
\begin{exercise}\label{chapter9:sectionC:exercise11}
    Prove or give a counterexample: If $\mathbb{F} = \mathbb{R}$, $T\in\lmap{V}$, and $\det T > 0$, then $T$ has a square root.
\end{exercise}

\begin{quote}
    If $\mathbb{F} = \mathbb{C}$, $T\in\lmap{V}$, and $\det T\ne 0$, then $T$ has a square root (see 8.41).
\end{quote}

\begin{proof}
    Here is a counterexample.

    On $V = \mathbb{R}^{2}$, let $T(x, y) = (-x, x - y)$. The matrix of $T$ with respect to $(1, 0), (0, 1)$ is
    \[
        A = \begin{pmatrix}
            -1 & 1  \\
            0  & -1
        \end{pmatrix}.
    \]

    $\det T = \det A = 1 > 0$. Assume $T$ has a square root $S\in\lmap{\mathbb{R}^{2}}$, let the matrix of $S$ with respect to $(1, 0), (0, 1)$ be $B$. Because $T = S^{2}$, so $B^{2} = A$, and we obtain the following system of equations
    \[
        \begin{cases}
            B_{1,1}^{2} + B_{1,2}B_{2,1} = -1, \\
            B_{2,2}^{2} + B_{1,2}B_{2,1} = -1, \\
            B_{1,2}(B_{1,1} + B_{2,2}) = 1,    \\
            B_{2,1}(B_{1,1} + B_{2,2}) = 0.
        \end{cases}
    \]

    From the last two equations, we deduce that $B_{1,1} + B_{2,2}\ne 0$ and $B_{2,1} = 0$. Therefore $-1 = B_{1,1}^{2} + B_{1,2}B_{2,1} = B_{1,1}^{2}$, which is impossible because $B_{1,1}$ is a real number.

    Hence the chosen operator $T$ does not have a square root.
\end{proof}
\newpage

% chapter9:sectionC:exercise12
\begin{exercise}\label{chapter9:sectionC:exercise12}
    Suppose $S, T\in\lmap{V}$ and $S$ is invertible. Define $p: \mathbb{F}\to\mathbb{F}$ by
    \[
        p(z) = \det(zS - T).
    \]

    Prove that $p$ is a polynomial of degree $\dim V$ and that the coefficient of $z^{\dim V}$ in this polynomial is $\det S$.
\end{exercise}

\begin{proof}
    \[
        p(z) = \det(zS - T) = \det(zIS - TS^{-1}S) = \det(zI - TS^{-1})\det(S)
    \]

    so the coefficient of $z^{\dim V}$ in this polynomial is $\det S$.
\end{proof}
\newpage

% chapter9:sectionC:exercise13
\begin{exercise}\label{chapter9:sectionC:exercise13}
    Suppose $\mathbb{F} = \mathbb{C}$, $T\in\lmap{V}$, and $n = \dim V > 2$. Let $\lambda_{1}, \ldots, \lambda_{n}$ denote the eigenvalues of $T$, with each eigenvalue included as many times as its multiplicity.
    \begin{enumerate}[label={(\alph*)}]
        \item Find a formula for the coefficient of $z^{n-2}$ in the characteristic polynomial of $T$ in terms of $\lambda_{1}, \ldots, \lambda_{n}$.
        \item Find a formula for the coefficient of $z$ in the characteristic polynomial of $T$ in terms of $\lambda_{1}, \ldots, \lambda_{n}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    The characteristic polynomial of $T$ is $(z - \lambda_{1})\cdots (z - \lambda_{n})$.

    \begin{enumerate}[label={(\alph*)}]
        \item The coefficient of $z^{n-2}$ in the characteristic polynomial of $T$ is
              \[
                  \sum_{1\leq i < j\leq n}\lambda_{i}\lambda_{j}.
              \]
        \item The coefficient of $z$ in the characteristic polynomial of $T$ is
              \[
                  {(-1)}^{n-1}\sum^{n}_{i=1}\left(\prod^{n}_{\stackrel{j=1}{j\ne i}}\lambda_{j}\right).
              \]
    \end{enumerate}
\end{proof}
\newpage

% chapter9:sectionC:exercise14
\begin{exercise}\label{chapter9:sectionC:exercise14}
    Suppose $V$ is an inner product space and $T$ is a positive operator on $V$. Prove that
    \[
        \det\sqrt{T} = \sqrt{\det T}.
    \]
\end{exercise}

\begin{proof}
    $T$ is a positive operator so $T$ is self-adjoint and hence normal. By the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{\dim V}$ of $V$ which consists of eigenvectors of $T$. Let the eigenvalue of $T$ corresponding to $e_{k}$ be $\lambda_{k}$, for every $k\in\{1,\ldots,\dim V\}$.

    Because $T$ is a positive operator, all eigenvalues of $T$ are nonnegative, and $T$ has a unique positive square root $\sqrt{T}$. Moreover
    \[
        \sqrt{T}e_{k} = \sqrt{\lambda_{k}}e_{k}
    \]

    for every $k\in\{1,\ldots,\dim V\}$. Hence
    \[
        \det\sqrt{T} = \sqrt{\lambda_{1}}\cdots\sqrt{\lambda_{\dim V}} = \sqrt{\lambda_{1}\cdots\lambda_{\dim V}} = \sqrt{\det T}.
    \]

    Thus $\det\sqrt{T} = \sqrt{\det T}$.
\end{proof}
\newpage

% chapter9:sectionC:exercise15
\begin{exercise}\label{chapter9:sectionC:exercise15}
    Suppose $V$ is an inner product space and $T\in\lmap{V}$. Use the polar decomposition to give a proof that
    \[
        \abs{\det T} = \sqrt{\det(T^{*}T)}
    \]

    that is different from the proof given earlier (see 9.60).
\end{exercise}

\begin{proof}
    By the polar decomposition, there exists an unitary operator $S\in\lmap{V}$ such that $T = S\sqrt{T^{*}T}$. Since $\abs{\det S} = 1$, we have
    \[
        \abs{\det T} = \abs{\det S\sqrt{T^{*}T}} = \abs{(\det S)(\det\sqrt{T^{*}T})} = \abs{\det S}\abs{\det\sqrt{T^{*}T}} = \abs{\det\sqrt{T^{*}T}}.
    \]

    $T^{*}T$ is a positive operator on $V$. By Exercise~\ref{chapter9:sectionC:exercise14}, $\det\sqrt{T^{*}T} = \sqrt{\det(T^{*}T)}$, we obtain
    \[
        \abs{\det T} = \sqrt{\det(T^{*}T)}.
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise16
\begin{exercise}\label{chapter9:sectionC:exercise16}
    Suppose $T\in\lmap{V}$. Define $g: \mathbb{F}\to\mathbb{F}$ by $g(x) = \det(I + xT)$. Show that $g'(0) = \operatorname{tr}T$.
\end{exercise}

\begin{quote}
    Look for a clean solution to this exercise, without using the explicit but complicated formula for the determinant of a matrix.
\end{quote}

\begin{proof}
    $g$ is a polynomial function, so $g$ is differentiable.

    Let $n = \dim V$ and $e_{1}, \ldots, e_{n}$ be a basis of $V$. Let $\alpha$ be an alternating multilinear form in $V^{(\dim V)}_{\text{alt}}$ such that $\alpha(e_{1}, \ldots, e_{n}) = 1$.
    \begin{align*}
        \det(I + xT) & = (\det(I + xT))\alpha(e_{1}, \ldots, e_{n})                                                                        \\
                     & = \alpha(e_{1} + xTe_{1}, \ldots, e_{n} + xTe_{n})                                                                  \\
                     & = \alpha(e_{1}, \ldots, e_{n}) + x\alpha_{1}(e_{1}, \ldots, e_{n}) + \cdots + x^{n}\alpha_{n}(e_{1}, \ldots, e_{n}) \\
                     & = 1 + x\alpha_{1}(e_{1}, \ldots, e_{n}) + \cdots + x^{n}\alpha_{n}(e_{1}, \ldots, e_{n})
    \end{align*}

    where $\alpha_{k}(e_{1}, \ldots, e_{n})$ is defined as follows: $\operatorname{comb}(n, k)$ is the set of subsets of $k$ elements from $\{1,\ldots,n\}$
    \[
        \alpha_{k}(e_{1}, \ldots, e_{n}) = \sum_{(i_{1}, \ldots, i_{k})\in\operatorname{comb}(n,k)}\alpha(v_{1}, \ldots, v_{n})
    \]

    where $v_{j} = Te_{j}$ if $j\in\{ i_{1}, \ldots, i_{k} \}$, and otherwise, $v_{j} = e_{j}$.

    Therefore
    \begin{multline*}
        \det(I + xT) - 1 \\
        = x(\alpha(Te_{1}, \ldots, e_{n}) + \cdots + \alpha(e_{1}, \ldots, Te_{n})) + x^{2}\alpha_{2}(e_{1}, \ldots, e_{n}) + \cdots + x^{n}\alpha_{n}(e_{1}, \ldots, e_{n}).
    \end{multline*}

    Let $A$ be the matrix of $T$ with respect to $e_{1}, \ldots, e_{n}$, then
    \[
        Te_{j} = A_{1,j}e_{1} + \cdots + A_{n,j}e_{n}.
    \]

    Because $\alpha$ is an alternating multilinear form,
    \begin{align*}
        \alpha(Te_{1}, \ldots, e_{n}) + \cdots + \alpha(e_{1}, \ldots, Te_{n}) & = \alpha(A_{1,1}e_{1}, \ldots, e_{n}) + \cdots + \alpha(e_{1}, \ldots, A_{n,n}e_{n}) \\
                                                                               & = (A_{1,1} + \cdots + A_{n,n})\alpha(e_{1}, \ldots, e_{n})                           \\
                                                                               & = \operatorname{tr}T.
    \end{align*}

    Therefore
    \[
        \frac{\det(I + xT) - 1}{x} = \operatorname{tr}T + x\alpha_{2}(e_{1}, \ldots, e_{n}) + \cdots + x^{n-1}\alpha_{n}(e_{1}, \ldots,e_{n})
    \]

    so
    \[
        \lim\limits_{x\to 0}\frac{\det(I + xT) - 1}{x} = \operatorname{tr}T.
    \]

    Equivalently, $g'(0) = \operatorname{tr}T$.
\end{proof}
\newpage

% chapter9:sectionC:exercise17
\begin{exercise}\label{chapter9:sectionC:exercise17}
    Suppose $a, b, c$ are positive numbers. Find the volume of the ellipsoid
    \[
        \left\{ (x,y,z)\in\mathbb{R}^{3}: \frac{x^{2}}{a^{2}} + \frac{y^{2}}{b^{2}} + \frac{z^{2}}{c^{2}} < 1 \right\}
    \]

    by finding a set $\Omega\subseteq\mathbb{R}^{3}$ whose volume you know and an operator $T$ on $\mathbb{R}^{3}$ such that $T(\Omega)$ equals the ellipsoid above.
\end{exercise}

\begin{proof}
    Let $T$ be the operator on $\mathbb{R}^{3}$ defined by $T(x, y, z) = \left(\frac{x}{a}, \frac{y}{b}, \frac{z}{c}\right)$.

    \[
        (x, y, z)\in \left\{ (x,y,z)\in\mathbb{R}^{3}: \frac{x^{2}}{a^{2}} + \frac{y^{2}}{b^{2}} + \frac{z^{2}}{c^{2}} < 1 \right\}
    \]

    if and only if
    \[
        T(x, y, z) = \left(\frac{x}{a}, \frac{y}{b}, \frac{z}{c}\right)\in \Omega = \left\{ (x,y,z)\in\mathbb{R}^{3}: x^{2} + y^{2} + z^{2} < 1 \right\}.
    \]

    The volume of the ellipsoid (in fact, this is a sphere)
    \[
        \Omega = \left\{ (x,y,z)\in\mathbb{R}^{3}: x^{2} + y^{2} + z^{2} < 1 \right\}
    \]

    is $\frac{4}{3}\pi$. On the other hand
    \[
        \abs{T^{-1}(\Omega)} = \abs{\det T^{-1}}\abs{\Omega}
    \]

    and $\abs{\Omega} = \frac{4}{3}\pi$, $\det T^{-1} = abc$. Thus the volume of the given ellipsoid is $\frac{4}{3}\pi abc$.
\end{proof}
\newpage

% chapter9:sectionC:exercise18
\begin{exercise}\label{chapter9:sectionC:exercise18}
    Suppose that $A$ is an invertible square matrix. Prove that Hadamard's inequality (9.66) is an equality if and only if each column of $A$ is orthogonal to the other columns.
\end{exercise}

\begin{proof}
    I rewrite the proof of the Hadamard's inequality here.

    Let $n$ be the number of columns of $A$. $v_{1}, \ldots, v_{n} \in \mathbb{F}^{n}$ are the columns of $A$.

    If $A$ is not invertible, then $\abs{\det A} = 0 \leq \norm{v_{1}}\cdots \norm{v_{n}}$. Suppose $A$ is invertible. By the QR decomposition, there exists an unitary matrix $Q$ and an upper-triangular matrix $R$ whose entries on the diagonal are positive numbers such that $A = QR$.
    \begin{align*}
        \abs{\det A} & = \abs{\det Q}\abs{\det R}             \\
                     & = \abs{\det R}                         \\
                     & = \prod^{n}_{k=1}R_{k,k}               \\
                     & \leq \prod^{n}_{k=1}\norm{R_{\cdot,k}} \\
                     & = \prod^{n}_{k=1}\norm{QR_{\cdot,k}}   \\
                     & = \prod^{n}_{k=1}\norm{v_{k}}.
    \end{align*}

    Assume that $\abs{\det A} = \prod^{n}_{k=1}\norm{v_{k}}$.

    If $A$ is not invertible, then $\det A = 0$ and $\prod^{n}_{k=1}\norm{v_{k}} = 0$. $\prod^{n}_{k=1}\norm{v_{k}} = 0$ if and only if $v_{1} = \cdots = v_{n} = 0$, which implies $A = 0$. Therefore the columns of $A$ are pairwise orthogonal.

    If $A$ is invertible, the proof implies
    \[
        \prod^{n}_{k=1}R_{k,k} = \prod^{n}_{k=1}\norm{R_{\cdot,k}}.
    \]

    This happens if and only if $R$ is a diagonal matrix. Since $R$ is a diagonal matrix, the $k$th column of $A$ is $Q_{\cdot,k}R_{k,k}$. Because the columns of $Q$ are pairwise orthogonal, then $Q_{\cdot,1}R_{1,1}, \ldots, Q_{\cdot,n}R_{n,n}$ are pairwise orthogonal. Thus the columns of $A$ are pairwise orthogonal.

    \bigskip
    Assume that the columns of $A$ are pairwise orthogonal.

    If $A$ is not invertible, then the columns of $A$ are linearly dependent. Moreover, the columns of $A$ are pairwise orthogonal so there must be at least one column consisting of all $0$ (because otherwise, they are linearly independent). Therefore $\abs{\det A} = 0 = \prod^{n}_{k=1}\norm{v_{k}}$.

    If $A$ is invertible, all columns of $A$ are nonzero vectors of $\mathbb{F}^{n}$, so
    \[
        A = \prod^{n}_{k=1}\norm{v_{k}}\begin{pmatrix}
            A_{1,1}/\norm{v_{1}} & \cdots & A_{1,n}/\norm{v_{n}} \\
            \vdots               &        & \vdots               \\
            A_{n,1}/\norm{v_{1}} & \cdots & A_{n,n}/\norm{v_{n}}
        \end{pmatrix}
    \]

    where the columns of
    \[
        \begin{pmatrix}
            A_{1,1}/\norm{v_{1}} & \cdots & A_{1,n}/\norm{v_{n}} \\
            \vdots               &        & \vdots               \\
            A_{n,1}/\norm{v_{1}} & \cdots & A_{n,n}/\norm{v_{n}}
        \end{pmatrix}
    \]

    constitute an orthonormal basis of $\mathbb{F}^{n}$, which means this matrix is unitary. Therefore $\abs{\det A} = \prod^{n}_{k=1}\norm{v_{k}}$.

    Thus the Hadamard's inequality is an equality if and only if the columns of $A$ are pairwise orthogonal.
\end{proof}
\newpage

% chapter9:sectionC:exercise19
\begin{exercise}\label{chapter9:sectionC:exercise19}
    Suppose $V$ is an inner product space, $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$, and $T\in\lmap{V}$ is a positive operator.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $\det T\leq \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}$.
        \item Prove that if $T$ is invertible, then the inequality in (a) is an equality if and only if $e_{k}$ is an eigenvector of $T$ for each $k = 1,\ldots,n$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $A$ be the matrix of $\sqrt{T}$ with respect to the basis $e_{1}, \ldots, e_{n}$ of $V$, then the $k$th column $v_{k}$ of $A$ is the coordinates of $\sqrt{T}e_{k}$ with respect to $e_{1}, \ldots, e_{n}$. By the Pythagorean theorem and the Parseval's identity (we use the standard inner products on $\mathbb{F}^{n}$ and the inner product on $V$),
              \begin{align*}
                  \norm{v_{k}}^{2} = \abs{A_{1,k}}^{2} + \cdots + \abs{A_{n,k}}^{2} = \abs{\innerprod{\sqrt{T}e_{k}, e_{1}}}^{2} + \cdots +\abs{\innerprod{\sqrt{T}e_{k}, e_{n}}}^{2} = \norm{\sqrt{T}e_{k}}^{2}.
              \end{align*}

              for every $k\in\{1,\ldots,n\}$.
              \begin{align*}
                  \det T & = {(\det \sqrt{T})}^{2}                                   & \text{(Exercise~\ref{chapter9:sectionC:exercise14})} \\
                         & \leq \prod^{n}_{k=1}\norm{v_{k}}^{2}                      & \text{(Hadamard's inequality)}                       \\
                         & = \prod^{n}_{k=1}\norm{v_{k}}^{2}                                                                                \\
                         & = \prod^{n}_{k=1}\innerprod{\sqrt{T}e_{k}, \sqrt{T}e_{k}}                                                        \\
                         & = \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}                & \text{($T$ is a positive operator)}
              \end{align*}

              Thus $\det T\leq \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}$.
        \item According to part (a) and the equality condition of Hadamard's inequality (Exercise~\ref{chapter9:sectionC:exercise18}), when $T$ is invertible, the inequality in (a) is an equality if and only if the columns of $\mathcal{M}(\sqrt{T}, (e_{1}, \ldots, e_{n}))$ are pairwise orthogonal.


              Suppose (a) is an equality and $T$ is invertible.

              The columns of $\mathcal{M}(\sqrt{T}, (e_{1}, \ldots, e_{n}))$ are pairwise orthogonal if and only if $\innerprod{\sqrt{T}e_{k}, \sqrt{T}e_{j}} = 0$ for all $j\ne k$. Equivalently, $\innerprod{Te_{k}, e_{j}} = 0$ for all $j\ne k$, this means the matrix of $T$ with respect to $e_{1}, \ldots, e_{n}$ is a diagonal matrix. Therefore $e_{1}, \ldots, e_{n}$ are eigenvectors of $T$.

              Suppose $e_{1}, \ldots, e_{n}$ are eigenvectors of $T$. Let $\lambda_{k}$ be the eigenvalue of $T$ corresponding to $e_{k}$ for every $k\in\{1,\ldots,n\}$, then
              \[
                  \det T = \prod^{n}_{k=1}\lambda_{k} = \prod^{n}_{k=1}\innerprod{Te_{k}, e_{k}}.
              \]

              Thus if $T$ is invertible, the inequality in (a) is an equality if and only if $e_{1}, \ldots, e_{n}$ are eigenvectors of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter9:sectionC:exercise20
\begin{exercise}\label{chapter9:sectionC:exercise20}
    Suppose $A$ is an $n$-by-$n$ matrix, and suppose $c$ is such that $\abs{A_{j,k}}\leq c$ for all $j, k\in\{1,\ldots,n\}$. Prove that
    \[
        \abs{\det A}\leq c^{n}n^{n/2}.
    \]
\end{exercise}

\begin{quote}
    The formula for the determinant of a matrix (9.46) shows that $\abs{\det A}\leq c^{n}n{!}$. However, the estimate given by this exercise is much better. For example, if $c = 1$ and $n = 100$, then $c^{n}n! \approx 10^{158}$, but the estimate given by this exercise is the much smaller number $10^{100}$. If $n$ is an integer power of $2$, then the inequality above is sharp and cannot be improved.
\end{quote}

\begin{proof}
    Let $v_{1}, \ldots, v_{n}\in\mathbb{F}^{n}$ be the columns of $A$. By the Hadamard's inequality
    \[
        \abs{\det A}\leq \prod^{n}_{k=1}\norm{v_{k}}.
    \]

    For every $k\in\{1,\ldots,n\}$
    \[
        \norm{v_{k}}^{2} = \abs{A_{1,k}}^{2} + \cdots + \abs{A_{n,k}}^{2} \leq nc^{2}.
    \]

    Therefore
    \[
        \abs{\det A}\leq {(c\sqrt{n})}^{n} = c^{n}n^{n/2}.
    \]
\end{proof}
\newpage

% chapter9:sectionC:exercise21
\begin{exercise}\label{chapter9:sectionC:exercise21}
    Suppose $n$ is a positive integer and $\delta: \mathbb{C}^{n,n}\to \mathbb{C}$ is a function such that
    \[
        \delta(AB) = \delta(A)\cdot\delta(B)
    \]

    for all $A, B\in\mathbb{C}^{n,n}$ and $\delta(A)$ equals the product of the diagonal entries of $A$ for each diagonal matrix $A\in\mathbb{C}^{n,n}$. Prove that
    \[
        \delta(A) = \det A
    \]

    for all $A\in\mathbb{C}^{n,n}$.
\end{exercise}

\begin{quote}
    Recall that $\mathbb{C}^{n,n}$ denotes set of $n$-by-$n$ matrices with entries in $\mathbb{C}$. This exercise shows that the determinant is the unique function defined on square matrices that is multiplicative and has the desired behavior on diagonal matrices. This result is analogous to Exercise~\ref{chapter8:sectionD:exercise10}, which shows that the trace is uniquely determined by its algebraic properties.
\end{quote}

\begin{proof}
    If $A$ is invertible, then $\delta(A)\delta(A^{-1}) = \delta(I) = 1\ne 0$, so $\delta(A)\ne 0$.

    If $A$ is not invertible, then there exists $v_{1}\in\mathbb{C}^{n,1}$ and $v_{1}\ne 0$ such that $Av_{1} = 0$. Extend $v_{1}$ to a basis $v_{1}, \ldots, v_{n}$ of $\mathbb{C}^{n,1}$, let $B$ be a matrix whose $k$th column is $v_{k}$, then
    \[
        AB = \begin{pmatrix}
            0      & *      & \cdots & *      \\
            0      & *      & \cdots & *      \\
            \vdots & \vdots &        & \vdots \\
            0      & *      & \cdots & *
        \end{pmatrix}
        =
        \begin{pmatrix}
            0      & *      & \cdots & *      \\
            0      & *      & \cdots & *      \\
            \vdots & \vdots &        & \vdots \\
            0      & *      & \cdots & *
        \end{pmatrix}
        \begin{pmatrix}
            0      & 0      & \cdots & 0      \\
            0      & 1      & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & 1
        \end{pmatrix}.
    \]

    Therefore $\delta(A)\delta(B) = \delta(AB) = 0$. Because $\delta(B)\ne 0$ (since $B$ is invertible), it follows that $\delta(A) = 0$. Hence $\delta(A) = 0$ if $A$ is not invertible.

    By the Schur's decomposition theorem, for each $A\in\mathbb{C}^{n,n}$ there exists an unitary matrix $Q$ such that $Q^{*}AQ$ is an upper-triangular matrix.
    \[
        \delta(Q^{*}AQ) = \delta(Q^{*})\delta(A)\delta(Q) = \delta(Q^{*})\delta(Q)\delta(A) = \delta(Q^{*}Q)\delta(A) = \delta(I)\delta(A) = \delta(A).
    \]

    Suppose $A$ is not invertible, then $\delta(A) = 0 = \det A$.

    Suppose $A$ is invertible, then all entries on the diagonal of $Q^{*}AQ$ are nonzero. Let $U = Q^{*}AQ$ and choose nonzero complex numbers $\lambda_{1}, \ldots, \lambda_{n}$ such that $U_{1,1}/\lambda_{1}, \ldots, U_{n,n}/\lambda_{n}$ are pairwise distinct, then
    \[
        \begin{pmatrix}
            U_{1,1} & U_{1,2} & \cdots & U_{1,n} \\
            0       & U_{2,2} & \cdots & U_{2,n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & 0       & \cdots & U_{n,n}
        \end{pmatrix} =
        \begin{pmatrix}
            U_{1,1}/\lambda_{1} & U_{1,2}/\lambda_{2} & \cdots & U_{1,n}/\lambda_{n} \\
            0                   & U_{2,2}/\lambda_{2} & \cdots & U_{2,n}/\lambda_{n} \\
            \vdots              & \vdots              &        & \vdots              \\
            0                   & 0                   & \cdots & U_{n,n}/\lambda_{n}
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_{1} & 0           & \cdots & 0           \\
            0           & \lambda_{2} & \cdots & 0           \\
            \vdots      & \vdots      &        & \vdots      \\
            0           & 0           & \cdots & \lambda_{n}
        \end{pmatrix}.
    \]

    The upper-triangular matrix
    \[
        W = \begin{pmatrix}
            U_{1,1}/\lambda_{1} & U_{1,2}/\lambda_{2} & \cdots & U_{1,n}/\lambda_{n} \\
            0                   & U_{2,2}/\lambda_{2} & \cdots & U_{2,n}/\lambda_{n} \\
            \vdots              & \vdots              &        & \vdots              \\
            0                   & 0                   & \cdots & U_{n,n}/\lambda_{n}
        \end{pmatrix}
    \]

    has distinct entries on the diagonal, so it has distinct $n$ eigenvalues (which are precisely the entries on the diagonal). Therefore it is diagonalizable, hence there exists an invertible matrix $C$ such that $C^{-1}WC$ is a diagonal matrix.
    \[
        \delta(C^{-1}WC) = \delta(C^{-1})\delta(W)\delta(C) = \delta(C^{-1})\delta(C)\delta(W) = \delta(C^{-1}C)\delta(W) = \delta(W).
    \]

    Hence $\delta(U) = \delta(W)\lambda_{1}\cdots\lambda_{n} = U_{1,1}\cdots U_{n,n}$. Moreover, $U_{1,1}, \ldots, U_{n,n}$ are also eigenvalues of $U$, each appears as many times as its multiplicity, so $\det U =  U_{1,1}\cdots U_{n,n}$. Therefore
    \[
        \delta(A) = \delta(Q^{*}AQ) = \delta(U) = U_{1,1}\cdots U_{n,n} = \det U = \det (Q^{*}AQ) = \det A.
    \]

    Thus for all $A\in\mathbb{C}^{n,n}$, $\delta(A) = \det A$.
\end{proof}
\newpage

\section{Tensor Products}

