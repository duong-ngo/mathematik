\chapter{Multilinear Algebra and Determinants}

\section{Bilinear Forms and Quadratic Forms}

% chapter9:sectionA:exercise1
\begin{exercise}\label{chapter9:sectionA:exercise1}
    Prove that if $\beta$ is a bilinear form on $\mathbb{F}$, then there exists $c\in\mathbb{F}$ such that
    \[
        \beta(x, y) = cxy
    \]

    for all $x, y\in\mathbb{F}$.
\end{exercise}

\begin{proof}
    For all $x, y\in\mathbb{F}$
    \[
        \beta(x, y) = \beta(x\cdot 1, y\cdot 1) = x\beta(1, y\cdot 1) = xy\beta(1,1).
    \]

    Let $c = \beta(1, 1)$, then $\beta(x, y) = cxy$ for every $x, y\in\mathbb{F}$.
\end{proof}
\newpage

% chapter9:sectionA:exercise2
\begin{exercise}\label{chapter9:sectionA:exercise2}
    Let $n = \dim V$. Suppose $\beta$ is a bilinear form on $V$. Prove that there exist $\varphi_{1}, \ldots, \varphi_{n}, \tau_{1}, \ldots, \tau_{n}\in V'$ such that
    \[
        \beta(u, v) = \varphi_{1}(u)\cdot\tau_{1}(v) + \cdots + \varphi_{n}(u)\cdot\tau_{n}(v)
    \]

    for all $u, v\in V$.
\end{exercise}

\begin{quote}
    This exercise shows that if $n = \dim V$, then every bilinear form on V is of
    the form given by the last bullet point of Example 9.2.
\end{quote}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be a basis of $V$. For every vector $v$ in $V$, there exist unique scalars $a_{1}, \ldots, a_{n}$ such that
    \[
        v = a_{1}e_{1} + \cdots + a_{n}e_{n}.
    \]

    For every $k\in\{1,\ldots,n\}$, the mappings
    \begin{align*}
        \tau_{k}    & : v \mapsto a_{k}           \\
        \varphi_{k} & : v \mapsto \beta(v, e_{k})
    \end{align*}

    are linear functionals on $V$. From these, we have
    \begin{align*}
        \beta(u, v) & = \beta(u, \tau_{1}(v)e_{1} + \cdots + \tau_{n}(v)e_{n})                     \\
                    & = \beta(u, e_{1})\cdot\tau_{1}(v) + \cdots + \beta(u, e_{n})\cdot\tau_{n}(v) \\
                    & = \varphi_{1}(u)\cdot\tau_{1}(v) + \cdots + \varphi_{n}(u)\cdot\tau_{n}(v)
    \end{align*}

    for all $u, v\in V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise3
\begin{exercise}\label{chapter9:sectionA:exercise3}
    Suppose $\beta: V\times V\to\mathbb{F}$ is a bilinear form on $V$ and also is a linear functional on $V\times V$. Prove that $\beta = 0$.
\end{exercise}

\begin{proof}
    For every $u, v\in V$
    \begin{align*}
        \beta(u, v) & = \beta(u + 0, v)                                                                   \\
                    & = \beta(u, v) + \beta(0, v)               & \text{($\beta$ is a bilinear form)}     \\
                    & = \beta(u, v + 0) + \beta(0, v)                                                     \\
                    & = \beta(u, v) + \beta(u, 0) + \beta(0, v) & \text{($\beta$ is a bilinear form)}     \\
                    & = \beta(u, v) + \beta(u, v)               & \text{($\beta$ is a linear functional)}
    \end{align*}

    so $\beta(u, v) = \beta(u, v) + (-\beta(u, v)) = 0$. Hence $\beta = 0$.
\end{proof}
\newpage

% chapter9:sectionA:exercise4
\begin{exercise}\label{chapter9:sectionA:exercise4}
    Suppose $V$ is a real inner product space and $\beta$ is a bilinear form on $V$. Show that there exists a unique operator $T\in\lmap{V}$ such that
    \[
        \beta(u, v) = \innerprod{u, Tv}
    \]

    for all $u, v\in V$.
\end{exercise}

\begin{quote}
    This exercise states that if $V$ is a real inner product space, then every bilinear form on $V$ is of the form given by the third bullet point in 9.2.
\end{quote}

\begin{proof}
    $n = \dim V$. Let $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. For every $u, v\in V$
    \begin{align*}
        \beta(u, v) & = \beta(\innerprod{u,e_{1}}e_{1} + \cdots + \innerprod{u,e_{n}}e_{n}, v)            \\
                    & = \innerprod{u,e_{1}}\beta(e_{1}, v) + \cdots + \innerprod{u,e_{n}}\beta(e_{n}, v).
    \end{align*}

    Let's define an operator $T$ on $V$ as follows:
    \[
        Tv = \beta(e_{1}, v)e_{1} + \cdots + \beta(e_{n}, v)e_{n}.
    \]

    Hence for every $u, v\in V$, $\beta(u, v) = \innerprod{u, Tv}$.

    Assume operators $S, T\in\lmap{V}$ satisfy $\beta(u, v) = \innerprod{u, Sv} = \innerprod{u, Tv}$ for every $u, v\in V$. It follows that for every $v\in V$, for every $u\in V$, $\innerprod{u, Sv - Tv} = 0$, so $Sv = Tv$ for every $v\in V$. Therefore $S = T$.

    Thus there exists a unique operator $T$ on $V$ such that $\beta(u, v) = \innerprod{u, Tv}$ for all $u, v\in V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise5
\begin{exercise}\label{chapter9:sectionA:exercise5}
    Suppose $\beta$ is a bilinear form on a real inner product space $V$ and $T$ is the unique operator on $V$ such that $\beta(u, v) = \innerprod{u, Tv}$ for all $u, v \in V$ (see Exercise~\ref{chapter9:sectionA:exercise4}). Show that $\beta$ is an inner product on $V$ if and only if $T$ is an invertible positive operator on $V$.
\end{exercise}

\begin{proof}
    $\beta$ is a bilinear form on a real inner product space $V$. Therefore $\beta$ is also an inner product on $V$ if and only if $\beta$ is conjugate symmetric and positive definite.

    $(\Rightarrow)$ $\beta$ is an inner product on $V$.

    $\beta$ is conjugate symmetric, so $\beta$ is symmetric, since $\mathbb{F} = \mathbb{R}$. For every $u, v\in V$
    \[
        \innerprod{u, Tv} = \beta(u, v) = \beta(v, u) = \innerprod{v, Tu} = \innerprod{Tu, v} = \innerprod{u, T^{*}v}.
    \]

    So $Tv = T^{*}v$ for every $v\in V$, which implies $T$ is self-adjoint.

    $\innerprod{Tv, v} = \innerprod{v, Tv} = \beta(v, v) > 0$ for every nonzero $v\in V$, and $\beta(0, 0) = 0$.

    Hence $T$ is an invertible positive operator on $V$.

    \bigskip
    $(\Leftarrow)$ $T$ is an invertible positive operator on $V$.

    So $T$ is self-adjoint. For every $u, v\in V$
    \[
        \beta(v, u) = \innerprod{v, Tu} = \conj{\innerprod{Tu, v}} = \conj{\innerprod{u, T^{*}v}} = \conj{\innerprod{u, Tv}} = \conj{\beta(u, v)}
    \]

    which means $\beta$ is conjugate symmetric.

    For every $v\in V$
    \[
        \beta(v, v) = \innerprod{v, Tv} = \innerprod{Tv, v} \geq 0
    \]

    because $T$ is positive, so $\beta$ is positive. Moreover, $\beta(v, v) = 0$ if and only if $\innerprod{Tv, v} = 0$. $\innerprod{Tv, v} = 0$ if and only if $v = 0$ because $T$ is an invertible positive operator. Therefore $\beta$ is positive definite.

    Hence $\beta$ is an inner product on $V$.

    \bigskip
    Thus $\beta$ is an inner product on $V$ if and only if $T$ is an invertible positive operator on $V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise6
\begin{exercise}\label{chapter9:sectionA:exercise6}
    Prove or give a counterexample: If $\rho$ is a symmetric bilinear form on $V$, then
    \[
        \{ v\in V : \rho(v, v) = 0 \}
    \]

    is a subspace of $V$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V = \mathbb{R}^{2}$ and
    \[
        \rho((x_{1}, x_{2}), (y_{1}, y_{2})) = x_{1}y_{1} - 2x_{1}y_{2} - 2x_{2}y_{1} + 3x_{2}y_{2}
    \]

    so $\rho$ is a symmetric bilinear form on $\mathbb{R}^{2}$. $(1, 1)$ and $(1, 3)$ are in $\{ v\in V : \rho(v, v) = 0 \}$.
    \[
        \rho((1, 1), (1, 1)) = \rho((3, 1), (3, 1)) = 0.
    \]

    However,
    \[
        \rho((4, 2), (4, 2)) = 4^{2} - 4\cdot 4\cdot 2 + 3\cdot 2^{2} = 3\ne 0
    \]

    so $(4, 2)$ is not in $\{ v\in V : \rho(v, v) = 0 \}$. So $\{ v\in V : \rho(v, v) = 0 \}$ is not closed under addition, which means it is not a subspace of $\mathbb{R}^{2}$.
\end{proof}
\newpage

% chapter9:sectionA:exercise7
\begin{exercise}\label{chapter9:sectionA:exercise7}
    Explain why the proof of 9.13 (diagonalization of a symmetric bilinear form by an orthonormal basis on a real inner product space) fails if the hypothesis that $\mathbb{F} = \mathbb{R}$ is dropped.
\end{exercise}

\begin{proof}
    I give a counterexample where $\mathbb{F} = \mathbb{C}$.

    Let $\rho$ be a symmetric bilinear form on a complex inner product space $\mathbb{C}^{2}$ whose matrix (with respect to the standard basis of $\mathbb{C}^{2}$) is
    \[
        \begin{pmatrix}
            1     & \iota \\
            \iota & 0
        \end{pmatrix}.
    \]

    Let $T$ be an operator on $\mathbb{C}^{2}$ whose matrix with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}1 & \iota \\ \iota & 0 \end{pmatrix}$. The matrix of $T^{*}$ with respect to the standard basis of $\mathbb{C}^{2}$ is
    \[
        \begin{pmatrix}
            1      & -\iota \\
            -\iota & 0
        \end{pmatrix}.
    \]

    However, $T$ is not a normal operator, because
    \[
        \mathcal{M}(T)\mathcal{M}(T^{*}) =
        \begin{pmatrix}
            2 & -\iota \\
            0 & 1
        \end{pmatrix} \ne
        \begin{pmatrix}
            2 & \iota \\
            0 & 1
        \end{pmatrix} = \mathcal{M}(T^{*})\mathcal{M}(T).
    \]

    By the complex spectral theorem, there exists not exist an orthonormal basis of $\mathbb{C}^{2}$ to which $T$ has a diagonal matrix. Hence $\rho$ is not diagonalizable by an orthonormal basis.
\end{proof}
\newpage

% chapter9:sectionA:exercise8
\begin{exercise}\label{chapter9:sectionA:exercise8}
    Find formulas for $\dim V_{\text{sym}}^{(2)}$ and $\dim V_{\text{alt}}^{(2)}$ in terms of $\dim V$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be a basis of $V$.

    $V^{(2)}$ is isomorphic to $\mathbb{F}^{\dim V,\dim V}$, where each bilinear form on $V$ corresponds to its matrix with respect to $e_{1}, \ldots, e_{\dim V}$.

    A symmetric bilinear form corresponds to a symmetric matrix, and the vector space of symmetric matrices with $\dim V$ columns has dimension $(\dim V)\times(\dim V + 1)/2$. Therefore
    \[
        \dim V^{(2)}_{\text{sym}} = \frac{(\dim V) \times (\dim V + 1)}{2}.
    \]

    Moreover, $\dim V^{(2)} = {(\dim V)}^{2}$ and $V^{(2)} = V^{(2)}_{\text{sym}}\oplus V^{(2)}_{\text{alt}}$ so
    \[
        \dim V^{(2)}_{\text{alt}} = {(\dim V)}^{2} - \frac{(\dim V) \times (\dim V + 1)}{2} = \frac{(\dim V)\times (\dim V - 1)}{2}.
    \]

    Thus $\dim V^{(2)}_{\text{sym}} = (\dim V)\times(\dim V + 1)/2$ and $\dim V^{(2)}_{\text{alt}} = (\dim V)\times(\dim V - 1)/2$.
\end{proof}
\newpage

% chapter9:sectionA:exercise9
\begin{exercise}\label{chapter9:sectionA:exercise9}
    Suppose that $n$ is a positive integer and $V = \{ p\in\mathscr{P}_{n}(\mathbb{R}): p(0) = p(1) \}$. Define $\alpha: V\times V\to\mathbb{R}$ by
    \[
        \alpha(p, q) = \int^{1}_{0}pq'.
    \]

    Show that $\alpha$ is an alternating bilinear form on $V$.
\end{exercise}

\begin{proof}
    For every $p_{1}, p_{2}, q \in V$
    \[
        \alpha(p_{1} + p_{2}, q) = \int^{1}_{0}(p_{1} + p_{2})q' = \int^{1}_{0}p_{1}q' + \int^{1}_{0}p_{2}q' = \alpha(p_{1}, q) + \alpha(p_{2}, q).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(\lambda p, q) = \int^{1}_{0}(\lambda p)q' = \lambda\int^{1}_{0}pq' = \lambda\alpha(p, q).
    \]

    For every $p, q_{1}, q_{2}\in V$
    \[
        \alpha(p, q_{1} + q_{2}) = \int^{1}_{0}p(q_{1} + q_{2})' = \int^{1}_{0}pq_{1}' + \int^{1}_{0}pq_{2}' = \alpha(p, q_{1}) + \alpha(p, q_{2}).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(p, \lambda q) = \int^{1}_{0}p(\lambda q)' = \lambda\int^{1}_{0}pq' = \lambda\alpha(p, q).
    \]

    So $\alpha$ is a bilinear form on $V$.

    For every $p\in V$
    \[
        \alpha(p, p) = \int^{1}_{0}pp' = \int^{p(1)}_{p(0)}pdp = \frac{{(p(1))}^{2} - {(p(0))}^{2}}{2} = 0.
    \]

    Hence $\alpha$ is an alternating bilinear form on $V$.
\end{proof}
\newpage

% chapter9:sectionA:exercise10
\begin{exercise}\label{chapter9:sectionA:exercise10}
    Suppose that $n$ is a positive integer and
    \[
        V = \{ p\in\mathscr{P}_{n}(\mathbb{R}): p(0) = p(1) \land p'(0) = p'(1) \}.
    \]

    Define $\rho: V\times V\to\mathbb{R}$ by
    \[
        \rho(p, q) = \int^{1}_{0}pq''.
    \]

    Show that $\rho$ is a symmetric bilinear form on $V$.
\end{exercise}

\begin{proof}
    For every $p_{1}, p_{2}, q \in V$
    \[
        \alpha(p_{1} + p_{2}, q) = \int^{1}_{0}(p_{1} + p_{2})q'' = \int^{1}_{0}p_{1}q'' + \int^{1}_{0}p_{2}q'' = \alpha(p_{1}, q) + \alpha(p_{2}, q).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(\lambda p, q) = \int^{1}_{0}(\lambda p)q'' = \lambda\int^{1}_{0}pq'' = \lambda\alpha(p, q).
    \]

    For every $p, q_{1}, q_{2}\in V$
    \[
        \alpha(p, q_{1} + q_{2}) = \int^{1}_{0}p(q_{1} + q_{2})'' = \int^{1}_{0}pq_{1}'' + \int^{1}_{0}pq_{2}'' = \alpha(p, q_{1}) + \alpha(p, q_{2}).
    \]

    For every $p, q\in V$ and $\lambda\in\mathbb{R}$
    \[
        \alpha(p, \lambda q) = \int^{1}_{0}p(\lambda q)'' = \lambda\int^{1}_{0}pq'' = \lambda\alpha(p, q).
    \]

    So $\alpha$ is a bilinear form on $V$.

    For every $p, q\in V$
    \begin{align*}
        \alpha(p, q) & = \int^{1}_{0}pq'' = \int^{1}_{0}p(x)q''(x)dx         \\
                     & = \int^{x=1}_{x=0}p(x)dq'(x)                          \\
                     & = p(x)q'(x)\Big{\vert}^{x=1}_{x=0} - \int^{1}_{0}q'p' \\
                     & = -\int^{1}_{0}q'p'.
    \end{align*}

    Therefore $\alpha(q, p) = -\int^{1}_{0}p'q' = -\int^{1}_{0}q'p' = \alpha(p, q)$.

    Hence $\alpha$ is a symmetric bilinear form on $V$.
\end{proof}
\newpage

\section{Alternating Multilinear Forms}

% chapter9:sectionB:exercise1
\begin{exercise}\label{chapter9:sectionB:exercise1}
    Suppose $m$ is a positive integer. Show that $\dim V^{(m)} = {(\dim V)}^{m}$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{\dim V}$ be a basis of $V$ and $\varphi_{1}, \ldots, \varphi_{\dim V}$ be its dual basis.

    For each \textit{choices (duplication is allowed)}  of $m$ linear functionals from $\varphi_{1}, \ldots, \varphi_{\dim V}$, we define an $m$-linear form as follows, where $\varphi_{j_{1}}, \ldots, \varphi_{j_{m}}$ is the selected listed, $j_{1}, \ldots, j_{m}$ are from $1, \ldots, \dim V$
    \[
        \alpha_{j_{1},\ldots,j_{m}}: (v_{1}, \ldots, v_{m})\mapsto \varphi_{j_{1}}(v_{1})\cdots \varphi_{j_{m}}(v_{m})
    \]

    Assume that
    \[
        \sum x_{j_{1},\ldots,j_{m}}\alpha_{j_{1},\ldots,j_{m}} = 0
    \]

    for all $v_{1}, \ldots, v_{m}\in V$ and we use all previously defined $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$. Plug $(v_{j_{1}}, \ldots, v_{j_{m}})\in \underbrace{V\times\cdots\times V}_{m}$ in, we obtain
    \[
        x_{j_{1},\ldots, j_{m}} = 0.
    \]

    Hence all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ are linearly independent. Moreover, for every $m$-linear form $\alpha$ on $V$,
    \begin{align*}
        \alpha(v_{1}, \ldots, v_{m}) & = \alpha\left(\sum^{\dim V}_{j=1}\varphi_{j}(v_{1})e_{j}, \ldots, \sum^{\dim V}_{j=1}\varphi_{j}(v_{m})e_{j}\right)                                                     \\
                                     & = \sum^{\dim V}_{j_{m}=1}\left(\cdots\left(\sum^{\dim V}_{j_{1}=1}\alpha(e_{j_{1}}, \ldots, e_{j_{m}})\varphi_{j_{1}}(v_{1})\cdots\varphi_{j_{m}}(v_{m})\right)\right)  \\
                                     & = \sum^{\dim V}_{j_{m}=1}\left(\cdots\left(\sum^{\dim V}_{j_{1}=1}\alpha(e_{j_{1}}, \ldots, e_{j_{m}})\varphi_{j_{1},\ldots,j_{m}}(v_{1}, \ldots, v_{m})\right)\right).
    \end{align*}

    This means all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ spans $V^{(m)}$. Therefore all $m$-linear forms $\alpha_{j_{1},\ldots,j_{m}}$ constitute a basis of $V^{(m)}$ and $\dim V^{(m)} = {(\dim V)}^{m}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise2
\begin{exercise}\label{chapter9:sectionB:exercise2}
    Suppose $n\geq 3$ and $\alpha: \mathbb{F}^{n}\times\mathbb{F}^{n}\times\mathbb{F}^{n} \to \mathbb{F}$ is defined by
    \begin{gather*}
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) \\
        = x_{1}y_{2}z_{3} - x_{2}y_{1}z_{3} - x_{3}y_{2}z_{1} - x_{1}y_{3}z_{2} + x_{3}y_{1}z_{2} + x_{2}y_{3}z_{1}.
    \end{gather*}

    Show that $\alpha$ is an alternating $3$-linear form on $\mathbb{F}^{n}$.
\end{exercise}

\begin{proof}
    \begin{align*}
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = x_{1}(y_{2}z_{3} - y_{3}z_{2}) + x_{2}(y_{3}z_{1} - y_{1}z_{3}) + x_{3}(y_{1}z_{2} - y_{2}z_{1}) \\
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = y_{1}(z_{2}x_{3} - z_{3}x_{2}) + y_{2}(z_{3}x_{1} - z_{1}x_{3}) + y_{3}(z_{1}x_{2} - z_{2}x_{1}) \\
        \alpha((x_{1}, \ldots, x_{n}), (y_{1}, \ldots, y_{n}), (z_{1}, \ldots, z_{n})) & = z_{1}(x_{2}y_{3} - x_{3}y_{2}) + z_{2}(x_{3}y_{1} - x_{1}y_{3}) + z_{3}(x_{1}y_{2} - x_{2}y_{1})
    \end{align*}

    so $\alpha$ is a $3$-linear form on $\mathbb{F}^{n}$ and $\alpha$ is alternating.
\end{proof}
\newpage

% chapter9:sectionB:exercise3
\begin{exercise}\label{chapter9:sectionB:exercise3}
    Suppose $m$ is a positive integer and $\alpha$ is an $m$-linear form on $V$ such that $\alpha(v_{1}, \ldots, v_{m}) = 0$ whenever $v_{1}, \ldots, v_{m}$ is a list of vectors in $V$ with $v_{j} = v_{j+1}$ for some $j\in\{1,\ldots,m-1\}$. Prove that $\alpha$ is an alternating $m$-linear form on $V$.
\end{exercise}

\begin{proof}
    We will show that for every positive integer $k$ less than $m$, $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{j+k}$ for every $j\in\{1,\ldots,m-k\}$.

    The statement is true for $k = 1$ due to the hypothesis.

    Assume the statement is true for all $k < \ell$ where $j + \ell < m$. Suppose that $v_{j} = v_{j + \ell}$.

    Consider the expression $\alpha(\ldots, v_{j} + v_{j+k}, \ldots, v_{j+k} + v_{j}, \ldots)$ where $v_{j} + v_{j+k}$ is at the $j$th slot, and $v_{j+k} + v_{j}$ is at the ${(j+k)}$th slot, $k < \ell$.

    We prove the anti-symmetric property first. By the induction hypothesis, for every
    \[
        (\ldots, w_{j} + w_{j+k}, \ldots, w_{j+k} + w_{j}, \ldots)\in \underbrace{V\times\cdots\times V}_{m}
    \]

    we have
    \begin{align*}
        0 = \alpha(\ldots, w_{j} + w_{j+k}, \ldots, w_{j+k} + w_{j}, \ldots) & = \alpha(\ldots, w_{j}, \ldots, w_{j+k}, \ldots) + \alpha(\ldots, w_{j+k}, \ldots, w_{j}, \ldots).
    \end{align*}

    So $\alpha(\ldots, w_{j}, \ldots, w_{j+k}, \ldots) = -\alpha(\ldots, w_{j+k}, \ldots, w_{j}, \ldots)$. Apply this result, we obtain that, if $(v_{1}, \ldots, v_{m})\in \underbrace{V\times\cdots\times V}_{m}$ such that $v_{j} = v_{j+\ell}$, then
    \[
        \alpha(\ldots, v_{j}, v_{j+1}, \ldots, v_{j+\ell}, \ldots) = -\alpha(\ldots, v_{j+1}, v_{j}, \ldots, v_{j+\ell}, \ldots).
    \]

    By the induction hypothesis, $\alpha(\ldots, v_{j+1}, v_{j}, \ldots, v_{j+\ell}, \ldots) = 0$ because $v_{j}$ is at the $(j+1)$th slot and $v_{j+\ell}$ is at the $(j+\ell)$th slot. Therefore $\alpha(\ldots, v_{j}, v_{j+1}, \ldots, v_{j+\ell}, \ldots) = 0$.

    Due to the principle of mathematical induction, for every positive integer $k < m$, for every positive integer $j\in\{1,\ldots,m-k\}$, $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{j + k}$.

    Thus $\alpha$ is an alternating $m$-linear form on $V$.
\end{proof}
\newpage

% chapter9:sectionB:exercise4
\begin{exercise}\label{chapter9:sectionB:exercise4}
    Prove or give a counterexample: If $\alpha\in V^{(4)}_{\text{alt}}$, then
    \[
        \{ (v_{1}, v_{2}, v_{3}, v_{4})\in V^{4}: \alpha(v_{1}, v_{2}, v_{3}, v_{4}) = 0 \}
    \]

    is a subspace of $V^{4}$.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V$ is a vector space with dimension $4$. Let $e_{1}, e_{2}, e_{3}, e_{4}$ be a basis of $V$, and $\alpha$ a nonzero alternating $4$-linear form on $V$. Because $\alpha$ is alternating,
    \[
        \alpha(e_{1}, e_{1}, e_{3}, 0) = \alpha(0, e_{2}, e_{2}, e_{4}) = 0.
    \]

    However $(e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4}) = (e_{1}, e_{1}, e_{3}, 0) + (0, e_{2}, e_{2}, e_{4})$ and
    \[
        \alpha(e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4})\ne 0
    \]

    because $e_{1}, e_{1} + e_{2}, e_{2} + e_{3}, e_{4}$. Therefore the set
    \[
        \{ (v_{1}, v_{2}, v_{3}, v_{4})\in V^{4}: \alpha(v_{1}, v_{2}, v_{3}, v_{4}) = 0 \}
    \]

    is not closed under addition, which implies it is not a subspace of $V^{4}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise5
\begin{exercise}\label{chapter9:sectionB:exercise5}
    Suppose $m$ is a positive integer and $\beta$ is an $m$-linear form on $V$. Define an $m$-linear form $\alpha$ on $V$ by
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}(\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    for $v_{1}, \ldots, v_{m}\in V$. Explain why $\alpha\in V^{(m)}_{\text{alt}}$.
\end{exercise}

\begin{proof}
    For each $(j_{1}, \ldots, j_{m})\in \operatorname{perm}m$, the map
    \[
        (v_{1}, \ldots, v_{m})\mapsto (\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    is an $m$-linear form on $V$. Therefore
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}(\operatorname{sign}(j_{1}, \ldots, j_{m}))\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    is an $m$-linear form on $V$.

    If $v_{j} = v_{k}$ for some $j\ne k$ then for each permutation $(x_{1}, \ldots, x_{m})$ in $\operatorname{perm}m$, there exists a unique permutation $(y_{1}, \ldots, y_{m})$ in $\operatorname{perm}m$ where these two differ by a swap $j\leftrightarrow k$, hence one has $+1$ sign and the other has $-1$ sign. Moreover,
    \[
        \beta(v_{x_{1}}, \ldots, v_{x_{m}}) = \beta(v_{y_{1}}, \ldots, v_{y_{m}}).
    \]

    Therefore $\alpha(v_{1}, \ldots, v_{m}) = 0$ if $v_{j} = v_{k}$ for some $j\ne k$. Hence $\alpha$ is alternating.

    Thus $\alpha\in V^{(m)}_{\text{alt}}$.
\end{proof}
\newpage

% chapter9:sectionB:exercise6
\begin{exercise}\label{chapter9:sectionB:exercise6}
    Suppose $m$ is a positive integer and $\beta$ is an $m$-linear form on $V$. Define an $m$-linear form $\alpha$ on $V$ by
    \[
        \alpha(v_{1}, \ldots, v_{m}) = \sum_{(j_{1}, \ldots, j_{m})\in\operatorname{perm}m}\beta(v_{j_{1}}, \ldots, v_{j_{m}})
    \]

    for $v_{1}, \ldots, v_{m}\in V$. Explain why
    \[
        \alpha(v_{k_{1}}, \ldots, v_{k_{m}}) = \alpha(v_{1}, \ldots, v_{m})
    \]

    for all $v_{1}, \ldots, v_{m}\in V$ and all $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$.
\end{exercise}

\begin{proof}
    For each $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$, $\alpha(v_{k_{1}}, \ldots, v_{k_{m}})$ is the sum of all $\beta(v_{i_{1}}, \ldots, v_{i_{m}})$ where each $(i_{1}, \ldots, i_{m})$ is a permutation of $(k_{1}, \ldots, k_{m})$. On the other hand, each $(i_{1}, \ldots, i_{m})$ is also a permutation of $(1, \ldots, m)$. Therefore $\alpha(v_{k_{1}}, \ldots, v_{k_{m}}) = \alpha(v_{1}, \ldots, v_{m})$ for all $v_{1}, \ldots, v_{m}\in V$ and all $(k_{1}, \ldots, k_{m})\in\operatorname{perm}m$.
\end{proof}
\newpage

% chapter9:sectionB:exercise7
\begin{exercise}\label{chapter9:sectionB:exercise7}
    Give an example of a nonzero alternating $2$-linear form $\alpha$ on $\mathbb{R}^{3}$ and a linearly independent list $v_{1}, v_{2}$ in $\mathbb{R}^{3}$ such that $\alpha(v_{1}, v_{2}) = 0$.
\end{exercise}

\begin{quote}
    This exercise shows that 9.39 can fail if the hypothesis that $n = \dim V$ is deleted.
\end{quote}

\begin{proof}
    Here is an example.
    \[
        \alpha((x_{1}, x_{2}, x_{3}), (y_{1}, y_{2}, y_{3})) = x_{1}y_{2} - x_{2}y_{1}
    \]

    $\alpha\in {(\mathbb{R}^{3})}^{(2)}_{\text{alt}}$. However,
    \[
        \alpha((1, 0, 0), (1, 0, 1)) = 0
    \]

    although $(1, 0, 0), (1, 0, 1)$ is a linearly independent list.
\end{proof}
\newpage

\section{Determinants}

\section{Tensor Products}

