\chapter{Operators on Inner Product Spaces}

\section{Self-Adjoint and Normal Operators}

% chapter7:sectionA:exercise1
\begin{exercise}
    Suppose $n$ is a positive integer. Define $T\in\lmap{\mathbb{F}^{n}}$ by
    \[
        T(z_{1}, \ldots, z_{n}) = (0, z_{1}, \ldots, z_{n-1}).
    \]

    Find a formula for $T^{*}(z_{1}, \ldots, z_{n})$.
\end{exercise}

\begin{proof}
    \begin{align*}
        \innerprod{(z_{1}, \ldots, z_{n}), T^{*}(w_{1}, \ldots, w_{n})} & = \innerprod{T(z_{1}, \ldots, z_{n}), (w_{1}, \ldots, w_{n})}      \\
                                                                        & = \innerprod{(0, z_{1}, \ldots, z_{n-1}), (w_{1}, \ldots, w_{n})}  \\
                                                                        & = 0 + z_{1}\conj{w_{2}} + \cdots + z_{n-1}\conj{w_{n}}             \\
                                                                        & = z_{1}\conj{w_{2}} + \cdots + z_{n-1}\conj{w_{n}} + z_{n}\conj{0} \\
                                                                        & = \innerprod{(z_{1}, \ldots, z_{n}), (w_{2}, \ldots, w_{n}, 0)}.
    \end{align*}

    Thus $T^{*}(w_{1}, \ldots, w_{n}) = (w_{2}, \ldots, w_{n}, 0)$.
\end{proof}
\newpage

% chapter7:sectionA:exercise2
\begin{exercise}
    Suppose $T\in\lmap{V, W}$. Prove that
    \[
        T = 0 \Longleftrightarrow T^{*} = 0 \Longleftrightarrow T^{*}T = 0 \Longleftrightarrow TT^{*} = 0.
    \]
\end{exercise}

\begin{proof}
    \begin{align*}
        T = 0 & \Longleftrightarrow \kernel{T} = V                                                                           \\
              & \Longleftrightarrow {(\kernel{T})}^{\bot} = \{ 0 \}                                                          \\
              & \Longleftrightarrow \range{T^{*}} = \{ 0 \}         & \text{(since $\range{T^{*}} = {(\kernel{T})}^{\bot}$)} \\
              & \Longleftrightarrow T^{*} = 0.
    \end{align*}

    $T^{*}T$ and $TT^{*}$ are self-adjoint operators.
    \begin{align*}
        T = 0     & \Longleftrightarrow \innerprod{Tv, Tv} = 0\quad\forall v\in V         \\
                  & \Longleftrightarrow \innerprod{v, T^{*}(Tv)} = 0\quad\forall v\in V   \\
                  & \Longleftrightarrow \innerprod{v, (T^{*}T)v} = 0\quad\forall v\in V   \\
                  & \Longleftrightarrow T^{*}T = 0,                                       \\
        T^{*} = 0 & \Longleftrightarrow \innerprod{T^{*}v, T^{*}v} = 0\quad\forall v\in V \\
                  & \Longleftrightarrow \innerprod{T(T^{*}v), v} = 0\quad\forall v\in V   \\
                  & \Longleftrightarrow \innerprod{(TT^{*})v, v} = 0\quad\forall v\in V   \\
                  & \Longleftrightarrow TT^{*} = 0.
    \end{align*}

    Thus $T = 0 \Longleftrightarrow T^{*} = 0 \Longleftrightarrow T^{*}T = 0 \Longleftrightarrow TT^{*} = 0$.
\end{proof}
\newpage

% chapter7:sectionA:exercise3
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $\lambda\in\mathbb{F}$. Prove that
    \[
        \text{$\lambda$ is an eigenvalue of $T$} \Longleftrightarrow \text{$\conj{\lambda}$ is an eigenvalue of $T^{*}$}.
    \]
\end{exercise}

\begin{proof}
    For every $\lambda\in\mathbb{F}$, ${(T - \lambda I)}^{*} = T^{*} - \conj{\lambda}I$.

    $\kernel{(T^{*} - \conj{\lambda}I)} = {(\range{(T - \lambda I)})}^{\bot}$ and $\kernel{(T - \lambda I)} = {(\range{(T^{*} - \conj{\lambda}I)})}^{\bot}$.

    If $\lambda$ is an eigenvalue of $T$, then $\kernel{(T - \lambda I)}\ne \{0\}$, so ${(\range{(T^{*} - \conj{\lambda}I)})}^{\bot}\ne \{0\}$, and it follows that ${\range{(T^{*} - \conj{\lambda}I)}}\ne V$. Therefore $\kernel{(T^{*} - \conj{\lambda} I)}\ne \{0\}$ (follows from the fundamental theorem of linear maps). So $\conj{\lambda}$ is an eigenvalue of $T^{*}$.

    \bigskip

    If $\conj{\lambda}$ is an eigenvalue of $T^{*}$, then $\conj{\conj{\lambda}} = \lambda$ is an eigenvalue of ${(T^{*})}^{*} = T$.

    \bigskip

    Thus $\lambda$ is an eigenvalue of $T$ if and only if $\conj{\lambda}$ is an eigenvalue of $T^{*}$.
\end{proof}

\begin{proof}
    For every $\lambda\in\mathbb{F}$, ${(T - \lambda I)}^{*} = T^{*} - \conj{\lambda}I$.

    $\lambda$ is not an eigenvalue of $T$ if and only if $T - \lambda I$ is invertible.

    $T - \lambda I$ is invertible if and only if there exists $S\in\lmap{V}$ such that $S(T - \lambda I) = (T - \lambda I)S = I$.

    $S(T - \lambda I) = (T - \lambda I)S = I$ if and only if $(T^{*} - \conj{\lambda}I)S^{*} = S^{*}(T^{*} - \conj{\lambda}I) = I$.

    $T^{*} - \conj{\lambda} I$ is invertible if and only if there exists $S^{*}\in\lmap{V}$ such that $(T^{*} - \conj{\lambda}I)S^{*} = S^{*}(T^{*} - \conj{\lambda}I) = I$.

    $T^{*} - \conj{\lambda} I$ is invertible if and only if $\conj{\lambda}$ is not an eigenvalue of $T^{*}$.

    Therefore $\lambda$ is not an eigenvalue of $T$ if and only if $\conj{\lambda}$ is not an eigenvalue of $T^{*}$. So $\lambda$ is an eigenvalue of $T$ if and only if $\conj{\lambda}$ is an eigenvalue of $T^{*}$.
\end{proof}
\newpage

% chapter7:sectionA:exercise4
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $U$ is a subspace of $V$. Prove that
    \[
        \text{$U$ is invariant under $T$}\Longleftrightarrow \text{$U^{\bot}$ is invariant under $T^{*}$}.
    \]
\end{exercise}

\begin{proof}
    Let $u$ be an arbitrary vector in $U$ and $w$ be an arbitrary vector in $U^{\bot}$.

    If $U$ is invariant under $T$, then $Tu\in U$ and $\innerprod{u, T^{*}w} = \innerprod{Tu, w} = 0$. It follows that $T^{*}w$ is orthogonal to every $u\in U$. Therefore $T^{*}w\in U^{\bot}$ for every $w\in U^{\bot}$, so $U^{\bot}$ is invariant under $T^{*}$.

    If $U^{\bot}$ is invariant under $T^{*}$, then $T^{*}w\in U^{\bot}$ and $\innerprod{Tu, w} = \innerprod{u, T^{*}w} = 0$. It follows that $Tu$ is orthogonal to every $w\in U^{\bot}$. Therefore $Tu\in U$ for every $u\in U$, so $U$ is invariant under $T$.
\end{proof}
\newpage

% chapter7:sectionA:exercise5
\begin{exercise}\label{chapter7:sectionA:exercise5}
    Suppose $T\in\lmap{V, W}$. Suppose $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$ and $f_{1}, \ldots, f_{m}$ is an orthonormal basis of $W$. Prove that
    \[
        \norm{Te_{1}}^{2} + \cdots + \norm{Te_{n}}^{2} = \norm{T^{*}f_{1}}^{2} + \cdots + \norm{T^{*}f_{m}}^{2}.
    \]
\end{exercise}

\begin{quote}
    The numbers $\norm{Te_{1}}^{2}, \ldots, \norm{Te_{n}}^{2}$ in the equation above depend on the orthonormal basis $e_{1}, \ldots, e_{n}$, but the right side of the equation does not depend on $e_{1}, \ldots, e_{n}$. Thus the equation above shows that the sum on the left side does not depend on which orthonormal basis $e_{1}, \ldots, e_{n}$ is used.
\end{quote}

\begin{proof}
    According to the definition of self-adjoint
    \[
        \sum^{n}_{j=1}\norm{Te_{j}}^{2} = \sum^{n}_{j=1}\innerprod{Te_{j}, Te_{j}} = \sum^{n}_{j=1}\innerprod{e_{j}, T^{*}(Te_{j})}.
    \]

    Since $Te_{j} = \innerprod{Te_{j}, f_{1}}f_{1} + \cdots + \innerprod{Te_{j}, f_{m}}f_{m}$, then
    \begin{align*}
        \sum^{n}_{j=1}\innerprod{e_{j}, T^{*}(Te_{j})} & = \sum^{n}_{j=1}\innerprod{e_{j}, T^{*}\left( \sum^{m}_{k=1}\innerprod{Te_{j}, f_{k}}f_{k} \right)} \\
                                                       & = \sum^{n}_{j=1}\innerprod{e_{j}, \sum^{m}_{k=1}\innerprod{Te_{j}, f_{k}} T^{*}f_{k}}               \\
                                                       & = \sum^{n}_{j=1}\sum^{m}_{k=1}\conj{\innerprod{Te_{j}, f_{k}}}\innerprod{e_{j}, T^{*}f_{k}}         \\
                                                       & =  \sum^{n}_{j=1}\sum^{m}_{k=1}\abs{\innerprod{T^{*}f_{k}, e_{j}}}^{2}                              \\
                                                       & = \sum^{m}_{k=1}\sum^{n}_{j=1}\abs{\innerprod{T^{*}f_{k}, e_{j}}}^{2}.
    \end{align*}

    By Parseval's theorem
    \[
        \sum^{m}_{k=1}\sum^{n}_{j=1}\abs{\innerprod{T^{*}f_{k}, e_{j}}}^{2} =  \sum^{m}_{k=1}\norm{T^{*}f_{k}}^{2}.
    \]

    Thus
    \[
        \norm{Te_{1}}^{2} + \cdots + \norm{Te_{n}}^{2} = \norm{T^{*}f_{1}}^{2} + \cdots + \norm{T^{*}f_{m}}^{2}.
    \]
\end{proof}
\newpage

% chapter7:sectionA:exercise6
\begin{exercise}
    Suppose $T\in\lmap{V, W}$. Prove that
    \begin{enumerate}[label={(\alph*)}]
        \item $T$ is injective $\Longleftrightarrow$ $T^{*}$ is surjective.
        \item $T$ is surjective $\Longleftrightarrow$ $T^{*}$ is injective.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \begin{align*}
                  \text{$T$ is injective} & \Longleftrightarrow \kernel{T} = \{0\}            \\
                                          & \Longleftrightarrow {(\kernel{T})}^{\bot} = V     \\
                                          & \Longleftrightarrow \range{T^{*}} = V             \\
                                          & \Longleftrightarrow \text{$T^{*}$ is surjective}.
              \end{align*}
        \item \begin{align*}
                  \text{$T$ is surjective} & \Longleftrightarrow \range{T} = W                \\
                                           & \Longleftrightarrow {(\range{T})}^{\bot} = \{0\} \\
                                           & \Longleftrightarrow \kernel{T^{*}} = \{0\}       \\
                                           & \Longleftrightarrow \text{$T^{*}$ is injective}.
              \end{align*}
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise7
\begin{exercise}\label{chapter7:sectionA:exercise7}
    Suppose $T\in\lmap{V, W}$, then
    \begin{enumerate}[label={(\alph*)}]
        \item $\dim \kernel{T^{*}} = \dim \kernel{T} + \dim W - \dim V$.
        \item $\dim \range{T^{*}} = \dim \range{T}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Because $\kernel{T^{*}} = {(\range{T})}^{\bot}$, it follows that $\dim \kernel{T^{*}} = \dim {(\range{T})}^{\bot}$.

              Moreover, $\dim {(\range{T})}^{\bot} = \dim W - \dim \range{T}$. By the fundamental theorem of linear maps, $\dim\range{T} = \dim V - \dim\kernel{T}$.

              Thus $\dim\kernel{T^{*}} = \dim W + \dim\kernel{T} - \dim V$.
        \item By (a) and the fundamental theorem of linear maps
              \[
                  \dim\range{T^{*}} = \dim W - \dim\kernel{T^{*}} = \dim V - \dim\kernel{T} = \dim\range{T}.
              \]
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise8
\begin{exercise}
    Suppose $A$ is an $m$-by-$n$ matrix with entries in $\mathbb{F}$. Use (b) in Exercise~\ref{chapter7:sectionA:exercise7} to prove that the row rank of $A$ equals the column rank of $A$.
\end{exercise}

\begin{proof}
    Let $T\in \lmap{\mathbb{F}^{n}, \mathbb{F}^{m}}$ defined by $T: x\mapsto Ax$. The adjoint $T^{*}\in \lmap{\mathbb{F}^{m}, \mathbb{F}^{n}}$ is $y\mapsto A^{*}y$.
    \begin{align*}
        \text{column rank of $A$} & = \dim\range{T}                 \\
                                  & = \dim\range{T^{*}}             \\
                                  & = \text{column rank of $A^{*}$} \\
                                  & = \text{row rank of $A$}.
    \end{align*}

    Thus the row rank of $A$ equals the column rank of $A$.
\end{proof}
\newpage

% chapter7:sectionA:exercise9
\begin{exercise}
    Prove that the product of two self-adjoint operators on $V$ is self-adjoint if and only if the two operators commute.
\end{exercise}

\begin{proof}
    Let $S, T$ be two self-adjoint operators on $V$.
    \begin{align*}
        \text{$ST$ is self-adjoint} & \Longleftrightarrow ST = {(ST)}^{*}                                                      \\
                                    & \Longleftrightarrow S^{*}T^{*} = {(ST)}^{*}     & \text{(since $S, T$ are self-adjoint)} \\
                                    & \Longleftrightarrow S^{*}T^{*} = T^{*}S^{*}                                              \\
                                    & \Longleftrightarrow ST = TS                     & \text{(since $S, T$ are self-adjoint)} \\
                                    & \Longleftrightarrow \text{$S$ and $T$ commute.}
    \end{align*}
\end{proof}
\newpage

% chapter7:sectionA:exercise10
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T \in \lmap{V}$. Prove that $T$ is self-adjoint if and only if
    \[
        \innerprod{Tv, v} = \innerprod{T^{*}v, v}
    \]

    for all $v\in V$.
\end{exercise}

\begin{proof}
    An operator $S$ on a complex vector space $V$ is $0$ if and only if $\innerprod{Sv, v} = 0$ for all $v\in V$.
    \begin{align*}
        \text{$T$ is self-adjoint} & \Longleftrightarrow T - T^{*} = 0                                              \\
                                   & \Longleftrightarrow \innerprod{(T - T^{*})v, v} = 0\,\forall v\in V            \\
                                   & \Longleftrightarrow \innerprod{Tv, v} = \innerprod{T^{*}v, v}\,\forall v\in V.
    \end{align*}
\end{proof}
\newpage

% chapter7:sectionA:exercise11
\begin{exercise}
    Define an operator $S: \mathbb{F}^{2}\to \mathbb{F}^{2}$ by $S(w, z) = (-z, w)$.
    \begin{enumerate}[label={(\alph*)}]
        \item Find a formula for $S^{*}$.
        \item Show that $S$ is normal but not self-adjoint.
        \item Find all eigenvalues of $S$.
    \end{enumerate}
\end{exercise}

\begin{quote}
    If $\mathbb{F} = \mathbb{R}$, then $S$ is the operator on $\mathbb{R}^{2}$ of counterclockwise rotation by $90^{\circ}$.
\end{quote}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \begin{align*}
                  \innerprod{(w, z), S^{*}(x, y)} & = \innerprod{S(w, z), (x, y)}  \\
                                                  & = \innerprod{(-z, w), (x, y)}  \\
                                                  & = -z\conj{x} + w\conj{y}       \\
                                                  & = w\conj{y} + z\conj{(-x)}     \\
                                                  & = \innerprod{(w, z), (y, -x)}.
              \end{align*}

              Hence $S^{*}(x, y) = (y, -x)$.
        \item \[
                  \begin{split}
                      (SS^{*})(w, z) = S(z, -w) = (w, z), \\
                      (S^{*}S)(w, z) = S^{*}(-z, w) = (w, z),
                  \end{split}
              \]

              so $SS^{*} = S^{*}S$, which means $S$ is normal.
              \[
                  S(1, 0) = (0, 1) \ne (0, -1) = S^{*}(1, 0)
              \]

              so $S$ is not self-adjoint.
        \item
              \[
                  S^{2}(w, z) = S(-z, w) = (-w, -z)
              \]

              Therefore $z^{2} + 1$ is a polynomial multiple of the minimal polynomial of $S$. On the other hand, the minimal polynomial of $S$ cannot have degree $1$. So $z^{2} + 1$ is the minimal polynomial of $S$.

              If $\mathbb{F} = \mathbb{R}$, then $S$ has no eigenvalues. If $\mathbb{F} = \mathbb{C}$, the eigenvalues of $S$ are $\iota$ and $-\iota$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise12
\begin{exercise}
    An operator $B\in\lmap{V}$ is called skew if
    \[
        B^{*} = -B
    \]

    Suppose that $T\in\lmap{V}$. Prove that $T$ is normal if and only if there exist commuting operators $A$ and $B$ such that $A$ is self-adjoint, $B$ is a skew operator, and $T = A + B$.
\end{exercise}

\begin{proof}
    Let $A = \frac{T + T^{*}}{2}$ and $B = \frac{T - T^{*}}{2}$.
    \begin{align*}
        A^{*} & = \conj{\left(\frac{1}{2}\right)}(T^{*} + {(T^{*})}^{*}) = \frac{1}{2}(T^{*} + T) = A,                                \\
        B^{*} & = \conj{\left(\frac{1}{2}\right)}T^{*} + \conj{\left(\frac{-1}{2}\right)}{(T^{*})}^{*} = \frac{1}{2}(T^{*} - T) = -B.
    \end{align*}

    So $A$ is a self-adjoint operator, and $B$ is a skew operator.
    \begin{align*}
        AB - BA & = \frac{(T + T^{*})(T - T^{*}) - (T - T^{*})(T + T^{*})}{4}                                       \\
                & = \frac{(T^{2} + T^{*}T - TT^{*} - {(T^{*})}^{2}) - (T^{2} - T^{*}T + TT^{*} - {(T^{*})}^{2})}{4} \\
                & = \frac{T^{*}T - TT^{*}}{2}.
    \end{align*}

    So $A$ and $B$ commute if and only if $T$ is normal.
\end{proof}
\newpage

% chapter7:sectionA:exercise13
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{R}$. Define $\mathcal{A}\in \lmap{\lmap{V}}$ by $\mathcal{A}T = T^{*}$ for all $T\in\lmap{V}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Find all eigenvalues of $\mathcal{A}$.
        \item Find the minimal polynomial of $\mathcal{A}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Assume $\lambda\in\mathbb{R}$ is an eigenvalue of $\mathcal{A}$, and $T$ is a corresponding eigenvector.
              \[
                  T^{*} = \mathcal{A}T = \lambda T.
              \]

              It follows that $T = {(T^{*})}^{*} = {(\lambda T)}^{*} = \conj{\lambda}T^{*} = \lambda T^{*}$. Therefore $T = \lambda T^{*} = \lambda^{2}T$, and $(1 - \lambda^{2})T = 0$. Because $T$ is not the zero vector, we conclude that $\lambda^{2} = 1$, which means $\lambda = 1$ or $\lambda = -1$.

              If $T$ is nonzero and self-adjoint, then $\mathcal{A}T = T = 1T$. If $T$ is nonzero and skew, then $\mathcal{A}T = -T = (-1)T$.

              If $\dim V = 1$, the only eigenvalue of $\mathcal{A}$ is $1$. Otherwise the eigenvalues of $\mathcal{A}$ are $1$ and $-1$.
        \item Find the minimal polynomial of $\mathcal{A}$.

              If $\dim V = 1$, the minimal polynomial of $\mathcal{A}$ is $(z - 1)$. Otherwise, by (a), the minimal polynomial of $\mathcal{A}$ is a polynomial multiple of $z^{2} - 1$. Moreover $\mathcal{A}^{2}T = \mathcal{A}T^{*} = {(T^{*})}^{*} = T$. Hence the minimal polynomial of $\mathcal{A}$ is $z^{2} - 1$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise14
\begin{exercise}
    Define an inner product on $\mathscr{P}_{2}(\mathbb{R})$ by $\innerprod{p, q} = \int^{1}_{0}pq$. Define an operator $T\in\lmap{\mathscr{P}_{2}(\mathbb{R})}$ by
    \[
        T(ax^{2} + bx + c) = bx.
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that with this inner product, the operator $T$ is not self-adjoint.
        \item The matrix of $T$ with respect to the basis $1, x, x^{2}$ is
              \[
                  \begin{pmatrix}
                      0 & 0 & 0 \\
                      0 & 1 & 0 \\
                      0 & 0 & 0
                  \end{pmatrix}.
              \]

              This matrix equals its conjugate transpose, even though $T$ is not self-adjoint. Explain why this is not a contradiction.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item \begin{align*}
                  \innerprod{ax^{2} + bx + c, T^{*}(c)} & = \innerprod{T(ax^{2} + bx + c), c} \\
                                                        & = \innerprod{bx, c}                 \\
                                                        & = \int^{1}_{0}bcx dx                \\
                                                        & = \frac{bc}{2},                     \\
                  \innerprod{ax^{2} + bx + c, T(c)}     & = \innerprod{ax^{2} + bx + c, 0}    \\
                                                        & = 0.
              \end{align*}

              Choose $b, c$ such that $bc\ne 0$, then $\innerprod{ax^{2} + bx + c, T^{*}(c)}\ne \innerprod{ax^{2} + bx + c, T(c)}$. Therefore $T^{*}(c)\ne T(c)$, which implies $T$ is not self-adjoint.
        \item This is not a contradiction because $1, x, x^{2}$ is not an orthonormal basis of $\mathscr{P}_{2}(\mathbb{R})$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise15
\begin{exercise}
    Suppose $T\in\lmap{V}$ is invertible. Prove that
    \begin{enumerate}[label={(\alph*)}]
        \item $T$ is self-adjoint $\Longleftrightarrow$ $T^{-1}$ is self-adjoint;
        \item $T$ is normal $\Longleftrightarrow$ $T^{-1}$ is normal.
    \end{enumerate}
\end{exercise}

\begin{proof}
    $T$ is invertible, then there exist a unique operator $T^{-1}$ such that $TT^{-1} = T^{-1}T = I$.

    Therefore $I = {(TT^{-1})}^{*} = {(T^{-1})}^{*}T^{*}$ and $I = {(T^{-1}T)}^{*} = T^{*}{(T^{-1})}^{*}$, so
    \[
        {(T^{-1})}^{*} = {(T^{*})}^{-1}.
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Because ${(T^{-1})}^{*} = {(T^{*})}^{-1}$, it follows that $T = T^{*} \Longleftrightarrow {(T^{-1})}^{*} = T^{-1}$. So $T$ is self-adjoint if and only if $T^{-1}$ is self-adjoint.
        \item Because ${(T^{-1})}^{*} = {(T^{*})}^{-1}$, we have
              \begin{align*}
                  {(T^{-1})}^{*}T^{-1} & = {(T^{*})}^{-1}T^{-1} = {(TT^{*})}^{-1}, \\
                  T^{-1}{(T^{-1})}^{*} & = T^{-1}{(T^{*})}^{-1} = {(T^{*}T)}^{-1}.
              \end{align*}

              Hence $TT^{*} = T^{*}T$ if and only if ${(T^{-1})}^{*}T^{-1} = T^{-1}{(T^{-1})}^{*}$. Thus $T$ is normal if and only if $T^{-1}$ is normal.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise16
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{R}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that the set of self-adjoint operators on $V$ is a subspace of $\lmap{V}$.
        \item What is the dimension of the subspace of $\lmap{V}$ in (a) [in terms of $\dim V$]?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $0 = 0^{*}$ so $0$ is in the set of self-adjoint operator.

              If $S, T$ are self-adjoint operator, then ${(S + T)}^{*} = {S^{*} + T^{*}} = S + T$ and ${(\lambda S)}^{*} = \conj{\lambda}S^{*} = \lambda S^{*} = \lambda S$. So the set of self-adjoint operators on $V$ is closed under addition and scalar multiplication.

              Thus the set of self-adjoint operators on $V$ is a subspace of $\lmap{V}$.
        \item Let $n = \dim V$ and $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. If $T$ is a self-adjoint operator on $V$, then the matrix of $T$ with respect to $e_{1}, \ldots, e_{n}$ is symmetric (we are working with $\mathbb{F} = \mathbb{R}$).

              Let $E_{i,i}$ be the operator on $V$ such that $E_{i,i}e_{i} = e_{i}$ and $E_{i,i}e_{j} = 0$ if $j\ne i$.

              Let $E_{i,j}$ be the operator on $V$ where $i<j$ such that $E_{i,j}e_{i} = e_{j}$, $E_{i,j}e_{j} = e_{i}$ and $E_{i,j}e_{k} = 0$ if $k\notin\{i, j\}$.

              All operators $E_{i,i}, E_{i,j}$ are self-adjoint (there are $n(n + 1)/2$ of them) and they constitute an independent list. Moreover,
              \[
                  T = \innerprod{Te_{1}, e_{1}}E_{1,1} + \cdots + \innerprod{Te_{n}, e_{n}}E_{n,n} + \sum_{1\leq i < j\leq n}\innerprod{Te_{i}, e_{j}}E_{i, j} = \sum_{1\leq i\leq j\leq n}\innerprod{Te_{i}, e_{j}}E_{i,j}.
              \]

              Therefore the list $E_{i,i}, E_{i,j}$ spans the subspace of self-adjoint operators on $V$, hence it is a basis of the subspace. Thus the dimension of the subspace in (a) is $\dim V \times (\dim V + 1)/2$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise17
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$. Show that the set of self-adjoint operators on $V$ is not a subspace of $\lmap{V}$.
\end{exercise}

\begin{proof}
    Let $\lambda$ be a complex number but not a real number, then for every nonzero self-adjoint operator $T$ on $V$, ${(\lambda T)}^{*} = \conj{\lambda}T^{*} = \conj{\lambda}T \ne \lambda T$. So the set of self-adjoint operators on $V$ is not closed under scalar multiplication. Thus the set of self-adjoint operators on $V$ is not a subspace of $\lmap{V}$.
\end{proof}
\newpage

% chapter7:sectionA:exercise18
\begin{exercise}
    Suppose $\dim V\geq 2$. Show that the set of normal operators on $V$ is not a subspace of $\lmap{V}$.
\end{exercise}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. I define $S$ and $T$ as follows:
    \[
        Se_{i} = \begin{cases}
            -e_{2} & \text{if $i = 1$} \\
            e_{1}  & \text{if $i = 2$} \\
            0      & \text{otherwise}
        \end{cases}\qquad
        Te_{i} = \begin{cases}
            e_{2} & \text{if $i = 1$} \\
            e_{1} & \text{if $i = 2$} \\
            0     & \text{otherwise}
        \end{cases}
    \]

    \begin{align*}
        \innerprod{a_{1}e_{1} + \cdots + a_{n}e_{n}, S^{*}(b_{1}e_{1} + \cdots + b_{n}e_{n})} & = \innerprod{S(a_{1}e_{1} + \cdots + a_{n}e_{n}), b_{1}e_{1} + \cdots + b_{n}e_{n}}     \\
                                                                                              & = \innerprod{a_{2}e_{1} + (-a_{1})e_{2}, b_{1}e_{1} + b_{2}e_{2} + \cdots + b_{n}e_{n}} \\
                                                                                              & = a_{2}\conj{b_{1}} + (-a_{1})\conj{b_{2}}                                              \\
                                                                                              & = \innerprod{a_{1}e_{1} + a_{2}e_{2} + \cdots + a_{n}e_{n}, -b_{2}e_{1} + b_{1}e_{2}}   \\
        \innerprod{a_{1}e_{1} + \cdots + a_{n}e_{n}, T^{*}(b_{1}e_{1} + \cdots + b_{n}e_{n})} & = \innerprod{T(a_{1}e_{1} + \cdots + a_{n}e_{n}), b_{1}e_{1} + \cdots + b_{n}e_{n}}     \\
                                                                                              & = \innerprod{a_{2}e_{1} + a_{1}e_{2}, b_{1}e_{1} + b_{2}e_{2} + \cdots + b_{n}e_{n}}    \\
                                                                                              & = a_{2}\conj{b_{1}} + a_{1}\conj{b_{2}}                                                 \\
                                                                                              & = \innerprod{a_{1}e_{1} + a_{2}e_{2} + \cdots + a_{n}e_{n}, b_{2}e_{1} + b_{1}e_{2}}
    \end{align*}

    So
    \[
        \begin{split}
            S^{*}(b_{1}e_{1} + \cdots + b_{n}e_{n}) = -b_{2}e_{1} + b_{1}e_{2} \\
            T^{*}(b_{1}e_{1} + \cdots + b_{n}e_{n}) = b_{2}e_{1} + b_{1}e_{2}  \\
        \end{split}
    \]
    \begin{align*}
        (SS^{*})(b_{1}e_{1} + \cdots + b_{n}e_{n}) & = S(-b_{2}e_{1} + b_{1}e_{2}) = b_{1}e_{1} + b_{2}e_{2}     \\
        (S^{*}S)(b_{1}e_{1} + \cdots + b_{n}e_{n}) & = S^{*}(-b_{1}e_{2} + b_{2}e_{1}) = b_{1}e_{1} + b_{2}e_{2} \\
        (TT^{*})(b_{1}e_{1} + \cdots + b_{n}e_{n}) & = T(b_{2}e_{1} + b_{1}e_{2}) = b_{1}e_{1} + b_{2}e_{2}      \\
        (T^{*}T)(b_{1}e_{1} + \cdots + b_{n}e_{n}) & = T^{*}(b_{1}e_{2} + b_{2}e_{1}) = b_{1}e_{1} + b_{2}e_{2}
    \end{align*}

    So $S$ and $T$ are normal operators. $R = S + T$.
    \begin{align*}
        \innerprod{a_{1}e_{1} + \cdots + a_{n}e_{n}, R^{*}(b_{1}e_{1} + \cdots + b_{n}e_{n})} & = \innerprod{R(a_{1}e_{1} + \cdots + a_{n}e_{n}), b_{1}e_{1} + \cdots + b_{n}e_{n}} \\
                                                                                              & = \innerprod{2a_{2}e_{1}, b_{1}e_{1} + \cdots + b_{n}e_{n}}                         \\
                                                                                              & = 2a_{2}\conj{b_{1}}                                                                \\
                                                                                              & = \innerprod{a_{1}e_{1} + \cdots + a_{n}e_{n}, 2b_{1}e_{2}}
    \end{align*}

    Hence $R^{*}(b_{1}e_{1} + \cdots + b_{n}e_{n}) = 2b_{1}e_{2}$.
    \begin{align*}
        (RR^{*})(b_{1}e_{1} + \cdots + b_{n}e_{n}) & = R(2b_{1}e_{2}) = 2b_{1}e_{1}                 \\
        (R^{*}R)(b_{1}e_{1} + \cdots + b_{n}e_{n}) & = R^{*}(b_{1}e_{2} + b_{2}e_{1}) = 2b_{2}e_{2}
    \end{align*}

    So $R$ and $R^{*}$ does not commute. Therefore the set of normal operators on $V$ is not closed under addition. Thus the set of normal operators on $V$ is not a subspace of $\lmap{V}$ if $\dim V\geq 2$.
\end{proof}
\newpage

% chapter7:sectionA:exercise19
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $\norm{T^{*}v} \leq \norm{Tv}$ for every $v\in V$. Prove that $T$ is normal.
\end{exercise}

\begin{quote}
    This exercise fails on infinite-dimensional inner product spaces, leading to what are called hyponormal operators, which have a well-developed theory.
\end{quote}

\begin{proof}
    Let $v$ be a nonzero vector in $V$. Let $e_{1} = v/\norm{v}$ and $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. By Exercise~\ref{chapter7:sectionA:exercise5}, we have
    \[
        \norm{Te_{1}}^{2} + \cdots + \norm{Te_{n}}^{2} = \norm{T^{*}e_{1}}^{2} + \cdots + \norm{T^{*}e_{n}}^{2}.
    \]

    According to the hypothesis, $\norm{Te_{i}}^{2}\geq \norm{T^{*}e_{i}}^{2}$ for each $i\in\{1,\ldots, n\}$. Together with the inequality, we conclude that $\norm{Te_{i}}^{2} = \norm{Te_{i}}^{2}$ for each $i\in\{1,\ldots, n\}$.

    Therefore
    \[
        \norm{Tv} = \norm{T\left(\norm{v}\frac{v}{\norm{v}}\right)} = \norm{\norm{v}T\left(\frac{v}{\norm{v}}\right)} = \norm{\norm{v}T^{*}\left(\frac{v}{\norm{v}}\right)} = \norm{T^{*}v}.
    \]

    If $v = 0$ then $\norm{Tv} = \norm{T^{*}v} = 0$. Hence $\norm{Tv} = \norm{T^{*}v}$ for every $v\in V$. This means $T$ is normal.
\end{proof}
\newpage

% chapter7:sectionA:exercise20
\begin{exercise}
    Suppose $P\in\lmap{V}$ is such that $P^{2} = P$. Prove that the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item $P$ is self-adjoint.
        \item $P$ is normal.
        \item There is a subspace $U$ of $V$ such that $P = P_{U}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I will show that $(a) \implies (b) \implies (c) \implies (a)$.

    Suppose (a) is true, then $P^{*}P = PP = PP^{*}$, so $P$ is normal. Therefore (b) is true.

    Suppose (b) is true. Since $P^{2} = P$, then $V = \kernel{P}\oplus\range{P}$ and every vector $v$ in $V$ admits the decomposition $v = (v - Pv) + Pv$. $\kernel{P}$ is the eigenspace with respect to the eigenvalue $0$, $\range{P}$ is the eigenspace with respect to the eigenvalue $1$.

    Since $P$ is normal, then the eigenvectors with respect to different eigenvalues are orthogonal. Let $e_{1}, \ldots, e_{m}$ be an orthonormal basis of $\kernel{P}$ and $f_{1}, \ldots, f_{n}$ be an orthonormal basis of $\range{P}$, then $e_{i}$ and $f_{j}$ are orthogonal, for every $i\in\{1,\ldots, m\}$ and $j\in\{ 1,\ldots,n \}$. Therefore $e_{1}, \ldots, e_{m}$, $f_{1}, \ldots, f_{n}$ is an orthonormal basis of $V$ and each of these vectors are eigenvectors of $P$. Let $U = \operatorname{span}(f_{1}, \ldots, f_{n})$.
    \begin{align*}
        v  & = \innerprod{v,e_{1}}e_{1} + \cdots + \innerprod{v,e_{m}}e_{m} + \innerprod{v, f_{1}}f_{1} + \cdots + \innerprod{v, f_{n}}f_{n}                       \\
        Pv & = 0 + \innerprod{v, f_{1}}Pf_{1} + \cdots + \innerprod{v, f_{n}}Pf_{n}                                                                                \\
           & = \innerprod{v, e_{1}}P_{U}e_{1} + \cdots + \innerprod{v, e_{n}}P_{U}e_{n} + \innerprod{v, f_{1}}P_{U}f_{1} + \cdots + \innerprod{v, f_{n}}P_{U}f_{n} \\
           & = P_{U}v.
    \end{align*}

    Hence $P = P_{U}$, so (c) is true.

    Suppose (c) is true. $V = U\oplus U^{\bot}$.

    Let $v\in V$ and $u\in U$, then $v - u\in U^{\bot}$. For every $w\in V$
    \begin{align*}
        \innerprod{Pv, w}     & = \innerprod{u, w} = \innerprod{u, P_{U}w} = \innerprod{P_{U}v, P_{U}w},  \\
        \innerprod{P^{*}v, w} & = \innerprod{v, Pw} = \innerprod{v, P_{U}w} = \innerprod{P_{U}v, P_{U}w}.
    \end{align*}

    Hence $\innerprod{Pv - P^{*}v, w} = 0$ for every $v, w\in V$. So $Pv = P^{*}v$ for every $v\in V$. Thus $P = P^{*}$, which means (a) is true.
\end{proof}
\newpage

% chapter7:sectionA:exercise21
\begin{exercise}
    Suppose $D: \mathscr{P}_{8}(\mathbb{R}) \to \mathscr{P}_{8}(\mathbb{R})$ is the differentiation operator defined by $Dp = p'$. Prove that there does not exist an inner product on $\mathscr{P}_{8}(\mathbb{R})$ that makes $D$ a normal operator.
\end{exercise}

\begin{proof}
    Assume that $D$ is a normal operator for some inner product $\innerprod{\cdot, \cdot}$ on $\mathscr{P}_{8}(\mathbb{R})$.
    \[
        \innerprod{x, D^{*}(1)} = \innerprod{Dx, 1} = \innerprod{1, 1} > 0.
    \]

    Therefore $D^{*}(1)\ne 0$. However, on the other hand
    \[
        \innerprod{D^{*}1, D^{*}1} = \innerprod{(DD^{*})1, 1} = \innerprod{(D^{*}D)1, 1} = \innerprod{D^{*}0, 1} = \innerprod{0, 1} = 0.
    \]

    This contradicts the property of definiteness of inner product so the assumption is false. Thus $D$ is not a normal operator, no matter which inner product are used on $\mathscr{P}_{8}(\mathbb{R})$.
\end{proof}
\newpage

% chapter7:sectionA:exercise22
\begin{exercise}
    Give an example of an operator $T\in\lmap{\mathbb{R}^{3}}$ such that $T$ is normal but not self-adjoint.
\end{exercise}

\begin{proof}
    Let $T(x, y, z) = (-y, x, 0)$.
    \begin{align*}
        \innerprod{(x_{1}, y_{1}, z_{1}), T^{*}(x_{2}, y_{2}, z_{2})} & = \innerprod{T(x_{1}, y_{1}, z_{1}), (x_{2}, y_{2}, z_{2})} \\
                                                                      & = \innerprod{(-y_{1}, x_{1}, 0), (x_{2}, y_{2}, z_{2})}     \\
                                                                      & = (-y_{1})x_{2} + x_{1}y_{2}                                \\
                                                                      & = \innerprod{(x_{1}, y_{1}, z_{1}), (y_{2}, -x_{2}, 0)}
    \end{align*}

    so $T^{*}(x, y, z) = (y, -x, 0)$. Hence $T$ is not self-adjoint.
    \begin{align*}
        (TT^{*})(x, y, z) & = T(y, -x, 0) = (x, y, 0)     \\
        (T^{*}T)(x, y, z) & = T^{*}(-y, x, 0) = (x, y, 0)
    \end{align*}

    so $TT^{*} = T^{*}T$. Hence $T$ is normal.
\end{proof}
\newpage

% chapter7:sectionA:exercise23
\begin{exercise}
    Suppose $T$ is a normal operator on $V$. Suppose also that $v, w \in V$ satisfy the equations
    \[
        \norm{v} = \norm{w} = 2,\quad Tv = 3v,\quad Tw = 4w.
    \]

    Show that $\norm{T(v + w)} = 10$.
\end{exercise}

\begin{proof}
    Since $\norm{v}$ and $\norm{w}$ are positive, $v$ and $w$ are nonzero. Because $Tv = 3v$ and $Tw = 4w$ so $v$ and $w$ are eigenvectors of $T$ with respect to two different eigenvalues $3$ and $4$. Therefore $v$ and $w$ are orthogonal (because eigenvectors of two different eigenvalues of a normal operator are orthogonal).
    \begin{align*}
        \norm{T(v + w)}^{2} & = \norm{Tv + Tw}^{2} = \norm{3v + 4w}^{2}                                \\
                            & = \norm{3v}^{2} + \norm{4w}^{2}           & \text{(Pythagorean theorem)} \\
                            & = 9\cdot 4 + 16\cdot 4 = 100.
    \end{align*}

    Thus $\norm{T(v + w)} = 10$.
\end{proof}
\newpage

% chapter7:sectionA:exercise24
\begin{exercise}\label{chapter7:sectionA:exercise24}
    Suppose $T\in\lmap{V}$ and
    \[
        a_{0} + a_{1}z + a_{2}z^{2} + \cdots + a_{m-1}z^{m-1} + z^{m}
    \]

    is the minimal polynomial of $T$. Prove that the minimal polynomial of $T^{*}$ is
    \[
        \conj{a_{0}} + \conj{a_{1}}z + \conj{a_{2}}z^{2} + \cdots + \conj{a_{m-1}}z^{m-1} + z^{m}
    \]
\end{exercise}

\begin{quote}
    This exercise shows that the minimal polynomial of $T^{*}$ equals the minimal polynomial of $T$ if $\mathbb{F} = \mathbb{R}$.
\end{quote}

\begin{proof}
    Consider two polynomials $p(z)$ and $q(z) = \conj{p(\conj{z})}$. Then $p(z) = \conj{q(\conj{z})}$.
    \begin{align*}
        p(T)v = 0\,\forall v\in V & \Longleftrightarrow \innerprod{p(T)v, w} = 0,\forall v, w\in V          \\
                                  & \Longleftrightarrow \innerprod{v, {(p(T))}^{*}w} = 0\,\forall v, w\in V \\
                                  & \Longleftrightarrow \innerprod{v, q(T)w} = 0\,\forall v, w\in V         \\
                                  & \Longleftrightarrow q(T)w = 0\,\forall w\in V.
    \end{align*}

    Denote the minimal polynomials of $T$ and $T^{*}$ by $\mu_{T}$ and $\mu_{T^{*}}$, respectively.

    Let $p = \mu_{T}$, then $q$ is a polynomial multiple of the minimal polynomial of $T^{*}$, so $\deg \mu_{T^{*}}\leq \deg q = \deg p = \deg \mu_{T}$.

    Let $q = \mu_{T^{*}}$, then $p$ is a polynomial multiple of the minimal polynomial of $T$, so $\deg \mu_{T}\leq \deg p = \deg q = \deg \mu_{T^{*}}$.

    Therefore $\deg \mu_{T} = \deg \mu_{T^{*}}$ and $\mu_{T^{*}}(z) = \conj{\mu_{T}(\conj{z})}$ and the result follows.
\end{proof}
\newpage

% chapter7:sectionA:exercise25
\begin{exercise}
    Suppose $T \in \lmap{V}$. Prove that $T$ is diagonalizable if and only if $T^{*}$ is diagonalizable.
\end{exercise}

\begin{proof}
    If $T$ is diagonalizable, then the minimal polynomial $p_{T}$ of $T$ is a product of different monic polynomials of degree $1$
    \[
        p_{T}(z) = (z - \lambda_{1})\cdots (z - \lambda_{n})
    \]

    where $n\geq 0$. By Exercise~\ref{chapter7:sectionA:exercise24}, the minimal polynomial of $T^{*}$ is
    \[
        p_{T^{*}}(z) = (z - \conj{\lambda_{1}})\cdots (z - \conj{\lambda_{n}})
    \]

    and $\conj{\lambda_{1}}, \ldots, \conj{\lambda_{n}}$ are pairwise distinct. So $T^{*}$ is diagonalizable.

    To prove statement in the other direction, we apply the previous direction. If $T^{*}$ is diagonalizable, then $T = {(T^{*})}^{*}$ is diagonalizable.

    Thus $T$ is diagonalizable if and only if $T^{*}$ is diagonalizable.
\end{proof}
\newpage

% chapter7:sectionA:exercise26
\begin{exercise}
    Fix $u, x\in V$. Define $T\in \lmap{V}$ by $Tv = \innerprod{v, u}x$ for every $v\in V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that if $V$ is a real vector space, then $T$ is self-adjoint if and only if the list $u, x$ is linearly dependent.
        \item Prove that $T$ is normal if and only if the list $u, x$ is linearly dependent.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{align*}
        \innerprod{v, T^{*}w} & = \innerprod{Tv, w} = \innerprod{\innerprod{v, u}x, w} \\
                              & = \innerprod{v, u}\innerprod{x, w}                     \\
                              & = \innerprod{v, \conj{\innerprod{x, w}}u}              \\
                              & = \innerprod{v, \innerprod{w, x}u}
    \end{align*}

    Therefore $T^{*}w = \innerprod{w, x}u$.
    \begin{enumerate}[label={(\alph*)}]
        \item If $T$ is self-adjoint, then $\innerprod{v, x}u = \innerprod{v, u}x$ for every $v\in V$. If either $u$ or $x$ is zero, then $u, x$ is linearly independent. If $u$ and $x$ are nonzero, then $\innerprod{v, x}$ and $\innerprod{v, u}$ are not simultaneously zero, so $u, x$ is linearly dependent.

              If $u, x$ is linearly dependent, then $u$ is a scalar multiple of $x$ or vice versa.
              \begin{itemize}
                  \item If $u = \lambda x$ (notice that $\lambda\in\mathbb{R}$), then
                        \[
                            Tv = \innerprod{v, u}x = \innerprod{v, \lambda x}x = \innerprod{v, x}\lambda x = \innerprod{v, x}u = T^{*}v
                        \]

                        for every $v\in V$.
                  \item If $x = \lambda u$ (notice that $\lambda\in\mathbb{R}$), then
                        \[
                            T^{*}v = \innerprod{v, x}u = \innerprod{v, \lambda u}u = \innerprod{v, u}\lambda u = \innerprod{v, u}x = Tv.
                        \]
              \end{itemize}

              Therefore $T$ is self-adjoint.
        \item \begin{align*}
                  TT^{*} = T^{*}T & \Longleftrightarrow (TT^{*})v = (T^{*}T)v\,\forall v\in V                                                  \\
                                  & \Longleftrightarrow T(\innerprod{v, x}u) = T^{*}(\innerprod{v, u}x)\,\forall v\in V                        \\
                                  & \Longleftrightarrow \innerprod{v, x}Tu = \innerprod{v, u}T^{*}x \,\forall v\in V                           \\
                                  & \Longleftrightarrow \innerprod{v, x}\innerprod{u, u}x = \innerprod{v, u}\innerprod{x, x}u\,\forall v\in V.
              \end{align*}

              If $T$ is normal, then $\innerprod{v, x}\innerprod{u, u}x = \innerprod{v, u}\innerprod{x, x}u\,\forall v\in V$. If either $u$ or $x$ is zero, then $u, x$ is linearly dependent. If $u$ and $x$ are non zero, then $\innerprod{v, x}\innerprod{u, u}$ and $\innerprod{v, u}\innerprod{x, x}$ are not simultaneously zero, so $u, x$ is linearly dependent.

              If $u, x$ is linearly dependent, then $u$ is a scalar multiple of $x$ or vice versa.
              \begin{itemize}
                  \item If $u = \lambda x$ (notice that $\lambda\in\mathbb{R}$), then
                        \begin{align*}
                            \innerprod{v, u}\innerprod{x, x}u & = \innerprod{v, \lambda x}\innerprod{x, x}\lambda x \\
                                                              & = \innerprod{v, x}\innerprod{x, x}\lambda^{2} x     \\
                                                              & = \innerprod{v, x}\innerprod{\lambda x, \lambda x}x \\
                                                              & = \innerprod{v, x}\innerprod{u, u}x
                        \end{align*}

                        for every $v\in V$.
                  \item If $x = \lambda u$ (notice that $\lambda\in\mathbb{R}$), then
                        \begin{align*}
                            \innerprod{v, x}\innerprod{u, u}x & = \innerprod{v, \lambda u}\innerprod{u, u}\lambda u \\
                                                              & = \innerprod{v, u}\innerprod{u, u}\lambda^{2}u      \\
                                                              & = \innerprod{v, u}\innerprod{\lambda u, \lambda u}u \\
                                                              & = \innerprod{v, u}\innerprod{x, x}u.
                        \end{align*}

                        for every $v\in V$.
              \end{itemize}

              So $T$ is normal.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionA:exercise27
\begin{exercise}\label{chapter7:sectionA:exercise27}
    Suppose $T\in\lmap{V}$ is normal. Prove that
    \[
        \kernel{T^{k}} = \kernel{T}\quad\text{and}\quad \range{T^{k}} = \range{T}
    \]

    for every positive integer $k$.
\end{exercise}

\begin{proof}
    I give a proof using mathematical induction.

    The statement is true for $k = 1$, since $\kernel{T} = \kernel{T}$.

    Assume the statement is true for $n$. We have $\kernel{T^{n}}\subseteq\kernel{T^{n+1}}$. Because $T$ is normal, it follows that $T^{*}$ is also normal.
    \[
        V = \kernel{T^{*}}\oplus{(\kernel{T^{*}})}^{\bot} = \kernel{T^{*}}\oplus\range{T} = \kernel{T^{*}}\oplus\range{T^{*}}
    \]

    where ${(\kernel{T^{*}})}^{\bot} = \range{T} = \range{T^{*}}$.

    Let $v$ be a vector in $\kernel{T^{n+1}}$ then $T^{n+1}v = 0$. $T^{n+1}v = 0$ so $\innerprod{T^{n+1}v, w} = 0$ for every $w\in V$.

    Because $V$ is the orthogonal sum of $\kernel{T^{*}}$ and $\range{T^{*}}$, there exist unique vectors $u\in \kernel{T^{*}}$ and $\hat{u}\in\range{T^{*}}$ such that $w = u + \hat{u}$. Because $\hat{u}\in \range{T^{*}}$, there exists a vector $\hat{v}\in V$ such that $T^{*}\hat{v} = \hat{u}$.
    \begin{align*}
        \innerprod{T^{n}v, w} & = \innerprod{T^{n}v, u + \hat{u}}                          \\
                              & = \innerprod{T^{n}v, u} + \innerprod{T^{n}v, \hat{u}}      \\
                              & = \innerprod{T^{n}v, u} + \innerprod{T^{n}v, T^{*}\hat{v}} \\
                              & = 0 + \innerprod{T^{n+1}v, \hat{v}}                        \\
                              & = \innerprod{0, \hat{v}} = 0
    \end{align*}

    where $\innerprod{T^{n}v, u} = 0$ because $T^{n}v\in\range{T}$ and $u\in\kernel{T^{*}} = {(\range{T})}^{\bot}$.

    So $\innerprod{T^{n}v, w} = 0$ for every $w\in V$, hence $T^{n}v = 0$, which precisely means $v\in\kernel{T^{n}}$. Therefore $\kernel{T^{n+1}}\subseteq \kernel{T^{n}}$.

    Hence $\kernel{T^{n}} = \kernel{T^{n+1}}$. According to the induction hypothesis, $\kernel{T^{n+1}} = \kernel{T^{n}} = \kernel{T}$.

    By the principle of mathematical induction, $\kernel{T^{k}} = \kernel{T}$ for every positive integer $k$.

    Because $T$ is normal, then so is $T^{k}$, and we have
    \begin{align*}
        \range{T^{k}} & = \range{(T^{k})}^{*}       & \text{(because $T^{k}$ is normal)}             \\
                      & = {(\kernel{T^{k}})}^{\bot} & \text{(range of the adjoint map)}              \\
                      & = {(\kernel{T})}^{\bot}     & \text{(because $\kernel{T^{k}} = \kernel{T}$)} \\
                      & = \range{T^{*}}             & \text{(range of the adjoint map)}              \\
                      & = \range{T}                 & \text{(because $T$ is normal)}
    \end{align*}

    Thus $\kernel{T^{k}} = \kernel{T}$ and $\range{T^{k}} = \range{T}$ for every positive integer $k$.
\end{proof}
\newpage

% chapter7:sectionA:exercise28
\begin{exercise}
    Suppose $T\in\lmap{V}$ is normal. Prove that if $\lambda\in\mathbb{F}$, then the minimal polynomial of $T$ is not a polynomial multiple of ${(x - \lambda)}^{2}$.
\end{exercise}

\begin{quote}
    This exercise, together with the fundamental theorem of algebra, and the necessarily and sufficient condition of an operator to be diagonalizable give another proof for the complex spectral theorem.
\end{quote}

\begin{proof}
    Assume that the minimal polynomial of $T$ is divisible by ${(x - \lambda)}^{2}$ for some $\lambda\in\mathbb{F}$, then the minimal polynomial $\mu_{T}$ of $T$ is of the form $\mu_{T}(x) = {(x - \lambda)}^{2}p(x)$.

    Because ${(x - \lambda)}^{2}p(x)$ is the minimal polynomial of $T$, then there exists a vector $v\in V$ such that $(T - \lambda I)p(T)v \ne 0$. It follows that $p(T)v \ne 0$. On the other hand, ${(T - \lambda I)}^{2}(p(T)v) = 0$ so $p(T)v\in \kernel{{(T - \lambda I)}^{2}}$ but $p(T)v\notin \kernel{(T - \lambda I)}$.

    Since $T$ is normal, then $(T - \lambda I)$ is also normal. By Exercise~\ref{chapter7:sectionA:exercise27}, $\kernel{(T - \lambda I)} = \kernel{{(T - \lambda I)}^{2}}$. Hence $p(T)v\in \kernel{{(T - \lambda I)}^{2}}$ but $p(T)v\notin \kernel{(T - \lambda I)}$ is indeed a contradiction, so the assumption is false.

    Thus the minimal polynomial of a normal operator does not have a multiple root.
\end{proof}
\newpage

% chapter7:sectionA:exercise29
\begin{exercise}
    Prove or give a counterexample: If $T\in \lmap{V}$ and there is an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ such that $\norm{Te_{k}} = \norm{T^{*}e_{k}}$ for each $k = 1,\ldots, n$, then $T$ is normal.
\end{exercise}

\begin{proof}
    I give a counterexample.

    Let $V = \mathbb{C}^{2}$, $e_{1}, e_{2}$ is the standard basis, and $T(x, y) = (x + y, - x - y)$ then $T^{*}(x, y) = (x - y, x - y)$.

    $\norm{Te_{1}} = \norm{T^{*}e_{1}} = \sqrt{2}$, $\norm{Te_{2}} = \norm{T^{*}e_{2}} = \sqrt{2}$. However
    \[
        \norm{T(e_{1} + e_{2})} = \norm{T(1, 1)} = \norm{(2, -2)} = \sqrt{8} \ne 0 = \norm{T^{*}(1, 1)} = \norm{T^{*}(e_{1} + e_{2})}
    \]

    so $T$ is not a normal operator.
\end{proof}
\newpage

% chapter7:sectionA:exercise30
\begin{exercise}
    Suppose that $T\in\lmap{\mathbb{F}^{3}}$ is normal and $T(1, 1, 1) = (2, 2, 2)$. Suppose $(z_{1}, z_{2}, z_{3})\in\kernel{T}$. Prove that $z_{1} + z_{2} + z_{3} = 0$.
\end{exercise}

\begin{proof}
    If $z_{1} = z_{2} = z_{3} = 0$ then $z_{1} + z_{2} + z_{3} = 0$.

    If $z_{1}, z_{2}, z_{3}$ are not simultaneously zero, then $(z_{1}, z_{2}, z_{3})$ is an eigenvector of $T$ corresponding to the eigenvalue $0$. $(1, 1, 1)$ is an eigenvector of $T$ corresponding to the eigenvalue $2$. Because $T$ is normal, it follows that the eigenvectors of $T$ with respect to different eigenvalues of $T$ are orthogonal, so
    \[
        0 = \innerprod{(z_{1}, z_{2}, z_{3}), (1, 1, 1)} = z_{1} + z_{2} + z_{3}.
    \]

    Thus $z_{1} + z_{2} + z_{3} = 0$.
\end{proof}
\newpage

% chapter7:sectionA:exercise31
\begin{exercise}\label{chapter7:sectionA:exercise31}
    Fix a positive integer $n$. In the inner product space of continuous real-valued functions on $[-\pi, \pi]$ with inner product $\innerprod{f, g} = \int^{\pi}_{-\pi}fg$, let
    \[
        V = \operatorname{span}(1, \cos x, \cos 2x, \ldots, \cos nx, \sin x, \sin 2x, \ldots, \sin nx).
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Define $D\in\lmap{V}$ by $Df = f'$. Show that $D^{*} = -D$. Conclude that $D$ is normal but not self-adjoint.
        \item Define $T\in\lmap{V}$ by $Tf = f''$. Show that $T$ is self-adjoint.
    \end{enumerate}
\end{exercise}

\begin{proof}
    An orthonormal basis of $V$ is
    \[
        \frac{1}{\sqrt{2\pi}}, \frac{\cos x}{\sqrt{\pi}}, \frac{\cos 2x}{\sqrt{\pi}}, \ldots, \frac{\cos nx}{\sqrt{\pi}}, \frac{\sin x}{\sqrt{\pi}}, \frac{\sin 2x}{\sqrt{\pi}}, \ldots, \frac{\sin nx}{\sqrt{\pi}}.
    \]

    The matrix of $D$ with respect to this basis has $(2n+1)$ rows and $(2n+1)$ columns, where
    \[
        {\mathcal{M}(D)}_{1+k, 1+2k} = k\qquad {\mathcal{M}(D)}_{1+2k, 1+k} = -k
    \]

    for $1\leq k\leq n$, and the other entries are zero. $\mathcal{M}(D) + {(\mathcal{M}(D))}^{*} = 0$ so $D^{*} = -D$. Therefore $D^{*}D = (-D)D = D(-D) = DD^{*}$ and $D^{*} = -D\ne D$, hence $D$ is normal but not self-adjoint.

    $T = D^{2}$ so $T^{*} = D^{*}D^{*} = (-D)(-D) = D^{2}$, therefore $T = T^{*}$, so $T$ is self-adjoint.
\end{proof}
\newpage

% chapter7:sectionA:exercise32
\begin{exercise}
    Suppose $T: V \to W$ is a linear map. Show that under the standard identification of $V$ with $V'$ (see 6.58) and the corresponding identification of $W$ with $W'$, the adjoint map $T^{*}: W\to V$ corresponds to the dual map $T': W'\to V'$. More precisely, show that
    \[
        T'(\varphi_{w}) = \varphi_{T^{*}w}
    \]

    for all $w\in W$, there $\varphi_{w}$ and $\varphi_{T^{*}w}$ are defined as in 6.58.
\end{exercise}

\begin{proof}
    For every vector $v\in V$, we have
    \begin{align*}
        T'(\varphi_{w})(v) & = \varphi_{w}(Tv)       & \text{(definition of dual map)}           \\
                           & = \innerprod{Tv, w}     & \text{(definition of $\varphi_{w}$)}      \\
                           & = \innerprod{v, T^{*}w} & \text{(definition of adjoint map)}        \\
                           & = \varphi_{T^{*}w}(v)   & \text{(definition of $\varphi_{T^{*}w}$)}
    \end{align*}

    so $T'(\varphi_{w}) = \varphi_{T^{*}w}$. Therefore the adjoint map $T^{*}$ corresponds to the dual map $T'$.
\end{proof}
\newpage

\section{The Spectral Theorem}

% chapter7:sectionB:exercise1
\begin{exercise}
    Prove that a normal operator on a complex inner product space is self-adjoint if and only if all its eigenvalues are real.
\end{exercise}

\begin{proof}
    Let $T$ be a normal operator on the complex inner product space $V$.

    $(\Rightarrow)$ $T$ is self-adjoint.

    Let $\lambda$ be an eigenvalue of $T$ and $v$ be a corresponding eigenvector. Since $T$ is self-adjoint, $\innerprod{Tv, v} = \innerprod{v, T^{*}v} = \innerprod{v, Tv}$.
    \[
        \lambda\innerprod{v, v} = \innerprod{\lambda v, v} = \innerprod{Tv, v} = \innerprod{v, Tv} = \innerprod{v, \lambda v} = \conj{\lambda}\innerprod{v, v}
    \]

    and because $\innerprod{v,v} > 0$, we conclude that $\lambda = \conj{\lambda}$, which means $\lambda\in\mathbb{R}$. Hence all eigenvalues of $T$ are real.

    $(\Leftarrow)$ All eigenvalues of $T$ are real.

    Because $T$ is a normal operator on a complex inner product space, then by the complex spectral theorem, there exists an orthonormal basis of the complex inner product space to which $T$ has a diagonal matrix. Since all eigenvalues of $T$ are real, so all entries on the diagonal of the diagonal matrix are real, this makes the matrix of $T$ and $T^{*}$ with respect to the orthonormal basis identical. Therefore $T = T^{*}$, equivalently, $T$ is self-adjoint.
\end{proof}
\newpage

% chapter7:sectionB:exercise2
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$. Suppose $T \in \lmap{V}$ is normal and has only one eigenvalue. Prove that $T$ is a scalar multiple of the identity operator.
\end{exercise}

\begin{proof}
    $T$ is normal so according to the complex spectral theorem, there exists an orthonormal basis of $V$ to which $T$ has a diagonal matrix. Since $T$ has only one eigenvalue (let it be $\lambda$), then the diagonal matrix is $\lambda I$. Therefore $T$ is a scalar multiple of the identity operator.
\end{proof}
\newpage

% chapter7:sectionB:exercise3
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$ is normal. Prove that the set of eigenvalues of $T$ is contained in $\{0, 1\}$ if and only if there is a subspace $U$ of $V$ such that $T = P_{U}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ The set of eigenvalues of $T$ is contained in $\{0, 1\}$.

    By the complex spectral theorem, there exists an orthonormal basis of $V$ to which $T$ has a diagonal matrix. In this basis, let $e_{1}, \ldots, e_{m}$ be the eigenvectors with respect to $0$ and $e_{m+1}, \ldots, e_{m+n}$ be the eigenvectors with respect to $1$.

    Let $U = \operatorname{span}(e_{m+1}, \ldots, e_{m+n})$ and $v$ be a vector in $V$. There exist scalars $a_{1}, \ldots, a_{m}, a_{m+1}, \ldots, a_{m+n}$ such that
    \[
        v = \underbrace{a_{1}e_{1} + \cdots + a_{m}e_{m}}_{\in U^{\bot}} + \underbrace{a_{m+1}e_{m+1} + \cdots + a_{m+n}e_{m+n}}_{\in U}.
    \]

    Then
    \[
        Tv = a_{m+1}e_{m+1} + \cdots + a_{m+n}e_{m+n} = P_{U}v.
    \]

    Hence $T = P_{U}$.

    $(\Leftarrow)$ There is a subspace $U$ of $V$ such that $T = P_{U}$.

    $V = U\oplus U^{\bot}$. Let $v$ be a vector in $V$, then there exist unique vectors $u\in U$ and $w\in U^{\bot}$ such that $v = u + w$. Then
    \[
        P_{U}^{2}v = P_{U}(P_{U}v) = P_{U}u = P_{U}(u + w) = P_{U}v.
    \]

    Therefore $P_{U}^{2} = P_{U}$, so $T^{2} = T$. $z^{2} - z$ is therefore a polynomial multiple of the minimal polynomial of $T$. $z^{2} - z$ has two roots $0$ and $1$, so the set of roots of the minimal polynomial of $T$ is contained in $\{ 0, 1 \}$. Because the eigenvalues of $T$ is precisely the set of roots of its minimal polynomial, so the set of eigenvalues of $T$ is contained in $\{ 0, 1 \}$.
\end{proof}
\newpage

% chapter7:sectionB:exercise4
\begin{exercise}
    Prove that a normal operator on a complex inner product space is skew (meaning it equals the negative of its adjoint) if and only if all its eigenvalues are purely imaginary (meaning that they have real part equal to $0$).
\end{exercise}

\begin{proof}
    Let $T$ be a normal operator on the complex inner product space $V$.

    $(\Rightarrow)$ $T$ is skew.

    $T$ is skew means $T^{*} = -T$. Let $\lambda$ be an eigenvalue of $T$ and $v$ be a corresponding eigenvector.
    \[
        \lambda\innerprod{v, v} = \innerprod{\lambda v, v} = \innerprod{Tv, v} = \innerprod{v, T^{*}v} = \innerprod{v, -Tv} = \innerprod{v, -\lambda v} = -\conj{\lambda}\innerprod{v, v}.
    \]

    Because $\innerprod{v, v} > 0$, it follows that $\lambda = -\conj{\lambda}$. Therefore $\lambda$ is purely imaginary.

    $(\Leftarrow)$ All eigenvalues of $T$ are purely imaginary.

    Because $T$ is a normal operator on a complex inner product space, then by the complex spectral theorem, there exists an orthonormal basis of the complex inner product space to which $T$ has a diagonal matrix $A$. Since all eigenvalues of $T$ are purely imaginary, then so are all entries on the diagonal of the diagonal matrix $A$. This makes $A^{*} = -A$. Therefore $T^{*} = -T$. Hence $T$ is skew.
\end{proof}
\newpage

% chapter7:sectionB:exercise5
\begin{exercise}
    Prove or give a counterexample: If $T \in \lmap{\mathbb{C}^{3}}$ is a diagonalizable operator, then $T$ is normal (with respect to the usual inner product).
\end{exercise}

\begin{proof}
    I give a counterexample.

    $Te_{1} = e_{1}$, $T(e_{1} + e_{2}) = 2(e_{1} + e_{2})$, $T(e_{1} + e_{2} + e_{3}) = 3(e_{1} + e_{2} + e_{3})$. Then $T$ has three distinct eigenvalues, so $T$ is diagonalizable.
    \[
        T(x, y, z) = (x + y + z, 2y + z, 3z).
    \]

    However, the eigenvectors corresponding to different eigenvalues of $T$ are not orthogonal. Therefore $T$ is not normal.
\end{proof}
\newpage

% chapter7:sectionB:exercise6
\begin{exercise}
    Suppose $V$ is a complex inner product space and $T \in \lmap{V}$ is a normal operator such that $T^{9} = T^{8}$. Prove that $T$ is self-adjoint and $T^{2} = T$.
\end{exercise}

\begin{proof}
    $z^{9} - z^{8} = z^{8}(z - 1)$ is a polynomial multiple of the minimal polynomial of $T$.

    Because $T$ is a normal operator on a complex inner product space, then $T$ is diagonalizable according to the complex spectral theorem. $T$ is diagonalizable, so the minimal polynomial of $T$ is a product of distinct monic polynomial of degree $1$. Hence the set of roots of the minimal polynomial of $T$ is contained in the set of roots of $z^{9} - z^{8}$. Therefore the set of roots of the minimal polynomial of $T$ is contained in $\{ 0, 1 \}$. So the minimal polynomial of $T$ is a divisor of $z^{2} - z$. Thus $T^{2} = T$.
\end{proof}
\newpage

% chapter7:sectionB:exercise7
\begin{exercise}
    Give an example of an operator $T$ on a complex vector space such that $T^{9} = T^{8}$ but $T^{2} \ne T$.
\end{exercise}

\begin{proof}
    Let $T$ be an operator on $\mathbb{C}^{3}$ such that its matrix with respect to the standard basis is
    \[
        \begin{pmatrix}
            0 & 0 & 0  \\
            1 & 0 & 0  \\
            0 & 1 & -1
        \end{pmatrix}
    \]

    which is also the companion matrix of $z^{3} - z^{2}$. Hence the minimal polynomial of $T$ is $z^{3} - z^{2}$. Therefore $T^{3} = T^{2}$ and $T^{2}\ne T$ ($z^{2} - z$ is a nontrivial proper divisor of $z^{3} - z^{2}$). $T^{3} = T^{2}$ implies $T^{9} = T^{8}$.
\end{proof}
\newpage

% chapter7:sectionB:exercise8
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$. Prove that $T$ is normal if and only if every eigenvector of $T$ is also an eigenvector of $T^{*}$.
\end{exercise}

\begin{proof}
    Let $n = \dim V$.

    $(\Rightarrow)$ $T$ is normal.

    According to the complex spectral theorem, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has a diagonal matrix. The eigenvalues corresponding to $e_{1}, \ldots, e_{n}$ are $\lambda_{1}, \ldots, \lambda_{n}$. The matrix of $T^{*}$ with respect to $e_{1}, \ldots, e_{n}$ is the conjugate transpose of the previous matrix, so it is also a diagonal matrix, and the eigenvalues corresponding to $e_{1}, \ldots, e_{n}$ are $\conj{\lambda_{1}}, \ldots, \conj{\lambda_{n}}$.

    Let $v$ be an eigenvector of $T$ with respect to an eigenvalue $\lambda$, and $a_{1}, \ldots, a_{n}$ are scalars such that $v = a_{1}e_{1} + \cdots + a_{n}e_{n}$.
    \[
        \lambda v = Tv = T(a_{1}e_{1} + \cdots + a_{n}e_{n}) = a_{1}\lambda_{1}e_{1} + \cdots + a_{n}\lambda_{n}e_{n}.
    \]

    Therefore
    \[
        (\lambda - \lambda_{1})a_{1}e_{1} + \cdots + (\lambda - \lambda_{n})a_{n}e_{n} = 0.
    \]

    Hence $(\lambda - \lambda_{i})a_{i} = 0$ for every $i\in\{1,\ldots, n\}$. On the other hand, $a_{1}, \ldots, a_{n}$ are not all zero, so there exists $i\in \{1,\ldots, n\}$ such that $\lambda = \lambda_{i}$, so
    \[
        v = \sum_{i \text{ such that } \lambda_{i} = \lambda}a_{i}e_{i}.
    \]

    Due to the matrices of $T^{*}$ and $T$ with respect to $e_{1}, \ldots, e_{n}$, we conclude that
    \[
        T^{*}v = \sum_{i \text{ such that } \lambda_{i} = \lambda}a_{i}\conj{\lambda}e_{i} = \conj{\lambda}v.
    \]

    Hence $v$ is also an eigenvector of $T^{*}$. Thus every eigenvector of $T$ is also an eigenvector of $T^{*}$.

    \bigskip

    $(\Leftarrow)$ Every eigenvector of $T$ is also an eigenvector of $T^{*}$.

    According to Schur's theorem, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has an upper-triangular matrix. Let $A = \mathcal{M}(T, (e_{1}, \ldots, e_{n}))$, then $A^{*}$ is the matrix of $T^{*}$ with respect to $e_{1}, \ldots, e_{n}$.

    $e_{1}$ is an eigenvector of $T$, so it is also an eigenvector of $T^{*}$.
    \[
        T^{*}e_{1} = \conj{A_{1,1}}e_{1} + \cdots + \conj{A_{1,n}}e_{n}
    \]

    therefore $A_{1,j} = 0$ if $j > 1$.

    Assume for every positive integer $\ell$ less than $k$, $A_{\ell,j} = 0$ if $j > \ell$ (I mean $A_{\ell,\ell+1}, \ldots, A_{\ell, n}$ are zero).

    According to the induction hypothesis
    \[
        Te_{k} = A_{1,k}e_{1} + \cdots + A_{k,k}e_{k} = A_{k,k}e_{k}
    \]

    so $e_{k}$ is an eigenvector of $T$, and also an eigenvector of $T^{*}$.
    \[
        T^{*}e_{k} = \conj{A_{k,1}}e_{1} + \cdots + \conj{A_{k,n}}e_{n} = \conj{A_{k,k}}e_{k} + \cdots + \conj{A_{k,n}}e_{n}.
    \]

    Because $e_{k}$ is an eigenvector of $T^{*}$, it follows that $\conj{A_{k,k+1}} = \cdots = \conj{A_{k,n}} = 0$. Therefore $A_{k, k+1}, \ldots, A_{k, n}$ are zero.

    By the principle of mathematical induction, we conclude that $A_{j,k} = 0$ if $k > j$. Moreover, $A$ is an upper-triangular matrix. Therefore $A$ is a diagonal matrix, and so is $A^{*}$. Hence $A$ and $A^{*}$ commute (because diagonal matrices commute), which implies $T$ and $T^{*}$ commute. Thus $T$ is a normal operator.
\end{proof}
\newpage

% chapter7:sectionB:exercise9
\begin{exercise}\label{chapter7:sectionB:exercise9}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$. Prove that $T$ is normal if and only if there exists a polynomial $p\in\mathscr{P}(\mathbb{C})$ such that $T^{*} = p(T)$.
\end{exercise}

\begin{proof}
    If there exists a polynomial $p\in\mathscr{P}(\mathbb{C})$ such that $T^{*} = p(T)$ then $T^{*}$ and $T$ commute because $p(T)$ and $T$ commute. Therefore $T$ is normal.

    Now suppose $T$ is normal. By the complex spectral theorem, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has a diagonal matrix $A$. It follows that the matrix of $T^{*}$ with respect to $e_{1}, \ldots, e_{n}$ is $A^{*}$, the conjugate transpose of $A$.

    Let $Te_{k} = \lambda_{k}e_{k}$ for each $k\in\{1,\ldots, n\}$, then $T^{*}e_{k} = \conj{\lambda_{k}}e_{k}$ for each $k\in\{1,\ldots, n\}$. Let $\lambda_{1}, \lambda_{n_{1}}, \ldots, \lambda_{n_{r}}$ be the distinct eigenvalues of $T$, where $1 < n_{1} < \cdots < n_{r}$, where $r\geq 0$. By the Lagrange interpolating polynomial theorem, there exists a polynomial $p\in\mathscr{P}(\mathbb{C})$ such that $p(\lambda_{k}) = \conj{\lambda_{k}}$ where $k\in \{ 1, n_{1}, \ldots, n_{r} \}$, therefore $p(\lambda_{k}) = \conj{\lambda_{k}}$ where $k\in \{ 1, \ldots, n \}$. It follows that $T^{*}e_{k} = p(T)e_{k}$ for each $k\in\{1,\ldots, n\}$, so $T^{*}v = p(T)v$ for every $v\in V$.

    Thus there exists a polynomial $p\in\mathscr{P}(\mathbb{C})$ such that $T^{*} = p(T)$.
\end{proof}
\newpage

% chapter7:sectionB:exercise10
\begin{exercise}
    Suppose $V$ is a complex inner product space. Prove that every normal operator on $V$ has a square root.
\end{exercise}

\begin{quote}
    An operator $S \in \lmap{V}$ is called a square root of $T\in \lmap{V}$ if $S^{2} = T$.
\end{quote}

\begin{proof}
    Let $T$ be a normal operator on $V$. By the complex spectral theorem, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ to which $T$ has a diagonal matrix. Let $Te_{k} = \lambda_{k}e_{k}$ for each $k\in\{ 1,\ldots, n \}$.

    There exists a complex number $\alpha_{k}$ such that $\alpha_{k}^{2} = \lambda_{k}$ for each $k\in\{1,\ldots, n\}$. Let
    \[
        S(x_{1}e_{1} + \cdots + x_{n}e_{n}) = x_{1}\alpha_{1}e_{1} + \cdots + x_{n}\alpha_{n}e_{n}
    \]

    then
    \begin{align*}
        S^{2}(x_{1}e_{1} + \cdots + x_{n}e_{n}) & = x_{1}\alpha_{1}^{2}e_{1} + \cdots + x_{n}\alpha_{n}^{2}e_{n} \\
                                                & = x_{1}\lambda_{1}e_{1} + \cdots + x_{n}\lambda_{n}e_{n}       \\
                                                & = T(x_{1}e_{1} + \cdots + x_{n}e_{n}).
    \end{align*}

    So $S^{2} = T$, hence $T$ has a square root.
\end{proof}
\newpage

% chapter7:sectionB:exercise11
\begin{exercise}
    Prove that every self-adjoint operator on $V$ has a cube root.
\end{exercise}

\begin{quote}
    An operator $S \in \lmap{V}$ is called a cube root of $T\in\lmap{V}$ if $S^{3} = T$.
\end{quote}

\begin{proof}
    Let $T$ be a self-adjoint operator on $V$. If $\mathbb{F} = \mathbb{R}$, we can apply the real spectral theorem. If $\mathbb{F} = \mathbb{C}$, then $T$ is normal and we can apply the complex spectral theorem.

    By the real and complex spectral theorems, there exists an orthonormal basis of $e_{1}, \ldots, e_{n}$ to which $T$ has a diagonal matrix. Let $Te_{k} = \lambda_{k}e_{k}$ for each $k\in\{ 1,\ldots, n \}$.

    There exists $\alpha_{k}\in\mathbb{F}$ such that $\alpha_{k}^{3} = \lambda_{k}$ for each $k\in\{1,\ldots, n\}$. Let
    \[
        S(x_{1}e_{1} + \cdots + x_{n}e_{n}) = x_{1}\alpha_{1}e_{1} + \cdots + x_{n}\alpha_{n}e_{n}
    \]

    then
    \begin{align*}
        S^{3}(x_{1}e_{1} + \cdots + x_{n}e_{n}) & = x_{1}\alpha_{1}^{3}e_{1} + \cdots + x_{n}\alpha_{n}^{3}e_{n} \\
                                                & = x_{1}\lambda_{1}e_{1} + \cdots + x_{n}\lambda_{n}e_{n}       \\
                                                & = T(x_{1}e_{1} + \cdots + x_{n}e_{n}).
    \end{align*}

    So $S^{3} = T$, hence $T$ has a cube root.
\end{proof}
\newpage

% chapter7:sectionB:exercise12
\begin{exercise}
    Suppose $V$ is a complex vector space and $T \in \lmap{V}$ is normal. Prove that if $S$ is an operator on $V$ that commutes with $T$, then $S$ commutes with $T^{*}$.
\end{exercise}

\begin{quote}
    The result in this exercise is called Fuglede's theorem.
\end{quote}

\begin{proof}
    By Exercise~\ref{chapter7:sectionB:exercise9}, there exists a polynomial $p\in\mathscr{P}(\mathbb{C})$ such that $T^{*} = p(T)$.

    If $S$ commutes with $T$ then $S$ commutes with $p(T)$, hence $S$ commutes with $T^{*}$.
\end{proof}
\newpage

% chapter7:sectionB:exercise13
\begin{exercise}
    Without using the complex spectral theorem, use the version of Schur's theorem that applies to two commuting operators (take $\mathcal{E} = \{T, T^{*}\}$ in Exercise~\ref{chapter6:sectionB:exercise20}) to give a different proof that if $\mathbb{F} = \mathbb{C}$ and $T \in \lmap{V}$ is normal, then $T$ has a diagonal matrix with respect to some orthonormal basis of $V$.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter6:sectionB:exercise20}, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ to which $T$ and $T^{*}$ have upper-triangular matrices.

    On the other hand, with respect to $e_{1}, \ldots, e_{n}$, the matrix of $T^{*}$ is the conjugate transpose of the matrix of $T$, so the matrices of $T$ and $T^{*}$ are lower-triangular. Therefore the matrix of $T$ with respect to the orthonormal basis $e_{1}, \ldots, e_{n}$ is a diagonal matrix.
\end{proof}
\newpage

% chapter7:sectionB:exercise14
\begin{exercise}\label{chapter7:sectionB:exercise14}
    Suppose $\mathbb{F} = \mathbb{R}$ and $T \in \lmap{V}$. Prove that $T$ is self-adjoint if and only if all pairs of eigenvectors corresponding to distinct eigenvalues of $T$ are orthogonal and $V = E(\lambda_{1}, T)\oplus \cdots\oplus E(\lambda_{m}, T)$, where $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct eigenvalues of $T$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $T$ is self-adjoint.

    Let $v_{i}, v_{j}$ be eigenvectors with respect to distinct eigenvalues $\lambda_{i}$ and $\lambda_{j}$ of $T$.
    \begin{align*}
        (\lambda_{i} - \lambda_{j})\innerprod{v_{i}, v_{j}} & = \innerprod{\lambda_{i}v_{i}, v_{j}} - \innerprod{v_{i}, \lambda_{j}v_{j}} \\
                                                            & = \innerprod{Tv_{i}, v_{j}} - \innerprod{v_{i}, Tv_{j}}                     \\
                                                            & = \innerprod{v_{i}, Tv_{j}} - \innerprod{v_{i}, Tv_{j}}                     \\
                                                            & = 0.
    \end{align*}

    Hence all pairs of eigenvectors corresponding to distinct eigenvalues of $T$ are orthogonal.

    If $T$ is self-adjoint then according to the real spectral theorem, $T$ is diagonalizable. So if $\lambda_{1}, \ldots, \lambda_{m}$ are the distinct eigenvalues of $T$, then $V = E(\lambda_{1}, T)\oplus \cdots\oplus E(\lambda_{m}, T)$.

    \bigskip
    $(\Leftarrow)$ All pairs of eigenvectors corresponding to distinct eigenvalues of $T$ are orthogonal and $V = E(\lambda_{1}, T)\oplus \cdots\oplus E(\lambda_{m}, T)$, where $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct eigenvalues of $T$.

    Combine orthonormal bases of $E(\lambda_{1}, T), \ldots, E(\lambda_{m}, T)$, we obtain an orthonormal basis of $V$, and each vector in this basis is an eigenvector of $T$.

    The matrix of $T$ with respect to this basis is a diagonal matrix of which all entries are real numbers, so the matrix of $T^{*}$ is the same as the matrix of $T$ with respect to this orthonormal basis. Hence $T = T^{*}$, which implies $T$ is self-adjoint.
\end{proof}
\newpage

% chapter7:sectionB:exercise15
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T \in \lmap{V}$. Prove that $T$ is normal if and only if all pairs of eigenvectors corresponding to distinct eigenvalues of $T$ are orthogonal and $V = E(\lambda_{1}, T)\oplus \cdots\oplus E(\lambda_{m}, T)$, where $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct eigenvalues of $T$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $T$ is normal.

    Let $v_{i}, v_{j}$ be eigenvectors with respect to distinct eigenvalues $\lambda_{i}$ and $\lambda_{j}$ of $T$. Because $T$ is normal, it follows that $T^{*}v_{j} = \conj{\lambda_{j}}v_{j}$.
    \begin{align*}
        (\lambda_{i} - \lambda_{j})\innerprod{v_{i}, v_{j}} & = \lambda_{i}\innerprod{v_{i}, v_{j}} - \lambda_{j}\innerprod{v_{i}, v_{j}}        \\
                                                            & = \innerprod{\lambda_{i}v_{i}, v_{j}} - \innerprod{v_{i}, \conj{\lambda_{j}}v_{j}} \\
                                                            & = \innerprod{Tv_{i}, v_{j}} - \innerprod{v_{i}, \conj{\lambda_{j}}v_{j}}           \\
                                                            & = \innerprod{v_{i}, T^{*}v_{j}} - \innerprod{v_{i}, \conj{\lambda_{j}}v_{j}}       \\
                                                            & = \innerprod{v_{i}, T^{*}v_{j} - \conj{\lambda_{j}}v_{j}}                          \\
                                                            & = 0.
    \end{align*}

    Hence $\innerprod{v_{i}, v_{j}} = 0$, which means all pairs of eigenvectors corresponding to different eigenvalues of $T$ are orthogonal.

    Because $T$ is normal, then by the complex spectral theorem, $T$ is diagonalizable. So if $\lambda_{1}, \ldots, \lambda_{m}$ are the distinct eigenvalues of $T$ then $V = E(\lambda_{1}, T)\oplus \cdots\oplus E(\lambda_{m}, T)$.

    \bigskip
    $(\Leftarrow)$ All pairs of eigenvectors corresponding to distinct eigenvalues of $T$ are orthogonal and $V = E(\lambda_{1}, T)\oplus \cdots\oplus E(\lambda_{m}, T)$, where $\lambda_{1}, \ldots, \lambda_{m}$ denote the distinct eigenvalues of $T$.

    Combine orthonormal bases of $E(\lambda_{1}, T), \ldots, E(\lambda_{m}, T)$, we obtain an orthonormal basis of $V$, and each vector in this basis is an eigenvector of $T$.

    The matrix of $T$ with respect to this basis is a diagonal matrix, and the matrix of $T^{*}$ with respect to this orthonormal basis is the conjugate transpose of the previous, which is again a diagonal matrix. Because two diagonal matrices commute so $T$ commutes with its adjoint map $T^{*}$. Hence $T$ is normal.
\end{proof}
\newpage

% chapter7:sectionB:exercise16
\begin{exercise}\label{chapter7:sectionB:exercise16}
    Suppose $\mathbb{F} = \mathbb{C}$ and $E \subseteq \lmap{V}$. Prove that there is an orthonormal basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix if and only if $S$ and $T$ are commuting normal operators for all $S, T \in \mathcal{E}$.
\end{exercise}

\begin{quote}
    This exercise extends the complex spectral theorem to the context of a collection of commuting normal operators.
\end{quote}

\begin{proof}
    Suppose that there is an orthonormal basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix, then every element of $\mathcal{E}$ is normal and all pairs of elements of $\mathcal{E}$ are commuting.

    \bigskip

    Suppose that $S$ and $T$ are commuting normal operators for all $S, T \in \mathcal{E}$. By Exercise~\ref{chapter6:sectionB:exercise20}, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which every operator in $\mathcal{E}$ has an upper-triangular matrix.

    Let $T$ be an operator in $\mathcal{E}$ and $A$ be its matrix with respect to $e_{1}, \ldots, e_{n}$. Because $T$ is normal, then $AA^{*} = A^{*}A$, it follows that the entry in the 1st row and 1st column of $AA^{*}$ and $A^{*}A$ is
    \[
        \norm{A_{1,1}}^{2} = A_{1,1}\conj{A_{1,1}} + A_{1,2}\conj{A_{1,2}} + \cdots + A_{1,n}\conj{A_{1,n}} = \norm{A_{1,1}}^{2} + \norm{A_{1,2}}^{2} + \cdots + \norm{A_{1,n}}^{2}
    \]

    so $A_{1,2}, \ldots, A_{1,n}$ are zero.

    Assume for every positive integer $\ell$ less than $k$, $A_{\ell, j} = 0$ if $j > \ell$. By the induction hypothesis and because $A$ is upper-triangular, the entries at row $k$, column $k$ of $AA^{*}$ and $A^{*}A$ are
    \begin{align*}
        A_{k,1}\conj{A_{k,1}} + \cdots + A_{k,n}\conj{A_{k,n}} & = \norm{A_{k,k}}^{2} + \cdots + \norm{A_{k,n}}^{2},                      \\
        \conj{A_{1,k}}A_{1,k} + \cdots + \conj{A_{n,k}}A_{n,k} & = \norm{A_{1,k}}^{2} + \cdots + \norm{A_{k,k}}^{2} = \norm{A_{k,k}}^{2}.
    \end{align*}

    Identify these, we obtain that $A_{k,j} = 0$ if $j > k$. So by the principle of mathematical induction, we conclude that $A_{k, j} = 0$ if $j > k$ for every $k\in\{ 1,\ldots, n \}$. Therefore $A$ is a diagonal matrix. Thus the matrix of $T$ with respect to $e_{1}, \ldots, e_{n}$ is a diagonal matrix, which is true for every element of $\mathcal{E}$.
\end{proof}
\newpage

% chapter7:sectionB:exercise17
\begin{exercise}\label{chapter7:sectionB:exercise17}
    Suppose $\mathbb{F} = \mathbb{R}$ and $\mathcal{E} \subseteq \lmap{V}$. Prove that there is an orthonormal basis of $V$ with respect to which every element of $\mathcal{E}$ has a diagonal matrix if and only if $S$ and $T$ are commuting self-adjoint operators for all $S, T \in \mathcal{E}$.
\end{exercise}

\begin{quote}
    This exercise extends the real spectral theorem to the context of a collection
    of commuting self-adjoint operators.
\end{quote}

\begin{proof}
    If there exists an orthonormal basis of $V$ to which every element of $\mathcal{E}$ has a diagonal matrix, then every element of $\mathcal{E}$ is self-adjoint, and for all pair of self-adjoint operators $S, T\in\mathcal{E}$, $S$ commutes with $T$.

    To prove the implication of the other direction, I give a proof using mathematical induction on $\dim V$.

    If $\dim V = 1$, let $v$ be a nonzero vector in $V$ then $v/\norm{v}$ is an orthonormal basis of $V$ to which every element of $\mathcal{E}$ has a diagonal matrix.

    Assume for every vector space of dimension less than $n$, for every $\mathcal{E}\subseteq\lmap{V}$ containing commuting self-adjoint operators, there is an orthonormal basis of $V$ to which every element of $\mathcal{E}$ has a diagonal matrix.

    Let $\dim V = n$. According to the real spectral theorem, each element of $\mathcal{E}$ is diagonalizable, hence each element has an eigenvalue. Let's consider the two following cases.
    \begin{itemize}
        \item Every element of $\mathcal{E}$ has exactly one eigenvalue.

              We can pick any orthonormal basis of $V$, and the matrix of every element of $\mathcal{E}$ with respect to this orthonormal basis is a diagonal matrix.
        \item There exists an element of $\mathcal{E}$ having at least two eigenvalues.

              Let $T$ be such an element of $\mathcal{E}$ and $\lambda_{1}, \ldots, \lambda_{m}$ be the distinct eigenvalues of $T$. By Exercise~\ref{chapter7:sectionB:exercise14}, $V = E(\lambda_{1}, T)\oplus \cdots \oplus E(\lambda_{m}, T)$.

              For every element $S$ of $\mathcal{E}$, $E(\lambda_{1}, T), \ldots, E(\lambda_{m}, T)$ are invariant under $S$ because $S$ commutes with $T$.

              Moreover, $E(\lambda_{1}, T), \ldots, E(\lambda_{m}, T)$ are proper subspace of $V$ because $m\geq 2$.

              By the induction hypothesis, for every $k\in\{1,\ldots, m\}$ there exists an orthonormal basis of $E(\lambda_{k}, T)$ such that every vector in this basis is an eigenvector of every element of $\mathcal{E}$ (restricted to $E(\lambda_{k}, T)$, of course).

              On the other hand, a vector in $E(\lambda_{i}, T)$ and a vector in $E(\lambda_{j}, T)$ are orthogonal. So by combining all the above orthonormal bases of $E(\lambda_{1}, T), \ldots, E(\lambda_{m}, T)$, we obtain an orthonormal basis of $V$, of which vectors are eigenvectors of every element in $\mathcal{E}$.

              Therefore every element of $\mathcal{E}$ has a diagonal matrix with respect to this orthonormal basis.
    \end{itemize}

    Thus by the principle of mathematical induction, there is an orthonormal basis of $V$, to which every element of $\mathcal{E}$ has a diagonal matrix.
\end{proof}
\newpage

% chapter7:sectionB:exercise18
\begin{exercise}
    Give an example of a real inner product space $V$, an operator $T \in \lmap{V}$, and real numbers $b, c$ with $b^{2} < 4c$ such that
    \[
        T^{2} + bT + cI
    \]

    is not invertible.
\end{exercise}

\begin{quote}
    This exercise shows that the hypothesis that $T$ is self-adjoint cannot be deleted in 7.26, even for real vector spaces.
\end{quote}

\begin{proof}
    Here is an example.

    Let $T(x, y) = (y, -x)$, then $T^{2}(x, y) = T(y, -x) = (-x, -y)$.

    Therefore $T^{2} + I = 0$, $b^{2} - 4c = 0 - 4 = -4 < 0$, and $T^{2} + I$ is not invertible.
\end{proof}
\newpage

% chapter7:sectionB:exercise19
\begin{exercise}\label{chapter7:sectionB:exercise19}
    Suppose $T \in \lmap{V}$ is self-adjoint and $U$ is a subspace of $V$ that is invariant under $T$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $U^{\bot}$ is invariant under $T$.
        \item Prove that $T\vert_{U}\in \lmap{U}$ is self-adjoint.
        \item Prove that $T\vert_{U^{\bot}}\in \lmap{U^{\bot}}$ is self-adjoint.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $w$ be a vector in $U^{\bot}$ and $u$ be a vector in $U$. Due to the definition of adjoint map, $\innerprod{u, T^{*}w} = \innerprod{Tu, w}$. Because $U$ is invariant under $T$, so $Tu$ is in $U$, therefore $\innerprod{Tu, w} = 0$. Hence $\innerprod{u, Tw} = \innerprod{u, T^{*}w} = 0$ for every $u\in U$ and $w\in U^{\bot}$, so $Tw\in U^{\bot}$ for every $w\in U^{\bot}$. Thus $U^{\bot}$ is invariant under $T$.
        \item Let $u_{1}, u_{2}$ be vectors in $U$. Because $U$ is invariant under $T$ and $T$ is self-adjoint
              \[
                  \innerprod{u_{1}, {(T\vert_{U})}^{*}u_{2}} = \innerprod{T\vert_{U}u_{1}, u_{2}} = \innerprod{Tu_{1}, u_{2}} = \innerprod{u_{1}, Tu_{2}} = \innerprod{u_{1}, T\vert_{U}u_{2}}.
              \]

              Therefore ${(T\vert_{U})}^{*} = T\vert_{U}$, hence $T\vert_{U}\in\lmap{U}$ is self-adjoint.
        \item In (b), replace $U$ with $U^{\bot}$, which is invariant under $T$, we conclude that $T\vert_{U^{\bot}}\in \lmap{U^{\bot}}$ is self-adjoint.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionB:exercise20
\begin{exercise}
    Suppose $T \in \lmap{V}$ is normal and $U$ is a subspace of $V$ that is invariant under $T$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $U^{\bot}$ is invariant under $T$.
        \item Prove that $U$ is invariant under $T^{*}$.
        \item Prove that ${(T\vert_{U})}^{*} = {(T^{*})\vert_{U}}$.
        \item Prove that $T\vert_{U}\in \lmap{U}$ and $T\vert_{U^{\bot}}\in \lmap{U^{\bot}}$ are normal operators.
    \end{enumerate}
\end{exercise}

\begin{quote}
    This exercise can be used to give yet another proof of the complex spectral theorem (use induction on $\dim V$ and the result that $T$ has an eigenvector).
\end{quote}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $e_{1}, \ldots, e_{m}$ be an orthonormal basis of $U$, and $f_{1}, \ldots, f_{n}$ be an orthonormal basis of $U^{\bot}$. Let $A$ be the matrix of $T$ with respect to this orthonormal basis $e_{1}, \ldots, e_{m}, f_{1}, \ldots, f_{n}$.
              \[
                  A =
                  \begin{pmatrix}
                      A_{1,1} & \cdots & A_{1,m} & A_{1,m+1}   & \cdots & A_{1,m+n}   \\
                      \vdots  &        & \vdots  & \vdots      &        & \vdots      \\
                      A_{m,1} & \cdots & A_{m,m} & A_{m,m+1}   & \cdots & A_{m,m+n}   \\
                      0       & \cdots & 0       & A_{m+1,m+1} & \cdots & A_{m+1,m+n} \\
                      \vdots  &        & \vdots  & \vdots      &        & \vdots      \\
                      0       & \cdots & 0       & A_{m+n,m+1} & \cdots & A_{m+n,m+n}
                  \end{pmatrix}
              \]

              Because $U$ is invariant under $T$, then $A_{j,k} = 0$ for entries that simultaneously satisfy $m+1\leq j\leq m+n$ and $1\leq k\leq m$. The matrix of $T^{*}$ with respect to the same orthonormal basis is $A^{*}$.

              Because $AA^{*} = A^{*}A$, then then ${(AA^{*})}_{r,r} = {(A^{*}A)}_{r,r}$ where $1\leq r\leq m$. By the definition of matrix multiplication, we identity the entries at row $r$, column $r$ of $AA^{*}$, $A^{*}A$, and obtain that
              \[
                  \norm{A_{r,1}}^{2} + \cdots + \norm{A_{r,m}}^{2} + \norm{A_{r,m+1}}^{2} + \cdots + \norm{A_{r,m+n}}^{2} = \norm{A_{r,1}}^{2} + \cdots + \norm{A_{r,m}}^{2}
              \]

              Therefore $A_{r,m+1} = \cdots = A_{r,m+n} = 0$. Hence
              \[
                  A =
                  \begin{pmatrix}
                      A_{1,1} & \cdots & A_{1,m} & 0           & \cdots & 0           \\
                      \vdots  &        & \vdots  & \vdots      &        & \vdots      \\
                      A_{m,1} & \cdots & A_{m,m} & 0           & \cdots & 0           \\
                      0       & \cdots & 0       & A_{m+1,m+1} & \cdots & A_{m+1,m+n} \\
                      \vdots  &        & \vdots  & \vdots      &        & \vdots      \\
                      0       & \cdots & 0       & A_{m+n,m+1} & \cdots & A_{m+n,m+n}
                  \end{pmatrix}.
              \]

              So $U^{\bot}$ is invariant under $T$.
        \item $U^{\bot}$ is invariant under $T$ so $U = {(U^{\bot})}^{\bot}$ is invariant under $T^{*}$.
        \item Let $u_{1}, u_{2}$ be vectors in $U$.
              \begin{align*}
                  \innerprod{u_{1}, {(T\vert_{U})}^{*}u_{2}} & = \innerprod{T\vert_{U}u_{1}, u_{2}}        \\
                                                             & = \innerprod{Tu_{1}, u_{2}}                 \\
                                                             & = \innerprod{u_{1}, T^{*}u_{2}}             \\
                                                             & = \innerprod{u_{1}, (T^{*})\vert_{U}u_{2}}.
              \end{align*}

              Hence ${(T\vert_{U})}^{*} = {(T^{*})}\vert_{U}$.
        \item For every vector $u\in U$
              \begin{align*}
                  (T\vert_{U}{(T\vert_{U})}^{*})u & = (T\vert_{U}{(T^{*})}\vert_{U})u & \text{(due to (c))} \\
                                                  & = T\vert_{U}(T^{*}u)                                    \\
                                                  & = T(T^{*}u) = (TT^{*})u                                 \\
                                                  & = (T^{*}T)u                                             \\
                                                  & = (T^{*})(T\vert_{U}u)                                  \\
                                                  & = ({(T^{*})\vert_{U}}T\vert_{U})u                       \\
                                                  & = ({(T\vert_{U})}^{*}T\vert_{U})u & \text{(due to (c))}
              \end{align*}

              so $T\vert_{U}$ is normal. Analogously, $T\vert_{U^{\bot}}$ is normal.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionB:exercise21
\begin{exercise}
    Suppose that $T$ is a self-adjoint operator on a finite-dimensional inner product space and that $2$ and $3$ are the only eigenvalues of $T$. Prove that
    \[
        T^{2} - 5T + 6I = 0.
    \]
\end{exercise}

\begin{proof}
    $T$ is self-adjoint so $T$ is also normal. By the real and complex spectral theorems, $T$ is diagonalizable.

    Because $T$ is diagonalizable, the minimal polynomial of $T$ is a product of distinct monic polynomial of degree $1$. Moreover, the eigenvalues of $T$ are precisely the roots of the minimal polynomial of $T$. $2$ and $3$ are the only eigenvalues of $T$.

    Therefore the minimal polynomial of $T$ is $(z - 2)(z - 3)$. Thus $T^{2} - 5T + 6I = 0$.
\end{proof}
\newpage

% chapter7:sectionB:exercise22
\begin{exercise}
    Give an example of an operator $T \in \lmap{\mathbb{C}^{3}}$ such that $2$ and $3$ are the only eigenvalues of $T$ and $T^{2} - 5T + 6I\ne 0$.
\end{exercise}

\begin{proof}
    Here is an example.

    Let $T\in\lmap{\mathbb{C}^{3}}$ such that the matrix of $T$ with respect to the standard basis is
    \[
        \begin{pmatrix}
            0 & 0 & 12  \\
            1 & 0 & -16 \\
            0 & 1 & 7
        \end{pmatrix}
    \]

    then the minimal polynomial of $T$ is ${(z-2)}^{2}(z-3)$. Hence $T^{2} - 5T + 6I\ne 0$.
\end{proof}
\newpage

% chapter7:sectionB:exercise23
\begin{exercise}
    Suppose $T \in \lmap{V}$ is self-adjoint, $\lambda \in \mathbb{F}$, and $\varepsilon > 0$. Suppose there exists $v \in V$ such that $\norm{v} = 1$ and
    \[
        \norm{Tv - \lambda v} < \varepsilon.
    \]

    Prove that $T$ has an eigenvalue $\lambda'$ such that $\abs{\lambda - \lambda'} < \varepsilon$.
\end{exercise}

\begin{quote}
    This exercise shows that for a self-adjoint operator, a number that is close to satisfying an equation that would make it an eigenvalue is close to an eigenvalue.
\end{quote}

\begin{proof}
    $T$ is self-adjoint so $T$ is also normal. By the real and complex spectral theorems, there exists an orthonormal basis of $V$ of which vectors are eigenvectors of $T$. Let the orthonormal basis be $e_{1}, \ldots, e_{n}$ and $Te_{k} = \lambda_{k}e_{k}$ for each $k\in\{1,\ldots,n\}$.

    There are scalars $a_{1}, \ldots, a_{n}$ such that $v = a_{1}e_{1} + \cdots + a_{n}e_{n}$. Because $\norm{v} = 1$, it follows that $\abs{a_{1}}^{2} + \cdots + \abs{a_{n}}^{2} = 1$.
    \begin{align*}
        \norm{Tv - \lambda v} & = \norm{a_{1}\lambda_{1}e_{1} + \cdots + a_{n}\lambda_{n}e_{n} - \lambda (a_{1}e_{1} + \cdots + a_{n}e_{n})} \\
                              & = \norm{a_{1}(\lambda_{1} - \lambda)e_{1} + \cdots + a_{n}(\lambda_{n} - \lambda)e_{n}} < \varepsilon.
    \end{align*}

    Assume $\abs{\lambda_{k} - \lambda}\geq\varepsilon$ for every $k\in\{ 1,\ldots,n \}$. By the Pythagorean theorem
    \begin{align*}
        \norm{Tv - \lambda v} & = \norm{a_{1}(\lambda_{1} - \lambda)e_{1} + \cdots + a_{n}(\lambda_{n} - \lambda)e_{n}}                           \\
                              & = \sqrt{\norm{a_{1}(\lambda_{1} - \lambda)e_{1}}^{2} + \cdots + \norm{a_{n}(\lambda_{n} - \lambda)e_{n}}^{2}}     \\
                              & = \sqrt{\abs{a_{1}}^{2}\abs{\lambda_{1} - \lambda}^{2} + \cdots + \abs{a_{n}}^{2}\abs{\lambda_{n} - \lambda}^{2}} \\
                              & \geq \sqrt{(\abs{a_{1}}^{2} + \cdots + \abs{a_{n}}^{2})\varepsilon^{2}} = \varepsilon
    \end{align*}

    and this contracts $\norm{Tv - \lambda v} < \varepsilon$. Hence there exists $k\in\{1,\ldots, n\}$ such that $\abs{\lambda - \lambda_{k}} < \varepsilon$.

    Thus $T$ has an eigenvalue $\lambda'$ such that $\abs{\lambda - \lambda'} < \varepsilon$.
\end{proof}
\newpage

% chapter7:sectionB:exercise24
\begin{exercise}
    Suppose $U$ is a finite-dimensional vector space and $T\in\lmap{U}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $\mathbb{F} = \mathbb{R}$. Prove that $T$ is diagonalizable if and only if there is a basis of $U$ such that the matrix of $T$ with respect to this basis equals its transpose.
        \item Suppose $\mathbb{F} = \mathbb{C}$. Prove that $T$ is diagonalizable if and only if there is a basis of $U$ such that the matrix of $T$ with respect to this basis commutes with its conjugate transpose.
    \end{enumerate}
\end{exercise}

\begin{quote}
    This exercise adds another equivalence to the list of conditions equivalent to diagonalizability in 5.55.
\end{quote}

\begin{proof}
    Let $n = \dim U$.

    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $T$ is diagonalizable, then there exists a basis of $U$ to which $T$ has a diagonal matrix $A$. Because $A$ is a diagonal matrix with real entries, then $A$ equals its transpose.

              Suppose there is a basis $u_{1}, \ldots, u_{n}$ of $U$ such that the matrix $A$ of $T$ with respect to this basis equals its transpose. Let $S\in\lmap{\mathbb{F}^{n}}$ such that $S: x\mapsto Ax$ then $A$ is the matrix of $S$ with respect to the standard basis of $\mathbb{F}^{n}$. The matrix of $S^{*}$ with respect to the standard basis (hence also orthonormal) of $\mathbb{F}^{n}$ is the conjugate transpose of $A$. However, all entries of $A$ are real numbers and $A = A^{\top}$ so $A = A^{*}$. Therefore $S$ is a self-adjoint operator on $\lmap{\mathbb{F}^{n}}$.

              By the real spectral theorem, $S$ is diagonalizable and there exists an orthonormal basis $f_{1}, \ldots, f_{n}$ of $\mathbb{F}^{n}$ to which the matrix of $S$ is diagonal. Let $C = \mathcal{M}(I, (f_{1}, \ldots, f_{n}), (e_{1}, \ldots, e_{n}))$, then
              \[
                  \mathcal{M}(S, (f_{1},\ldots, f_{n})) = C^{-1}AC
              \]

              Because $C$ is invertible, there exists a basis $v_{1}, \ldots, v_{n}$ of $U$ such that
              \[
                  C = \mathcal{M}(I, (v_{1}, \ldots, v_{n}), (u_{1}, \ldots, u_{n})).
              \]

              Then
              \[
                  \mathcal{M}(T, (v_{1}, \ldots, v_{n})) = C^{-1}AC = \mathcal{M}(S, (f_{1},\ldots, f_{n}))
              \]

              which is a diagonal matrix. Hence $T$ is diagonalizable.
        \item Suppose $T$ is diagonalizable, then there exists a basis of $U$ to which $T$ has a diagonal matrix $A$. Because $A$ is a diagonal matrix, then so is $A^{*}$ and $A$ commutes with $A^{*}$.

              Suppose there is a basis $u_{1}, \ldots, u_{n}$ of $U$ such that the matrix of $T$ with respect to this basis commutes with its conjugate transpose. Let $S\in\lmap{\mathbb{F}^{n}}$ such that $S: x\mapsto Ax$ then $A$ is the matrix of $S$ with respect to the standard basis (hence also orthonormal) of $\mathbb{F}^{n}$. The matrix of $S^{*}$ with respect to the standard basis of $\mathbb{F}^{n}$ is $A^{*}$. $A$ commutes with $A^{*}$ so $S$ is normal.

              By the complex spectral theorem, $S$ is diagonalizable and there exists an orthonormal basis $f_{1}, \ldots, f_{n}$ of $\mathbb{F}^{n}$ to which the matrix of $S$ is diagonal. Let $C = \mathcal{M}(I, (f_{1}, \ldots, f_{n}), (e_{1}, \ldots, e_{n}))$, then
              \[
                  \mathcal{M}(S, (f_{1},\ldots, f_{n})) = C^{-1}AC
              \]

              Because $C$ is invertible, there exists a basis $v_{1}, \ldots, v_{n}$ of $U$ such that
              \[
                  C = \mathcal{M}(I, (v_{1}, \ldots, v_{n}), (u_{1}, \ldots, u_{n})).
              \]

              Then
              \[
                  \mathcal{M}(T, (v_{1}, \ldots, v_{n})) = C^{-1}AC = \mathcal{M}(S, (f_{1},\ldots, f_{n}))
              \]

              which is a diagonal matrix. Hence $T$ is diagonalizable.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionB:exercise25
\begin{exercise}\label{chapter7:sectionB:exercise25}
    Suppose that $T \in \lmap{V}$ and there is an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ consisting of eigenvectors of $T$, with corresponding eigenvalues $\lambda_{1}, \ldots, \lambda_{n}$. Show that if $k \in \{1, \ldots, n\}$, then the pseudoinverse $T^{\dagger}$ satisfies the equation
    \[
        T^{\dagger}e_{k} = \begin{cases}
            \frac{1}{\lambda_{k}}e_{k} & \text{if $\lambda_{k}\ne 0$}, \\
            0                          & \text{if $\lambda_{k} = 0$}.
        \end{cases}
    \]
\end{exercise}

\begin{proof}
    According to the definition of the pseudoinverse map
    \[
        T^{\dagger} = {(T\vert_{{(\kernel T)}^{\bot}})}^{-1}P_{\range{T}}
    \]

    Let $v$ be a vector in $V$. $v = a_{1}e_{1} + \cdots + a_{n}e_{n}$.
    \[
        Tv = T(a_{1}e_{1} + \cdots + a_{n}e_{n}) = \sum_{k: \lambda_{k}\ne 0} a_{k}\lambda_{k}e_{k}
    \]

    so $\range T$ is the span of $e_{k}$ where $\lambda_{k}\ne 0$. $\kernel{T}$ is the span of $e_{k}$ where $\lambda_{k} = 0$. Therefore ${(\kernel{T})}^{\bot} = \range{T}$.

    If $\lambda_{k}\ne 0$
    \begin{align*}
        T^{\dagger}e_{k} & = ({(T\vert_{{(\kernel T)}^{\bot}})}^{-1}P_{\range{T}})e_{k}                                               \\
                         & = {(T\vert_{{(\kernel T)}^{\bot}})}^{-1}e_{k}                                                              \\
                         & = \frac{1}{\lambda_{k}}e_{k}                                 & \text{(since $Te_{k} = \lambda_{k}e_{k}$)}.
    \end{align*}

    If $\lambda_{k} = 0$, $T^{\dagger}e_{k} = ({(T\vert_{{(\kernel T)}^{\bot}})}^{-1}P_{\range{T}})e_{k} = {(T\vert_{{(\kernel T)}^{\bot}})}^{-1}(0) = 0$.

    Thus
    \[
        T^{\dagger}e_{k} = \begin{cases}
            \frac{1}{\lambda_{k}}e_{k} & \text{if $\lambda_{k}\ne 0$}, \\
            0                          & \text{if $\lambda_{k} = 0$}.
        \end{cases}
    \]
\end{proof}
\newpage

\section{Positive Operators}

% chapter7:sectionC:exercise1
\begin{exercise}
    Suppose $T \in \lmap{V}$. Prove that if both $T$ and $-T$ are positive operators, then $T = 0$.
\end{exercise}

\begin{proof}
    $T$ and $-T$ are positive operators, so $\innerprod{Tv, v}\geq 0$ and $\innerprod{-Tv, v}\geq 0$ for every $v\in V$. Therefore $\innerprod{Tv, v}\geq 0$ and $\innerprod{Tv, v}\leq 0$ for every $v\in V$. So $\innerprod{Tv, v} = 0$ for every $v\in V$.

    Since $T$ is self-adjoint (because $T$ is a positive operator) and $\innerprod{Tv, v} = 0$ for every $v\in V$, then $T = 0$.
\end{proof}
\newpage

% chapter7:sectionC:exercise2
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{F}^{4}}$ is the operator whose matrix (with respect to the standard basis) is
    \[
        \begin{pmatrix}
            2  & -1 & 0  & 0  \\
            -1 & 2  & -1 & 0  \\
            0  & -1 & 2  & -1 \\
            0  & 0  & -1 & 2
        \end{pmatrix}.
    \]

    Show that $T$ is an invertible positive operator.
\end{exercise}

\begin{proof}
    Because the matrix of $T$ with respect to the standard basis is symmetric and all entries are real numbers, then $T$ is self-adjoint.
    \[
        \begin{pmatrix}
            2  & -1 & 0  & 0  \\
            -1 & 2  & -1 & 0  \\
            0  & -1 & 2  & -1 \\
            0  & 0  & -1 & 2
        \end{pmatrix}
        \begin{pmatrix}
            z_{1} \\
            z_{2} \\
            z_{3} \\
            z_{4}
        \end{pmatrix} =
        \begin{pmatrix}
            2z_{1} - z_{2}          \\
            -z_{1} + 2z_{2} - z_{3} \\
            -z_{2} + 2z_{3} - z_{4} \\
            -z_{3} + 2z_{4}
        \end{pmatrix}
    \]

    So
    \begin{align*}
          & \innerprod{T(z_{1}, z_{2}, z_{3}, z_{4}), (z_{1}, z_{2}, z_{3}, z_{4})}                                                                                                                                  \\
        = & (2z_{1} - z_{2})\conj{z_{1}} + (2z_{2} - z_{1} - z_{3})\conj{z_{2}} + (2z_{3} - z_{2} - z_{4})\conj{z_{3}} + (2z_{4} - z_{3})\conj{z_{4}}                                                                \\
        = & 2\abs{z_{1}}^{2} + 2\abs{z_{2}}^{2} + 2\abs{z_{3}}^{2} + 2\abs{z_{4}}^{2} - (z_{2}\conj{z_{1}} + z_{1}\conj{z_{2}}) - (\conj{z_{2}}z_{3} + z_{2}\conj{z_{3}}) - (z_{4}\conj{z_{3}} + z_{3}\conj{z_{4}}).
    \end{align*}

    By Cauchy-Schwarz's inequality
    \[
        \begin{split}
            \abs{z_{2}\conj{z_{1}} + z_{1}\conj{z_{2}}} = \abs{\innerprod{(z_{1}, z_{2}), (z_{2}, z_{1})}} \leq \abs{z_{1}}^{2} + \abs{z_{2}}^{2} \\
            \abs{\conj{z_{2}}z_{3} + z_{2}\conj{z_{3}}} = \abs{\innerprod{(z_{2}, z_{3}), (z_{3}, z_{2})}} \leq \abs{z_{2}}^{2} + \abs{z_{3}}^{2} \\
            \abs{\conj{z_{3}}z_{4} + z_{3}\conj{z_{4}}} = \abs{\innerprod{(z_{3}, z_{4}), (z_{4}, z_{3})}} \leq \abs{z_{3}}^{2} + \abs{z_{4}}^{2}
        \end{split}
    \]

    therefore
    \[
        \innerprod{T(z_{1}, z_{2}, z_{3}, z_{4}), (z_{1}, z_{2}, z_{3}, z_{4})} \geq \abs{z_{1}}^{2} + \abs{z_{4}}^{2} \geq 0.
    \]

    $\innerprod{T(z_{1}, z_{2}, z_{3}, z_{4}), (z_{1}, z_{2}, z_{3}, z_{4})} = 0$ if and only if $\abs{z_{1}} = \abs{z_{2}} = \abs{z_{3}} = \abs{z_{4}} = 0$. Hence $T$ is a positive invertible operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise3
\begin{exercise}
    Suppose $n$ is a positive integer and $T\in\lmap{\mathbb{F}^{n}}$ is the operator whose matrix (with respect to the standard basis) consists of all $1$'s. Show that $T$ is a positive operator.
\end{exercise}

\begin{proof}
    $T$ is self-adjoint, since the matrix of $T$ with respect to the standard basis is symmetric and all of its entries are real numbers.

    Let $S$ be the operator on $\mathbb{F}^{n}$ whose matrix with respect to the standard basis consists of all $\frac{1}{\sqrt{n}}$.
    \begin{align*}
        S^{2}e_{k} & = S\left(\frac{1}{\sqrt{n}}e_{1} + \cdots + \frac{1}{\sqrt{n}}e_{n}\right)                                                                                                                                                                      \\
                   & = \underbrace{\frac{1}{\sqrt{n}}\left(\frac{1}{\sqrt{n}}e_{1} + \cdots + \frac{1}{\sqrt{n}}e_{n}\right) + \cdots + \frac{1}{\sqrt{n}}\left(\frac{1}{\sqrt{n}}e_{1} + \cdots + \frac{1}{\sqrt{n}}e_{n}\right)}_{\text{$n$ pairs of parentheses}} \\
                   & = e_{1} + \cdots + e_{n}                                                                                                                                                                                                                        \\
                   & = Te_{k}.
    \end{align*}

    Hence $T = S^{2}$. Moreover, $S$ is self-adjoint. So $T$ is a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise4
\begin{exercise}
    Suppose $n$ is an integer with $n > 1$. Show that there exists an $n$-by-$n$ matrix $A$ such that all of the entries of $A$ are positive numbers and $A = A^{*}$, but the operator on $\mathbb{F}^{n}$ whose matrix (with respect to the standard basis) equals $A$ is not a positive operator.
\end{exercise}

\begin{proof}
    Let $A_{j,j} = 1$ and $A_{j,k} = 2$ if $j\ne k$ and $(x_{1}, \ldots, x_{n})\in \mathbb{F}^{n}$.
    \begin{align*}
        \innerprod{Ax, x} & = \sum^{n}_{j=1}\left(x_{j}\conj{x_{j}} + (2x_{1} + \cdots + 2x_{n})\conj{x_{j}} - 2x_{j}\conj{x_{j}}\right) \\
                          & = \sum^{n}_{j=1}\left(-x_{j}\conj{x_{j}} + (2x_{1} + \cdots + 2x_{n})\conj{x_{j}}\right)                     \\
                          & = 2\left(\sum^{n}_{j=1}x_{j}\right)\left(\sum^{n}_{j=1}\conj{x_{j}}\right) - \sum^{n}_{j=1}\abs{x_{j}}^{2}   \\
                          & = 2\abs{\sum^{n}_{j=1}x_{j}}^{2} - \sum^{n}_{j=1}\abs{x_{j}}^{2}
    \end{align*}

    If $n$ is even, let $x_{1} = 1, x_{2} = -1, \ldots, x_{n-1} = 1, x_{n} = -1$, then $\innerprod{Ax, x} = -n < 0$.

    If $n$ is odd, let $x_{1} = 1, x_{2} = -1, \ldots, x_{n-1} = -1, x_{n} = 1$, then $\innerprod{Ax, x} = 2 - n < 0$ (since $n > 1$ and $n$ is odd).

    So the operator on $\mathbb{F}^{n}$ whose matrix equals the chosen matrix $A$ is not a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise5
\begin{exercise}
    Suppose $T\in\lmap{V}$ is self-adjoint. Prove that $T$ is a positive operator if and only if for every orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$, all entries on the diagonal of $\mathcal{M}(T, (e_{1}, \ldots, e_{n}))$ are nonnegative numbers.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $T$ is a positive operator.

    Let $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. The entries on the diagonal of $\mathcal{M}(T, (e_{1}, \ldots, e_{n}))$ are $\innerprod{Te_{i}, e_{i}}$, which are nonnegative numbers, according to the definition of positive operator.

    \bigskip
    $(\Leftarrow)$ For every orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$, all entries on the diagonal of $\mathcal{M}(T, (e_{1}, \ldots, e_{n}))$ are nonnegative numbers.

    Let $v$ be a nonzero vector of $V$ then $f_{1} = v/\norm{v}$ is an orthonormal list of $V$. Extend this list to an orthonormal basis of $V$, we obtain $f_{1}, \ldots, f_{n}$. The entry in the first row, first column of the matrix of $T$ with respect to $f_{1}, \ldots, f_{n}$ is $\innerprod{Tf_{1}, f_{1}}$.
    \[
        \innerprod{Tv, v} = \norm{v}^{2}\innerprod{T\left(\frac{v}{\norm{v}}\right), \frac{v}{\norm{v}}} = \norm{v}^{2}\innerprod{Tf_{1}, f_{1}}\geq 0.
    \]

    So $\innerprod{Tv, v}\geq 0$ for every nonzero vector $v\in V$. Moreover, $\innerprod{T(0), 0} = 0$. Hence $\innerprod{Tv, v}\geq 0$ for every vector $v\in V$. Thus $T$ is a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise6
\begin{exercise}\label{chapter7:sectionC:exercise6}
    Prove that the sum of two positive operators on $V$ is a positive operator.
\end{exercise}

\begin{proof}
    Let $S, T$ be two positive operators on $V$, then $S, T$ are self-adjoint. Therefore $S + T$ is also a self-adjoint operator.

    Let $v$ be a vector in $V$.
    \[
        \innerprod{(S + T)v, v} = \innerprod{Sv + Tv, v} = \innerprod{Sv, v} + \innerprod{Tv, v}.
    \]

    Because $\innerprod{Tv, v}\geq 0$ and $\innerprod{Sv, v}\geq 0$, it follows that $\innerprod{(S + T)v, v}\geq 0$. Thus $S + T$ is a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise7
\begin{exercise}
    Suppose $S \in \lmap{V}$ is an invertible positive operator and $T\in\lmap{V}$ is a positive operator. Prove that $S + T$ is invertible.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter7:sectionC:exercise6}, $S + T$ is a positive operator.

    Assume a vector $v\in V$ satisfies $\innerprod{(S + T)v, v} = 0$, then $\innerprod{Sv, v} + \innerprod{Tv, v} = \innerprod{(S + T)v, v} = 0$. Because $\innerprod{Sv, v}\geq 0$ and $\innerprod{Tv, v}\geq 0$, it follows that $\innerprod{Sv, v} = \innerprod{Tv, v} = 0$. $S$ is an invertible positive operator, so $v = 0$. Hence $\innerprod{(S + T)v, v} = 0$ if and only if $v = 0$. Thus $S + T$ is a positive invertible operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise8
\begin{exercise}
    Suppose $T \in \lmap{V}$. Prove that $T$ is a positive operator if and only if the pseudoinverse $T^{\dagger}$ is a positive operator.
\end{exercise}

\begin{proof}
    If $T$ is a positive operator, then $T$ is self-adjoint, and hence normal. By the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ where each of these vectors are eigenvectors of $T$. Let $Te_{k} = \lambda_{k}e_{k}$ for each $k\in\{1,\ldots,n\}$. Because $T$ is a positive operator, then $T$ is self-adjoint and $\lambda_{k}$ is a nonnegative real number for each $k\in\{1,\ldots,n\}$. By Exercise~\ref{chapter7:sectionB:exercise25}
    \[
        T^{\dagger}e_{k} = \begin{cases}
            \frac{1}{\lambda_{k}}e_{k} & \text{if $\lambda_{k}\ne 0$}, \\
            0                          & \text{if $\lambda_{k} = 0$}.
        \end{cases}
    \]

    Therefore $T^{\dagger}$ is also self-adjoint and all eigenvalues of $T^{\dagger}$ are nonnegative. Thus $T^{\dagger}$ is a positive operator.

    If $T^{\dagger}$ is a positive operator, then due to the previous implication, we conclude that ${(T^{\dagger})}^{\dagger}$ is a positive operator. By Exericse~\ref{chapter6:sectionC:exercise23}, ${(T^{\dagger})}^{\dagger} = T$, so $T$ is a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise9
\begin{exercise}
    Suppose $T \in \lmap{V}$ is a positive operator and $S \in \lmap{W, V}$. Prove that $S^{*}TS$ is a positive operator on $W$.
\end{exercise}

\begin{proof}
    The adjoint map of $S^{*}TS$ is $S^{*}T^{*}{(S^{*})}^{*} = S^{*}T^{*}S$. Moreover, $T$ is self-adjoint, so $T = T^{*}$. Therefore $S^{*}T^{*}S = S^{*}TS$. Hence $S^{*}TS$ is self-adjoint.

    Let $w$ be a vector in $W$.
    \begin{align*}
        \innerprod{(S^{*}TS)w, w} & = \innerprod{(TS)w, Sw}        & \text{(definition of adjoint map)}  \\
                                  & = \innerprod{T(Sw), Sw} \geq 0 & \text{($T$ is a positive operator)}
    \end{align*}

    Hence $S^{*}TS$ is a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise10
\begin{exercise}
    Suppose $T$ is a positive operator on $V$. Suppose $v, w \in V$ are such that
    \[
        Tv = w \qquad\text{and}\qquad Tw = v.
    \]

    Prove that $v = w$.
\end{exercise}

\begin{proof}
    Because $T$ is a positive operator, then $\innerprod{T(v - w), v - w}\geq 0$.
    \[
        0\leq \innerprod{T(v - w), v - w} = \innerprod{Tv - Tw, v - w} = \innerprod{w - v, v - w}.
    \]

    If $v\ne w$, then $\innerprod{w - v, v - w} = -\norm{v - w}^{2} < 0$, which is a contradiction. Hence $v = w$.
\end{proof}
\newpage

% chapter7:sectionC:exercise11
\begin{exercise}
    Suppose $T$ is a positive operator on $V$ and $U$ is a subspace of $V$ invariant under $T$. Prove that $T\vert_{U} \in \lmap{U}$ is a positive operator on $U$.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter7:sectionB:exercise19}, $T\vert_{U}$ is self-adjoint. Let $u$ be a vector in $U$.
    \[
        \innerprod{T\vert_{U}u, u} = \innerprod{Tu, u}\geq 0.
    \]

    Hence $T\vert_{U}\in\lmap{U}$ is a positive operator on $U$.
\end{proof}
\newpage

% chapter7:sectionC:exercise12
\begin{exercise}
    Suppose $T\in\lmap{V}$ is a positive operator. Prove that $T^{k}$ is a positive operator for every positive integer $k$.
\end{exercise}

\begin{proof}
    Because $T$ is a positive operator, then there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has a diagonal matrix (each of these vectors is an eigenvector of $T$) and all eigenvalues of $T$ are nonnegative real numbers. Let $Te_{j} = \lambda_{j}e_{j}$ for each $j\in\{ 1, \ldots, n \}$.

    So $T^{k}e_{j} = \lambda_{j}^{k}e_{j}$ for each $j\in\{ 1, \ldots, n \}$. Hence $T^{k}$ has a diagonal matrix (whose entries on the diagonal are nonnegative real numbers) with respect to the orthonormal basis $e_{1}, \ldots, e_{n}$. Thus $T^{k}$ is a positive operator for every positive integer $k$.
\end{proof}

\newpage

% chapter7:sectionC:exercise13
\begin{exercise}
    Suppose $T \in \lmap{V}$ is self-adjoint and $\alpha\in\mathbb{R}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $T - \alpha I$ is a positive operator if and only if $\alpha$ is less than or equal to every eigenvalue of $T$.
        \item Prove that $\alpha I - T$ is a positive operator if and only if $\alpha$ is greater than or equal to every eigenvalue of $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    Since $T$ is self-adjoint and hence normal, we can apply the real and complex spectral theorems, and conclude that there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has a diagonal matrix $A$. $T$ is self-adjoint so all eigenvalues of $T$ are real numbers, which implies all entries on the diagonal of $A$ are real numbers.
    \begin{enumerate}[label={(\alph*)}]
        \item The matrix of $T - \alpha I$ with respect to the orthonormal basis $e_{1}, \ldots, e_{n}$ is $A - \alpha I$. This is a diagonal matrix whose entries on the diagonal are real numbers.

              $T - \alpha I$ is a positive operator if and only if all entries on the diagonal of $A - \alpha I$ are nonnegative numbers. All entries on the diagonal of $A - \alpha I$ are nonnegative numbers if and only if $\alpha$ is less than or equal to every eigenvalue of $T$.

              Thus $T - \alpha I$ is a positive operator if and only if $\alpha$ is less than or equal to every eigenvalue of $T$.
        \item The matrix of $\alpha I - T$ with respect to the orthonormal basis $e_{1}, \ldots, e_{n}$ is $\alpha I - A$. This is a diagonal matrix whose entries on the diagonal are real numbers.

              $\alpha T - I$ is a positive operator if and only if all entries on the diagonal of $\alpha I - A$ are nonnegative numbers. All entries on the diagonal of $\alpha I - A$ are nonnegative numbers if and only if $\alpha$ is greater than or equal to every eigenvalue of $T$.

              Thus $\alpha I - T$ is a positive operator if and only if $\alpha$ is greater than or equal to every eigenvalue of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionC:exercise14
\begin{exercise}
    Suppose $T$ is a positive operator on $V$ and $v_{1}, \ldots, v_{m} \in V$. Prove that
    \[
        \sum^{m}_{j=1}\sum^{m}_{k=1}\innerprod{Tv_{k}, v_{j}} \geq 0.
    \]
\end{exercise}

\begin{proof}
    Because $T$ is a positive operator, then there exists a unique positive operator $\sqrt{T}$ such that $T = \sqrt{T}\sqrt{T}$.

    \begin{align*}
        \sum^{m}_{j=1}\sum^{m}_{k=1}\innerprod{Tv_{k}, v_{j}} & = \sum^{m}_{j=1}\sum^{m}_{k=1}\innerprod{\sqrt{T}v_{k}, \sqrt{T}v_{j}} & \text{($\sqrt{T}$ is self-adjoint)}  \\
                                                              & = \sum^{m}_{j=1}\innerprod{\sum^{m}_{k=1}\sqrt{T}v_{k}, \sqrt{T}v_{j}} & \text{(additivity of inner product)} \\
                                                              & = \innerprod{\sum^{m}_{k=1}\sqrt{T}v_{k}, \sum^{m}_{j=1}\sqrt{T}v_{j}} & \text{(additivity of inner product)} \\
                                                              & \geq 0                                                                 & \text{(positivity of inner product)}
    \end{align*}
\end{proof}
\newpage

% chapter7:sectionC:exercise15
\begin{exercise}
    Suppose $T\in\lmap{V}$ is self-adjoint. Prove that there exist positive operators $A, B\in\lmap{V}$ such that
    \[
        T = A - B\quad\text{and}\quad \sqrt{T^{*}T} = A + B\quad\text{and}\quad AB = BA = 0.
    \]
\end{exercise}

\begin{proof}
    Let $A = \frac{T + \sqrt{T^{*}T}}{2}$ and $B = \frac{\sqrt{T^{*}T} - T}{2}$, then $T = A - B$ and $\sqrt{T^{*}T} = A + B$.

    Because $T$ is self-adjoint, then by the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has a diagonal matrix $C$. The matrix of $T^{*}$ with respect to this orthonormal basis is $C^{*}$. The matrix of $\sqrt{T^{*}T}$ with respect to this orthonormal basis is also a diagonal matrix, where the entry in the $j$th row, $j$th column is $\sqrt{C_{j,j}C^{*}_{j,j}} = \abs{C_{j,j}}$. Therefore $T$ and $\sqrt{T^{*}T}$ are commuting.

    $A$ and $B$ are diagonal matrices with real entries. Because $T$ is self-adjoint, then $C_{j,j}$ is a real number for every $j\in\{1,\ldots, n\}$. The entry in the $j$th row, $j$th column of $A$ is $(C_{j,j} + \abs{C_{j,j}})/2 \geq 0$. The entry in the $j$th row, $j$th column of $B$ is $(\abs{C_{j,j}} - C_{j,j})/2 \geq 0$. Hence $A$ and $B$ are positive operators.

    Because $T = T^{*}$ and $T\sqrt{T^{*}T} = \sqrt{T^{*}T}T$, we have
    \begin{align*}
        AB & = \frac{(T + \sqrt{T^{*}T})(\sqrt{T^{*}T} - T)}{4} = \frac{T\sqrt{T^{*}T} + T^{*}T - T^{2} - \sqrt{T^{*}T}T}{4} = \frac{T\sqrt{T^{*}T} - \sqrt{T^{*}T}T}{4} = 0, \\
        BA & = \frac{(\sqrt{T^{*}T} - T)(T + \sqrt{T^{*}T})}{4} = \frac{\sqrt{T^{*}T}T - T^{2} + T^{*}T - T\sqrt{T^{*}T}}{4} = \frac{\sqrt{T^{*}T}T - T\sqrt{T^{*}T}}{4} = 0.
    \end{align*}

    Thus there exist positive operators $A, B\in\lmap{V}$ such that $T = A - B$ and $\sqrt{T^{*}T} = A + B$ and $AB = BA = 0$.
\end{proof}
\newpage

% chapter7:sectionC:exercise16
\begin{exercise}
    Suppose $T$ is a positive operator on $V$. Prove that
    \[
        \kernel{\sqrt{T}} = \kernel{T}\quad\text{and}\quad\range{\sqrt{T}} = \range{T}.
    \]
\end{exercise}

\begin{proof}
    If $v\in \kernel{\sqrt{T}}$ then $Tv = \sqrt{T}(\sqrt{T}v) = 0$, therefore $v\in\kernel{T}$, so $\kernel{\sqrt{T}}\subseteq\kernel{T}$.
    \[
        \innerprod{Tv, v} = \innerprod{\sqrt{T}(\sqrt{T}v), v} = \innerprod{\sqrt{Tv}, \sqrt{T}v}.
    \]

    Therefore if $v\in\kernel{T}$ then $\norm{\sqrt{T}v} = 0$ and it follows that $v\in\kernel{\sqrt{T}}$. So $\kernel{T}\subseteq\kernel{\sqrt{T}}$.

    Hence $\kernel{\sqrt{T}} = \kernel{T}$. On the other hand,
    \[
        \range{\sqrt{T}} = {(\kernel{\sqrt{T}})}^{\bot} = {(\kernel{T})}^{\bot} = (\range{T^{*}}) = \range{T}.
    \]

    So $\range{\sqrt{T}} = \range{T}$.
\end{proof}
\newpage

% chapter7:sectionC:exercise17
\begin{exercise}\label{chapter7:sectionC:exercise17}
    Suppose that $T \in \lmap{V}$ is a positive operator. Prove that there exists a polynomial $p$ with real coefficients such that $\sqrt{T} = p(T)$.
\end{exercise}

\begin{proof}
    $T$ is a positive operator on $V$ so there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ to which $T$ has a diagonal matrix whose entries on the diagonal are nonnegative. Let $Te_{k} = \lambda_{k}e_{k}$ for every $k\in\{1,\ldots,n\}$. $\lambda_{k}$ is nonnegative number for every $k\in\{1,\ldots,n\}$.

    The positive square root of $T$ satisfies $\sqrt{T}e_{k} = \sqrt{\lambda_{k}}e_{k}$ for every $k\in\{1,\ldots,n\}$. Assume $\lambda_{1}, \lambda_{n_{1}}, \ldots, \lambda_{n_{r}}$ are the distinct eigenvalues of $T$. By the Lagrange interpolating polynomial formula, there exists a polynomial $p$ with real coefficients such that $p(\lambda_{i}) = \sqrt{\lambda_{i}}$ where $i\in\{1, n_{1}, \ldots, n_{r}\}$. Therefore $p(\lambda_{i}) = \sqrt{\lambda_{i}}$ for each $i\in\{1,\ldots,n\}$.

    For each vector $v\in V$, there exist scalars $a_{1}, \ldots, a_{n}$ such that $v = a_{1}e_{1} + \cdots + a_{n}e_{n}$.
    \begin{align*}
        \sqrt{T}v & = \sqrt{T}(a_{1}e_{1} + \cdots + a_{n}e_{n})                           \\
                  & = a_{1}\sqrt{T}e_{1} + \cdots + a_{n}\sqrt{T}e_{n}                     \\
                  & = a_{1}\sqrt{\lambda_{1}}e_{1} + \cdots + a_{n}\sqrt{\lambda_{n}}e_{n} \\
                  & = a_{1}p(\lambda_{1})e_{1} + \cdots + a_{n}p(\lambda_{n})e_{n}         \\
                  & = a_{1}p(T)e_{1} + \cdots + a_{n}p(T)e_{n}                             \\
                  & = p(T)(a_{1}e_{1} + \cdots + a_{n}e_{n})                               \\
                  & = p(T)v.
    \end{align*}

    Hence there exists a polynomial $p$ with real coefficients such that $\sqrt{T} = p(T)$.
\end{proof}
\newpage

% chapter7:sectionC:exercise18
\begin{exercise}
    Suppose $S$ and $T$ are positive operators on $V$. Prove that $ST$ is a positive operator if and only if $S$ and $T$ commute.
\end{exercise}

\begin{proof}
    If $ST$ is a positive operator then $ST$ is self-adjoint. Therefore ${(ST)}^{*} = ST$. Moreover, ${(ST)}^{*} = T^{*}S^{*} = TS$, so $ST = TS$. Hence $S$ and $T$ commute.

    If $S$ and $T$ commute, then by Exercise~\ref{chapter7:sectionB:exercise16} and~\ref{chapter7:sectionB:exercise17}, there exists an orthonormal basis of $V$ to which $S$ and $T$ have diagonal matrices. Moreover, the entries on the diagonals of these matrices are nonnegative, so the matrix of $ST$ with respect to this orthonormal basis is also a diagonal matrix with nonnegative entries on the diagonal. Hence $ST$ is a positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise19
\begin{exercise}
    Show that the identity operator on $\mathbb{F}^{2}$ has infinitely many self-adjoint square roots.
\end{exercise}

\begin{proof}
    Let $T_{\phi}\in\lmap{\mathbb{F}^{2}}$ defined by
    \[
        T(x, y) = \begin{pmatrix}
            \cos\phi & \sin\phi  \\
            \sin\phi & -\cos\phi
        \end{pmatrix}
        \begin{pmatrix}
            x \\
            y
        \end{pmatrix}
    \]

    then $T = T^{*}$ and $T_{\phi}^{2} = I$. Thus the identity operator on $\mathbb{F}^{2}$ has infinitely many self-adjoint square roots.
\end{proof}
\newpage

% chapter7:sectionC:exercise20
\begin{exercise}\label{chapter7:sectionC:exercise20}
    Suppose $T \in \lmap{V}$ and $e_{1}, \ldots, e_{n}$ is an orthonormal basis of $V$. Prove that $T$ is a positive operator if and only if there exist $v_{1}, \ldots, v_{n} \in V$ such that
    \[
        \innerprod{Te_{k}, e_{j}} = \innerprod{v_{k}, v_{j}}
    \]

    for all $j, k = 1,\ldots, n$.
\end{exercise}

\begin{quote}
    The numbers ${\{ \innerprod{Te_{i}, e_{j}} \}}_{j,k=1,\ldots, n}$ are the entries in the matrix of $T$ with respect to the orthonormal basis $e_{1}, \ldots, e_{n}$.
\end{quote}

\begin{proof}
    $(\Rightarrow)$ $T$ is a positive operator on $V$.

    Let $v_{k} = \sqrt{T}e_{k}$ for every $k\in\{1,\ldots,n\}$ then $\innerprod{Te_{k}, e_{j}} = \innerprod{\sqrt{T}e_{k}, \sqrt{T}e_{j}} = \innerprod{v_{k}, v_{j}}$ for every $j, k = 1,\ldots,n$.

    $(\Leftarrow)$ There exist $v_{1}, \ldots, v_{n} \in V$ such that $\innerprod{Te_{k}, e_{j}} = \innerprod{v_{k}, v_{j}}$ for all $j, k = 1,\ldots, n$.
    \[
        \mathcal{M}(T, (e_{1}, \ldots, e_{n})) =
        \begin{pmatrix}
            \innerprod{Te_{1}, e_{1}} & \cdots & \innerprod{Te_{n}, e_{1}} \\
            \vdots                    &        & \vdots                    \\
            \innerprod{Te_{1}, e_{n}} & \cdots & \innerprod{Te_{n}, e_{n}}
        \end{pmatrix} =
        \begin{pmatrix}
            \innerprod{v_{1}, v_{1}} & \cdots & \innerprod{v_{n}, v_{1}} \\
            \vdots                   &        & \vdots                   \\
            \innerprod{v_{1}, v_{n}} & \cdots & \innerprod{v_{n}, v_{n}}
        \end{pmatrix}.
    \]

    Because of the conjugate symmetry and the positive property of inner product, we conclude that the matrix $\mathcal{M}(T, (e_{1}, \ldots, e_{n}))$ is identical to its conjugate transpose. Therefore $T$ is a self-adjoint operator on $V$.

    Let $v$ be a vector in $V$. There exist scalars $a_{1}, \ldots, a_{n}$ such that $v = a_{1}e_{1} + \cdots + a_{n}e_{n}$.
    \begin{align*}
        \innerprod{Tv, v} & = \innerprod{T(a_{1}e_{1} + \cdots + a_{n}e_{n}), a_{1}e_{1} + \cdots + a_{n}e_{n}} \\
                          & = \innerprod{a_{1}Te_{1} + \cdots + a_{n}Te_{n}, a_{1}e_{1} + \cdots + a_{n}e_{n}}  \\
                          & = \sum^{n}_{j=1}\sum^{n}_{k=1}\innerprod{a_{k}Te_{k}, a_{j}e_{j}}                   \\
                          & = \sum^{n}_{j=1}\sum^{n}_{k=1}a_{k}\conj{a_{j}}\innerprod{Te_{k}, e_{j}}            \\
                          & = \sum^{n}_{j=1}\sum^{n}_{k=1}a_{k}\conj{a_{j}}\innerprod{v_{k}, v_{j}}             \\
                          & = \sum^{n}_{j=1}\sum^{n}_{k=1}\innerprod{a_{k}v_{k}, a_{j}v_{j}}                    \\
                          & = \innerprod{\sum^{n}_{j=1}a_{j}v_{j}, \sum^{n}_{j=1}a_{j}v_{j}} \geq 0.
    \end{align*}

    Hence $\innerprod{Tv, v}\geq 0$ for every $v\in V$. Together with $T$ being a self-adjoint operator on $V$, we conclude that $T$ is a positive operator on $V$.
\end{proof}
\newpage

% chapter7:sectionC:exercise21
\begin{exercise}
    Suppose $n$ is a positive integer. The $n$-by-$n$ Hilbert matrix is the $n$-by-$n$ matrix whose entry in row $j$, column $k$ is $\frac{1}{j + k - 1}$. Suppose $T\in\lmap{V}$ is an operator whose matrix with respect to some orthonormal basis of $V$ is the $n$-by-$n$ Hilbert matrix. Prove that $T$ is an invertible positive operator.
\end{exercise}

\begin{proof}
    Because the $n$-by-$n$ Hilbert matrix is symmetric and all of its entries are real number, so $T$ is self-adjoint.

    Consider the inner product space $\mathscr{P}_{n-1}(\mathbb{R})$ with the inner product $\innerprod{p, q} = \int^{1}_{0}pq$. Then the matrix (whose the entry in row $j$, column $k$ is $\innerprod{x^{j-1}, x^{k-1}} = \int^{1}_{0}x^{j-1}x^{k-1}dx = \frac{1}{j + k - 1}$) is the $n$-by-$n$ Hilbert matrix.

    Let $p_{1}, \ldots, p_{n}$ be an orthonormal basis of $\mathscr{P}_{n-1}(\mathbb{R})$. I define $S\in\lmap{\mathscr{P}_{n-1}(\mathbb{R})}$ as follows:
    \[
        Sp_{k} = \sum^{n}_{j=1}\innerprod{x^{k-1}, x^{j-1}}p_{j}
    \]

    for each $k\in\{1,\ldots, n\}$. Moreover, because $p_{1}, \ldots, p_{n}$ is an orthonormal basis of $\mathscr{P}_{n-1}(\mathbb{R})$, it follows that
    \[
        Sp_{k} = \sum^{n}_{j=1}\innerprod{Sp_{k}, p_{j}}p_{j}
    \]

    so $\innerprod{Sp_{k}, p_{j}} = \innerprod{x^{k-1}, x^{j-1}}$ for each $j, k\in \{ 1,\ldots,n \}$. By Exercise~\ref{chapter7:sectionC:exercise20}, $S$ is a positive operator.

    Because $x^{0}, x^{1}, \ldots, x^{n-1}$ is linearly independent, so the columns of the following matrix
    \[
        \begin{pmatrix}
            \innerprod{x^{1-1}, x^{1-1}} & \innerprod{x^{2-1}, x^{1-1}} & \cdots & \innerprod{x^{n-1}, x^{1-1}} \\
            \innerprod{x^{1-1}, x^{2-1}} & \innerprod{x^{2-1}, x^{2-1}} & \cdots & \innerprod{x^{n-1}, x^{2-1}} \\
            \vdots                       & \vdots                       &        & \vdots                       \\
            \innerprod{x^{1-1}, x^{n-1}} & \innerprod{x^{2-1}, x^{n-1}} & \cdots & \innerprod{x^{n-1}, x^{n-1}} \\
        \end{pmatrix}
    \]

    are linearly independent. So the $n$-by-$n$ Hilbert matrix is invertible. Therefore $S$ is an invertible positive operator.

    \bigskip
    $V$ and $\mathscr{P}_{n-1}(\mathbb{R})$ are isomorphic, where an isomorphism from $V$ onto $\mathscr{P}_{n-1}(\mathbb{R})$ is defined by the linear map $f: e_{k}\mapsto p_{k}$ (for each $k\in\{1,\ldots,n\}$) where $e_{1}, \ldots, e_{n}$ is the orthonormal basis of $V$ to which the matrix of $T$ is the $n$-by-$n$ Hilbert matrix. By identifing $e_{k}$ and $p_{k}$, we identify $T$ and $S$, and we conclude that $T$ is an invertible positive operator.
\end{proof}
\newpage

% chapter7:sectionC:exercise22
\begin{exercise}
    Suppose $T \in \lmap{V}$ is a positive operator and $u \in V$ is such that $\norm{u} = 1$ and $\norm{Tu}\geq \norm{Tv}$ for all $v \in V$ with $\norm{v} = 1$ Show that $u$ is an eigenvector of $T$ corresponding to the largest eigenvalue of $T$.
\end{exercise}

\begin{proof}
    Because $T$ is a positive operator on $V$, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has a diagonal matrix whose entries on the diagonal are nonnegative. Let $Te_{k} = \lambda_{k}e_{k}$ for every $k\in\{1,\ldots,n\}$. Let $\lambda$ be the largest eigenvalue of $T$.

    There exist scalars $a_{1}, \ldots, a_{n}$ such that $u = a_{1}e_{1} + \cdots + a_{n}e_{n}$ and $\abs{a_{1}}^{2} + \cdots + \abs{a_{n}}^{2} = 1$.
    \begin{align*}
        \norm{Tu} & = \norm{T(a_{1}e_{1} + \cdots + a_{n}e_{n})}                                          \\
                  & = \norm{a_{1}Te_{1} + \cdots + a_{n}Te_{n}}                                           \\
                  & = \norm{a_{1}\lambda_{1}e_{1} + \cdots + a_{n}\lambda_{n}e_{n}}                       \\
                  & = \sqrt{\abs{a_{1}}^{2}{\lambda_{1}}^{2} + \cdots + \abs{a_{n}}^{2}{\lambda_{n}}^{2}} \\
                  & \leq \sqrt{\abs{a_{1}}^{2}{\lambda}^{2} + \cdots + \abs{a_{n}}^{2}{\lambda}^{2}}      \\
                  & = \lambda.
    \end{align*}

    Let $w$ be an eigenvector of $T$ with respect to $\lambda$, and $v = w/\norm{w}$. Due to the hypothesis, $\norm{Tu}\geq \norm{Tv} = \lambda$. Therefore $\norm{Tu} = \lambda$, and $a_{k} = 0$ if $\lambda_{k} < \lambda$. Hence
    \[
        u = \sum_{k: \lambda_{k} = \lambda}a_{k}e_{k}
    \]

    so $u$ is an eigenvector of $T$ corresponding to the largest eigenvalue $\lambda$ of $T$.
\end{proof}
\newpage

% chapter7:sectionC:exercise23
\begin{exercise}
    For $T\in\lmap{V}$ and $u, v\in V$, define ${\innerprod{u, v}}_{T} = \innerprod{Tu, v}$.
    \begin{enumerate}[label={(\alph*)}]
        \item Suppose $T\in\lmap{V}$. Prove that ${\innerprod{\cdot,\cdot}}_{T}$ is an inner product on $V$ if and only if $T$ is an invertible positive operator (with respect to the original inner product $\innerprod{\cdot, \cdot}$).
        \item Prove that every inner product on $V$ is of the form ${\innerprod{\cdot, \cdot}}_{T}$ for some positive invertible operator $T\in\lmap{V}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $(\Rightarrow)$ $\innerprod{\cdot,\cdot}_{T}$ is an inner product on $V$.

              For every $u, v\in V$
              \[
                  \innerprod{Tu, v} = \innerprod{u, v}_{T} = \conj{\innerprod{v, u}_{T}} = \conj{\innerprod{Tv, u}} = \innerprod{u, Tv}
              \]

              so $T$ is self-adjoint. On the other hand, $0\leq \innerprod{v, v}_{T} = \innerprod{Tv, v}$ for every $v\in V$ so $T$ is a positive operator on $V$.
              \begin{align*}
                  Tu = 0 & \Longleftrightarrow \innerprod{Tu, v} = 0\,\forall v\in V    \\
                         & \Longleftrightarrow \innerprod{u, v}_{T} = 0\,\forall v\in V \\
                         & \Longleftrightarrow u = 0
              \end{align*}

              so $T$ is injective.

              For each $w\in V$, $v\mapsto \innerprod{v, w}$ is a linear functional on $V$. According to Riesz's representation theorem, there exists a unique vector $u\in V$ such that $\innerprod{v, w} = \innerprod{v, u}_{T}$ for every $v\in V$. Therefore $\innerprod{u, v}_{T} = \innerprod{w, v}$ for every $v\in V$, so $Tu = w$. Hence $T$ is surjective.

              Thus $T$ is an invertible positive operator on $V$.
              \bigskip

              $(\Leftarrow)$ $T$ is an invertible positive operator.

              For all $u_{1}, u_{2}, u, v\in V$ and $\lambda\in\mathbb{F}$.
              \begin{align*}
                  \innerprod{u_{1} + u_{2}, v}_{T} & = \innerprod{T(u_{1} + u_{2}), v} = \innerprod{Tu_{1} + Tu_{2}, v}                                                                   \\
                                                   & = \innerprod{Tu_{1}, v} + \innerprod{Tu_{2}, v} = \innerprod{u_{1}, v}_{T} + \innerprod{u_{2}, v}_{T}                                \\
                  \innerprod{\lambda u, v}         & = \innerprod{T(\lambda u), v} = \innerprod{\lambda Tu, v}                                                                            \\
                                                   & = \lambda\innerprod{Tu, v} = \lambda\innerprod{u, v}_{T}                                                                             \\
                  \innerprod{v, u}_{T}             & = \innerprod{Tv, u} = \innerprod{v, Tu}                                                               & \text{($T$ is self-adjoint)} \\
                                                   & = \conj{\innerprod{Tu, v}} = \conj{\innerprod{u, v}_{T}}
              \end{align*}

              $\innerprod{v, v}_{T} = \innerprod{Tv, v}\geq 0$ for every $v\in V$ because $T$ is a positive operator.
              \begin{align*}
                  \innerprod{v, v}_{T} = 0 & \Longleftrightarrow \innerprod{Tv, v} = 0                                                                    \\
                                           & \Longleftrightarrow \innerprod{\sqrt{T}v, \sqrt{T}v} = 0                                                     \\
                                           & \Longleftrightarrow \sqrt{T}v = 0                                                                            \\
                                           & \Longleftrightarrow Tv = 0                               & \text{(because $\kernel{\sqrt{T}} = \kernel{T}$)} \\
                                           & \Longleftrightarrow v = 0                                & \text{(because $T$ is invertible)}
              \end{align*}

              Hence $\innerprod{\cdot,\cdot}_{T}$ is an inner product on $V$.
        \item Let $[\![\cdot,\cdot]\!]$ be an inner product on $V$.

              For each $v\in V$, $\varphi_{v}: u\mapsto [\![u, v]\!]$ is a linear functional on $V$. By Riesz's representation theorem, there exists a unique vector $Tv$ such that $\varphi_{v}(u) = \innerprod{u, Tv}$. Now I will prove that $T$ is an invertible linear map.

              For every $u\in V$, $\lambda\in\mathbb{F}$
              \begin{align*}
                  \innerprod{u, T(v_{1} + v_{2})} & = [\![u, v_{1} + v_{2}]\!] = [\![u, v_{1}]\!] + [\![u, v_{2}]\!] \\
                                                  & = \innerprod{u, Tv_{1}} + \innerprod{u, Tv_{2}}                  \\
                                                  & = \innerprod{u, Tv_{1} + Tv_{2}}                                 \\
                  \innerprod{u, T(\lambda v)}     & = [\![u, \lambda v]\!] = \conj{\lambda}[\![u, v]\!]              \\
                                                  & = \conj{\lambda}\innerprod{u, Tv}                                \\
                                                  & = \innerprod{u, \lambda Tv}
              \end{align*}

              hence $T(v_{1} + v_{2}) = Tv_{1} + Tv_{2}$ and $T(\lambda v) = \lambda Tv$. So $T$ is a linear map.

              $Tv = 0$ if and only if $\innerprod{u, Tv} = 0$ for every $u\in V$. $\innerprod{u, Tv} = 0$ for every $u\in V$ if and only if $[\![u, v]\!] = 0$ for every $u\in V$. $[\![u, v]\!] = 0$ for every $u\in V$ if and only if $v = 0$. Therefore $T$ is injective.

              For each $v\in V$, $u\mapsto \innerprod{u, v}$ is a linear functional on $V$. By Riesz's representation theorem, there exists a vector $w\in V$ such that $[\![u, w]\!] = \innerprod{u, v}$ for every $u\in V$. Then $Tw = v$, so $T$ is surjective. Thus $T$ is an invertible linear map.

              So for all $u, v\in V$.
              \[
                  [\![u, v]\!] = \conj{[\![v, u]\!]} = \conj{\innerprod{v, Tu}} = \innerprod{Tu, v}
              \]

              hence $\innerprod{Tu, v} = \innerprod{u, Tv}$ for all $u, v\in V$. Therefore $T$ is self-adjoint. Moreover
              \[
                  0\leq [\![v,v]\!] = \innerprod{v, Tv} = \innerprod{Tv, v}
              \]

              so $T$ is a positive operator on $V$.

              Hence for every inner product $[\![\cdot, \cdot]\!]$ on $V$, there exists an invertible positive operator $T$ on $V$ such that $[\![u, v]\!] = \innerprod{u, Tv} = \innerprod{Tu, v}$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionC:exercise24
\begin{exercise}
    Suppose $S$ and $T$ are positive operators on $V$. Prove that
    \[
        \kernel{(S + T)} = \kernel{S}\cap\kernel{T}.
    \]
\end{exercise}

\begin{proof}
    $S$ and $T$ are positive operators on $V$, then $(S + T)$ is self-adjoint and $\innerprod{(S + T)v, v} = \innerprod{Sv, v} + \innerprod{Tv, v}\geq 0$ for every $v\in V$. So $(S + T)$ is a positive operator on $V$.

    If $v\in \kernel{S}\cap \kernel{T}$ then $Sv = Tv = 0$ so $(S + T)v = 0$. Hence $v\in \kernel{(S + T)}$, so $\kernel{S}\cap\kernel{T}\subseteq \kernel{(S + T)}$.

    If $v\in \kernel{(S + T)}$ then $Sv + Tv = 0$.
    \[
        0 = \innerprod{(S + T)v, v} = \innerprod{Sv, v} + \innerprod{Tv, v}.
    \]

    Because $\innerprod{Sv, v}\geq 0$ and $\innerprod{Tv, v}\geq 0$, it follows that $\innerprod{Sv, v} = 0$ and $\innerprod{Tv, v} = 0$. Therefore $\sqrt{S}v = 0$ and $\sqrt{T}v = 0$, so $Sv = Tv = 0$. Hence $v\in \kernel{S}\cap\kernel{T}$.

    Thus $\kernel{(S + T)} = \kernel{S}\cap\kernel{T}$.
\end{proof}
\newpage

% chapter7:sectionC:exercise25
\begin{exercise}
    Let $T$ be the second derivative operator in Exercise~\ref{chapter7:sectionA:exercise31}{(b)} in Section 7A. Show that $-T$ is a positive operator.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter7:sectionA:exercise31}, $T$ is self-adjoint, so $-T$ is self-adjoint.

    Let $f$ be a function in $V$, then there exist real numbers $a_{0}, a_{1}, \ldots, a_{n}, b_{1}, \ldots, b_{n}$ such that
    \[
        f(x) = a_{0}1 + a_{1}\cos x + \cdots + a_{n}\cos nx + b_{1}\sin x + \cdots + b_{n}\sin nx
    \]

    then
    \[
        (-Tf)(x) = 0 + a_{1}\cos x + \cdots + n^{2}a_{n}\cos nx + b_{1}\sin x + \cdots + b_{n}n^{2}\sin nx
    \]

    so
    \begin{align*}
        \innerprod{-Tf, f} & = \innerprod{a_{1}\cos x, a_{1}\cos x} + \cdots + \innerprod{n^{2}a_{n}\cos nx, \cos nx}      \\
                           & + \innerprod{b_{1}\sin x, b_{1}\sin x} + \cdots + \innerprod{n^{2}b_{n}\sin nx, b_{n}\sin nx} \\
                           & \geq 0.
    \end{align*}

    Hence $-T$ is a positive operator.
\end{proof}
\newpage

\section{Isometries, Unitary Operators, and Matrix Factorization}

% chapter7:sectionD:exercise1
\begin{exercise}
    Suppose $\dim V \geq 2$ and $S \in \lmap{V, W}$. Prove that $S$ is an isometry if and only if $Se_{1}, Se_{2}$ is an orthonormal list in $W$ for every orthonormal list $e_{1}, e_{2}$ of length two in $V$.
\end{exercise}

\begin{proof}
    If $S$ is an isometry then $Se_{1}, Se_{2}$ is an orthonormal list in $W$ for every orthonormal list $e_{1}, e_{2}$ of length two in $V$.

    \bigskip
    Suppose $Se_{1}, Se_{2}$ is an orthonormal list in $W$ for every orthonormal list $e_{1}, e_{2}$ of length two in $V$. Let $v$ be a vector in $V$. If $v\ne 0$, there exists another nonzero vector $w$ in $V$ such that $v$ and $w$ are linearly independent (since $\dim V\geq 2$). By apply the Gram-Schmidt's procedure to $v, w$, we obtain $v/\norm{v}$ and another vector $w_{0}$. Therefore $S(v/\norm{v}), Sw_{0}$ is an orthonormal list of length two, and
    \[
        \norm{Sv} = \norm{\norm{v}S\left(\frac{v}{\norm{v}}\right)} = \norm{v}\norm{S\left(\frac{v}{\norm{v}}\right)} = \norm{v}.
    \]

    If $v = 0$ then $\norm{Sv} = \norm{0} = \norm{v}$. Therefore $\norm{Sv} = \norm{v}$, hence $S$ is an isometry.
\end{proof}
\newpage

% chapter7:sectionD:exercise2
\begin{exercise}
    Suppose $T \in \lmap{V, W}$. Prove that $T$ is a scalar multiple of an isometry if and only if $T$ preserves orthogonality.
\end{exercise}

\begin{quote}
    The phrase ``$T$ preserves orthogonality'' means that $\innerprod{Tu, Tv} = 0$ for all $u, v \in V$ such that $\innerprod{u, v} = 0$.
\end{quote}

\begin{proof}
    If $T$ is a scalar multiple of an isometry $S$, then $T = a S$ for some scalar $a\in\mathbb{F}$. If $\innerprod{u, v} = 0$ then $\innerprod{Tu, Tv} = \innerprod{aSu, aSv} = a\conj{a}\innerprod{Su, Sv} = 0$. So $T$ preserves orthogonality.

    \bigskip
    Suppose $T$ preserves orthogonality. Let $v$ be a nonzero vector in $V$ and $u \in {\{v\}}^{\bot}$, then $\innerprod{u, v} = 0$, it follows that
    \[
        0 = \innerprod{Tu, Tv} = \innerprod{u, (T^{*}T)v}
    \]

    for all $u\in {\{v\}}^{\bot}$. Therefore $(T^{*}T)v\in\operatorname{span}(v)$, so $v$ is an eigenvector of $T^{*}T$. Because every nonzero vector $v$ of $V$ is an eigenvector of $T^{*}T$, then $T^{*}T$ is a scalar multiple of the identity operator, according to Exercise~\ref{chapter5:sectionA:exercise26}. So there exists $\lambda\in\mathbb{F}$ such that $T^{*}T = \lambda I$.

    Moreover, $\innerprod{(T^{*}T)v, v} = \innerprod{Tv, Tv}\geq 0$ and $\innerprod{(T^{*}T)v, v} = \innerprod{\lambda v, v} = \lambda\innerprod{v, v}$ so $\lambda\geq 0$.

    If $\lambda = 0$ then $\innerprod{Tv, Tv} = \innerprod{(T^{*}T)v, v} = \innerprod{\lambda v, v} = 0$ for every $v\in V$, so $Tv = 0$ (definiteness of inner product) for every $v\in V$. Hence $T = 0$.

    If $\lambda \ne 0$ then $\lambda > 0$, then there exists $a\in\mathbb{F}$ such that $a\conj{a} = \lambda$. Therefore
    \[
        \left({\conj{a}}^{-1}T^{*}\right)\left(a^{-1}T\right) = I
    \]

    hence $a^{-1}T$ is an isometry. Thus, in both cases, $T$ is a scalar multiple of an isometry.
\end{proof}
\newpage

% chapter7:sectionD:exercise3
\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Show that the product of two unitary operators on $V$ is a unitary operator.
        \item Show that the inverse of a unitary operator on $V$ is a unitary operator.
    \end{enumerate}
\end{exercise}

\begin{quote}
    This exercise shows that the set of unitary operators on $V$ is a group, where the group operation is the usual product of two operators.
\end{quote}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $S, T$ be two unitary operators on $V$, then $S^{*}S = T^{*}T = I$.
              \[
                  {(ST)}^{*}(ST) = (T^{*}S^{*})(ST) = T^{*}(S^{*}S)T = T^{*}T = I.
              \]

              So $ST$ is a unitary operator on $V$. Thus the product of two unitary operators on $V$ is a unitary operator.
        \item Let $S$ be a unitary operator on $V$, then $S$ is invertible and $S^{-1} = S^{*}$. $S$ is a unitary operator on $V$ so $S^{*}$ is also a unitary operator on $V$. So $S^{-1}$ is also a unitary operator on $V$. Thus the inverse of a unitary operator on $V$ is a unitary operator on $V$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionD:exercise4
\begin{exercise}\label{chapter7:sectionD:exercise4}
    Suppose $\mathbb{F} = \mathbb{C}$ and $A, B \in \lmap{V}$ are self-adjoint. Show that $A + \iota B$ is unitary if and only if $AB = BA$ and $A^{2} + B^{2} = I$.
\end{exercise}

\begin{proof}
    Because $A, B$ are self-adjoint, it follows that the adjoint of $A + \iota B$ is $A - \iota B$.

    $(\Rightarrow)$ $A + \iota B$ is unitary.

    $A + \iota B$ is unitary if and only if
    \[
        (A + \iota B)(A - \iota B) = (A - \iota B)(A + \iota B) = I.
    \]

    Therefore $(A^{2} + B^{2}) + \iota(BA - AB) = I$ and $(A^{2} + B^{2}) + \iota (AB - BA) = I$. So
    \[
        AB - BA = \frac{1}{2\iota}(I - I) = 0
    \]

    and $A^{2} + B^{2} = (A^{2} + B^{2}) + \iota(BA - AB) = I$.

    \bigskip
    $(\Leftarrow)$ $AB = BA$ and $A^{2} + B^{2} = I$.
    \[
        \begin{split}
            (A - \iota B)(A + \iota B) = (A^{2} + B^{2}) + \iota (AB - BA) = I, \\
            (A + \iota B)(A - \iota B) = (A^{2} + B^{2}) + \iota (BA - AB) = I.
        \end{split}
    \]

    So $A + \iota B$ is unitary.
\end{proof}
\newpage

% chapter7:sectionD:exercise5
\begin{exercise}
    Suppose $S\in\lmap{V}$. Prove that the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item $S$ is a self-adjoint unitary operator.
        \item $S = 2P - I$ for some orthogonal projection $P$ on $V$.
        \item There exists a subspace $U$ of $V$ such that $Su = u$ for every $u \in U$ and $Sw = -w$ for every $w \in U^{\bot}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    I will prove $(a) \implies (b) \implies (c) \implies (a)$.

    Suppose (a) is true, then $S = S^{*}$ and $S^{*}S = SS^{*} = I$. Let $P = (I + S)/2$.
    \[
        P^{2} = \frac{1}{4}(I^{2} + IS + SI + S^{2}) = \frac{1}{4}(I + S + S + S^{*}S) = \frac{1}{4}(2I + 2S) = P.
    \]

    Because $P^{2} = P$, it follows that only $0$ and $1$ can be the eigenvalues of $P$ and $V = \kernel{P}\oplus\range{P}$.
    \[
        P^{*} = \frac{1}{2}I^{*} + \frac{1}{2}S^{*} = \frac{1}{2}I + \frac{1}{2}S = P.
    \]

    So $P$ is self-adjoint. Let $u$ be a vector in $\range{P}$ and $w$ be a vector in $\kernel{P}$. Because $u\in\range{P}$, there exists a vector $v\in V$ such that $Pv = u$.
    \[
        \innerprod{u, w} = \innerprod{Pv, w} = \innerprod{v, Pw} = \innerprod{v, 0} = 0.
    \]

    So $u$ is orthogonal to $w$ for every $u\in\range{P}$ and $w\in\kernel{P}$. Moreover, $V = \kernel{P}\oplus\range{P}$ so $\kernel{P}$ is the orthogonal complement of $\range{P}$ in $V$. Let $U = \range{P}$, then $P$ is the orthogonal projection onto $U$. Hence $S = 2P - I = 2P_{U} - I$. So (a) implies (b).

    Suppose (b) is true, then there exists a subspace $U$ of $V$ such that $P = P_{U}$. Let $u\in U$ and $w\in U^{\bot}$, then
    \begin{align*}
        Su & = (2P - I)u = 2Pu - Iu = 2u - u = u, \\
        Sw & = (2P - I)w = 2Pw - Iw = 0 - w = -w.
    \end{align*}

    So (b) implies (c).

    Suppose (c) is true.

    $V = U\oplus U^{\bot}$. Let $v_{1}, v_{2}$ be two vectors in $V$ and $v_{1} = u_{1} + w_{1}, v_{2} = u_{2} + w_{2}$ where $u_{1}, u_{2}\in U$ and $w_{1}, w_{2}\in U^{\bot}$.
    \begin{align*}
        \innerprod{Sv_{1}, v_{2}} & = \innerprod{u_{1} - w_{1}, v_{2}} = \innerprod{v_{1}, v_{2}} - \innerprod{2w_{1}, v_{2}} \\
                                  & = \innerprod{v_{1}, v_{2}} - \innerprod{w_{1}, 2v_{2}}                                    \\
                                  & = \innerprod{v_{1}, v_{2}} - \innerprod{w_{1}, 2w_{2}}                                    \\
                                  & = \innerprod{v_{1}, v_{2}} - \innerprod{v_{1}, 2w_{2}}                                    \\
                                  & = \innerprod{v_{1}, v_{2} - 2w_{2}} = \innerprod{v_{1}, u_{2} - w_{2}}                    \\
                                  & = \innerprod{v_{1}, Sv_{2}}
    \end{align*}

    therefore $S$ is self-adjoint. For every vector $v$ in $V$, there exist unique vectors $u\in U$ and $w\in U^{\bot}$ such that $v = u + w$
    \[
        (S^{*}S)v = S^{2}v = S^{2}(u + w) = S(u - w) = u + w = v.
    \]

    So $S^{*}S = I$ and $SS^{*} = S^{*}S = I$ (because $S$ is self-adjoint). Therefore $S$ is a unitary self-adjoint operator, or equivalently, (a) is true. Hence (c) implies (a).
\end{proof}
\newpage

% chapter7:sectionD:exercise6
\begin{exercise}
    Suppose $T_{1}, T_{2}$ are both normal operators on $\mathbb{F}^{3}$ with $2, 5, 7$ as eigenvalues. Prove that there exists a unitary operator $S \in \lmap{\mathbb{F}^{3}}$ such that $T_{1} = S^{*} T_{2} S$.
\end{exercise}

\begin{proof}
    $T_{1}, T_{2}$ are operators on $\mathbb{F}^{3}$ and has three distinct eigenvalues, so they are diagonalizable. Moreover, $T_{1}, T_{2}$ are normal operators, so eigenvectors of an operator with respect to distinct eigenvalues are orthogonal.

    Let $e_{1}, e_{2}, e_{3}$ be an orthonormal basis of $\mathbb{F}^{3}$ such that $T_{1}e_{1} = 2e_{1}$, $T_{1}e_{2} = 5e_{2}$, $T_{1}e_{3} = 7e_{3}$.

    Let $f_{1}, f_{2}, f_{3}$ be an orthonormal basis of $\mathbb{F}^{3}$ such that $T_{2}f_{1} = 2f_{1}$, $T_{2}f_{2} = 5f_{2}$, $T_{2}f_{3} = 7f_{3}$.

    Let's define an operator $S$ on $\mathbb{F}^{3}$ by $Se_{1} = f_{1}, Se_{2} = f_{2}, Se_{3} = f_{3}$, then $S$ is unitary, since
    \begin{align*}
        \norm{S(x_{1}e_{1} + x_{2}e_{2} + x_{3}e_{3})} & = \norm{x_{1}Se_{1} + x_{2}Se_{2} + x_{3}Se_{3}}                                                                \\
                                                       & = \sqrt{\abs{x_{1}}^{2}\norm{Se_{1}}^{2} + \abs{x_{2}}^{2}\norm{Se_{2}}^{2} + \abs{x_{3}}^{2}\norm{Se_{3}}^{2}} \\
                                                       & = \sqrt{\abs{x_{1}}^{2}\norm{f_{1}}^{2} + \abs{x_{2}}^{2}\norm{f_{2}}^{2} + \abs{x_{3}}^{2}\norm{f_{3}}^{2}}    \\
                                                       & = \sqrt{\abs{x_{1}}^{2}\norm{e_{1}}^{2} + \abs{x_{2}}^{2}\norm{e_{2}}^{2} + \abs{x_{3}}^{2}\norm{e_{3}}^{2}}    \\
                                                       & = \norm{x_{1}e_{1} + x_{2}e_{2} + x_{3}e_{3}}.
    \end{align*}

    Furthermore
    \begin{align*}
        (S^{*}T_{2}S)(x_{1}e_{1} + x_{2}e_{2} + x_{3}e_{3}) & = (S^{*}T_{2})(x_{1}f_{1} + x_{2}f_{2} + x_{3}f_{3}) \\
                                                            & = S^{*}(2x_{1}f_{1} + 5x_{2}f_{2} + 7x_{3}f_{3})     \\
                                                            & = 2x_{1}e_{1} + 5x_{2}e_{2} + 7x_{3}e_{3}            \\
                                                            & = T_{1}(x_{1}e_{1} + x_{2}e_{2} + x_{3}e_{3}).
    \end{align*}

    Hence $S^{*}T_{2}S = T_{1}$.
\end{proof}
\newpage

% chapter7:sectionD:exercise7
\begin{exercise}
    Give an example of two self-adjoint operators $T_{1}, T_{2} \in \lmap{\mathbb{F}^{4}}$ such that the eigenvalues of both operators are $2, 5, 7$ but there does not exist a unitary operator $S \in \lmap{\mathbb{F}^{4}}$ such that $T_{1} = S^{*} T_{2} S$. Be sure to explain why there is no unitary operator with the required property.
\end{exercise}

\begin{proof}
    Here is an example.
    \[
        \begin{split}
            T_{1}(x_{1}e_{1} + x_{2}e_{2} + x_{3}e_{3} + x_{4}e_{4}) & = 2x_{1}e_{1} + 5x_{2}e_{2} + 7x_{3}e_{3} + 2x_{4}e_{4}, \\
            T_{2}(x_{1}e_{1} + x_{2}e_{2} + x_{3}e_{3} + x_{4}e_{4}) & = 2x_{1}e_{1} + 5x_{2}e_{2} + 7x_{3}e_{3} + 5x_{4}e_{4}.
        \end{split}
    \]

    Assume there exists a unitary operator $S$ such that $T_{1} = S^{*} T_{2} S$.

    Let $v = S^{-1}e_{4}$ and $w = S^{-1}e_{2}$, then
    \begin{align*}
        T_{1}v & = (S^{*}T_{2}S)v = (S^{*}T_{2})e_{4} = S^{*}(5e_{4}) = S^{-1}(5e_{4}) = 5v, \\
        T_{1}w & = (S^{*}T_{2}S)w = (S^{*}T_{2})e_{2} = S^{*}(5e_{2}) = S^{-1}(5e_{2}) = 5w
    \end{align*}

    so $v, w\in E(5, T_{1}) = \operatorname{span}(e_{2})$. Hence $v, w$ is linearly dependent. However, $v = S^{-1}e_{4}$ and $w = S^{-1}e_{2}$ so $v, w$ is an orthonormal list, which implies linear independence. So the assumption is false.

    Thus there exists no unitary operator $S$ such that $T_{1} = S^{*} T_{2} S$.
\end{proof}
\newpage

% chapter7:sectionD:exercise8
\begin{exercise}
    Prove or give a counterexample: If $S\in\lmap{V}$ and there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ such that $\norm{Se_{k}} = 1$ for each $e_{k}$, then $S$ is a unitary operator.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    If $\dim V > 1$ and $S\in\lmap{V}$ is defined by $Se_{k} = e_{1}$ for every $k\in\{1,\ldots, n\}$. So $\range{S} = \operatorname{span}(e_{1})\ne V$, which means $S$ is not invertible. Hence $S$ is not a unitary operator.
\end{proof}
\newpage

% chapter7:sectionD:exercise9
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$. Suppose every eigenvalue of $T$ has absolute value $1$ and $\norm{Tv}\leq\norm{v}$ for every $v\in V$. Prove that $T$ is a unitary operator.
\end{exercise}

\begin{proof}
    By Schur's theorem, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T$ has an upper-triangular matrix $A$. Then the eigenvalues of $T$ are precisely the entries on the diagonal of $A$.

    Since $\norm{Te_{j}} = \norm{A_{1,j}e_{1} + \cdots + A_{j,j}e_{j}} = \sqrt{\abs{A_{1,j}}^{2} + \cdots + \abs{A_{j-1,j}}^{2} + \abs{A_{j,j}}^{2}} \leq \norm{e_{j}} = 1$ and $\abs{A_{j,j}} = 1$ then $A_{1,j} = \cdots = A_{j-1,j} = 0$.

    Therefore $A_{j,k} = 0$ if $j < k$. Moreover, $A$ is an upper-triangular matrix, so $A$ is a symmetric matrix whose entries on the diagonal have absolute value $1$. So $Te_{1}, \ldots, Te_{n}$ is also an orthonormal basis of $V$. Hence $T$ is a unitary operator.
\end{proof}
\newpage

% chapter7:sectionD:exercise10
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T\in\lmap{V}$ is a self-adjoint operator such that $\norm{Tv}\leq \norm{v}$ for all $v\in V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Show that $I - T^{2}$ is a positive operator.
        \item Show that $T + \iota\sqrt{I - T^{2}}$ is a unitary operator.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Because $T$ is a self-adjoint operator, then the adjoint map of $I - T^{2}$ is $I - T^{2}$, so $I - T^{2}$ is self-adjoint.

              For every $v\in V$
              \[
                  \innerprod{(I - T^{2})v, v} = \innerprod{v, v} - \innerprod{T^{2}v, v} = \norm{v}^{2} - \innerprod{Tv, Tv} = \norm{v}^{2} - \norm{Tv}^{2}\geq 0.
              \]

              Hence $I - T^{2}$ is a positive operator.
        \item By Exercise~\ref{chapter7:sectionC:exercise17}, $\sqrt{I - T^{2}}$ is a polynomial of $(I - T^{2})$, so it is also a polynomial of $T$. Therefore $\sqrt{I - T^{2}}$ commutes with $T$.

              $T\sqrt{I - T^{2}} = \sqrt{I - T^{2}}T$ and $T^{2} + {(\sqrt{I - T^{2}})}^{2} = I$, and $T$ and $\sqrt{I - T^{2}}$ are self-adjoint, so by Exercise~\ref{chapter7:sectionD:exercise4}, it follows that $T + \iota\sqrt{I - T^{2}}$ is a unitary operator.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionD:exercise11
\begin{exercise}
    Suppose $S\in\lmap{V}$. Prove that $S$ is a unitary operator if and only if
    \[
        \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \} = \{ v\in V: \norm{v}\leq 1 \}.
    \]
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ $S$ is a unitary operator.

    If $w\in \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \}$ then there exists $v\in V$ such that $\norm{v}\leq 1$ and $Sv = w$. Moreover, $\norm{w} = \norm{Sv} = \norm{v}\leq 1$ so $w\in \{ v\in V: \norm{v}\leq 1 \}$. Hence
    \[
        \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \} \subseteq \{ v\in V: \norm{v}\leq 1 \}.
    \]

    If $w\in \{ v\in V: \norm{v}\leq 1 \}$ then $\norm{w}\leq 1$. Let $v = S^{-1}w$ then $\norm{v} = \norm{S^{-1}w} = \norm{S^{*}w} = \norm{w}\leq 1$ and $\norm{Sv} = \norm{w}\leq 1$. So $w\in \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \}$. Hence
    \[
        \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \} \supseteq \{ v\in V: \norm{v}\leq 1 \}.
    \]

    Thus
    \[
        \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \} = \{ v\in V: \norm{v}\leq 1 \}.
    \]

    \bigskip
    $(\Leftarrow)$ $\{ Sv : v\in V \text{ and } \norm{v}\leq 1 \} = \{ v\in V: \norm{v}\leq 1 \}$.

    Let $A = \{ Sv : v\in V \text{ and } \norm{v}\leq 1 \}$ and $B = \{ v\in V: \norm{v}\leq 1 \}$.

    \begin{enumerate}[label={(\roman*)}]
        \item Prove that $\norm{Sv}\leq\norm{v}$ and $S$ is invertible.

              Let $v$ be a nonzero vector in $V$. $\norm{v/\norm{v}} = 1$ so $v/\norm{v}\in A$ and $v/\norm{v}\in B$. Therefore $\norm{S(v/\norm{v})}\leq 1$, which means $\norm{Sv}\leq\norm{v}$. Moreover, there exists $u\in B$ such that $Su = v/\norm{v}$, so $S(\norm{v}u) = v$, which implies $S$ is surjective. Because $S$ is an operator on a finite-dimensional vector space, then surjectivity implies invertibility. Hence $S$ is invertible.
        \item Prove that if $\norm{w} = 1$ and $Sv = w$ then $\norm{v} = 1$.

              Let $w$ be a vector in $V$ such that $\norm{w} = 1$, then $w\in B = A$, so there exists $v\in B$ such that $Sv = w$. Therefore $1 = \norm{w} = \norm{Sv}\leq \norm{v}$. Moreover, because $v\in B$, we have $\norm{v}\leq 1$. So $\norm{v} = 1$. Thus if $\norm{w} = 1$ then any $v$ in the preimage of $w$ has norm $1$.
        \item Prove that $\norm{v} = \norm{Sv}$ for $v\ne 0$.

              Let $v$ be a nonzero vector in $V$, then $Sv\ne 0$, because $S$ is invertible.
              \[
                  \norm{\frac{Sv}{\norm{Sv}}} = 1 \implies \norm{S\left(\frac{v}{\norm{Sv}}\right)} = 1
              \]

              $\dfrac{v}{\norm{Sv}}$ is in the preimage of $S\left(\dfrac{v}{\norm{Sv}}\right)$, which has norm $1$, so $\dfrac{v}{\norm{Sv}}$ also has norm $1$. Therefore $\norm{v} = \norm{Sv}$.
    \end{enumerate}

    Thus $\norm{v} = \norm{Sv}$ for every $v\in V$. Hence $S$ is a unitary operator.
\end{proof}
\newpage

% chapter7:sectionD:exercise12
\begin{exercise}
    Prove or give a counterexample: If $S\in\lmap{V}$ is invertible and $\norm{S^{-1}v} = \norm{Sv}$ for every $v\in V$, then $S$ is unitary.
\end{exercise}

\begin{proof}
    Here is a counterexample.

    $V = \mathbb{F}^{2}$, $S\in\lmap{V}$ and the matrix of $S$ with respect to the standard basis is
    \[
        \begin{pmatrix}
            1 & 1  \\
            0 & -1
        \end{pmatrix}
    \]

    then $S^{2} = I$, so $S = S^{-1}$, which implies $\norm{S^{-1}v} = \norm{Sv}$ for every $v\in V$. However, $S$ is not unitary because $S^{-1}\ne S^{*}$.
\end{proof}
\newpage

% chapter7:sectionD:exercise13
\begin{exercise}
    Explain why the columns of a square matrix of complex numbers form an orthonormal list in $\mathbb{C}^{n}$ if and only if the rows of the matrix form an orthonormal list in $\mathbb{C}^{n}$.
\end{exercise}

\begin{proof}
    Let $A$ be a square matrix of complex numbers.

    The columns of $A$ form an orthonormal list in $\mathbb{C}^{n}$ if and only if $A^{*}A = I$.

    The rows of $A$ form an orthonormal list in $\mathbb{C}^{n}$ if and only if $AA^{*} = I$.

    $A^{*}A = I$ if and only if $AA^{*} = I$, so the columns of a square matrix of complex numbers form an orthonormal list in $\mathbb{C}^{n}$ if and only if the rows of the matrix form an orthonormal list in $\mathbb{C}^{n}$.
\end{proof}
\newpage

% chapter7:sectionD:exercise14
\begin{exercise}
    Suppose $v \in V$ with $\norm{v} = 1$ and $b \in \mathbb{F}$. Also suppose $\dim V \geq 2$. Prove that there exists a unitary operator $S \in \lmap{V}$ such that $\innerprod{Sv, v} = b$ if and only if $\abs{b}\leq 1$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exists a unitary operator $S \in \lmap{V}$ such that $\innerprod{Sv, v} = b$.

    By Cauchy-Schwarz's inequality, $\abs{b} = \abs{\innerprod{Sv, v}}\leq \norm{Sv}\norm{v} = \norm{v}^{2}\leq 1$.

    $(\Leftarrow)$ $\abs{b}\leq 1$.

    Let $v = e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$. Because $\abs{b}\leq 1$ and $\dim V > 1$, there exists $f_{1}\in V$ such that $\norm{f_{1}} = 1$ and $\innerprod{f_{1}, v} = b$ (remind that $f_{1} = \innerprod{f_{1}, e_{1}}e_{1} + \cdots + \innerprod{f_{2}, e_{n}}e_{n}$).

    Let $f_{1}, \ldots, f_{n}$ be an orthonormal basis of $V$ and define $S\in\lmap{V}$ such that $Se_{k} = f_{k}$ for each $k\in\{ 1,\ldots,n \}$. Hence $S$ is a unitary operator.
\end{proof}
\newpage

% chapter7:sectionD:exercise15
\begin{exercise}
    Suppose $T$ is a unitary operator on $V$ such that $T - I$ is invertible.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $(T + I){(T - I)}^{-1}$ is a skew operator (meaning that it equals the negative of its adjoint).
        \item Prove that if $\mathbb{F} = \mathbb{C}$, then $\iota (T + I){(T - I)}^{-1}$ is a self-adjoint operator.
    \end{enumerate}
\end{exercise}

\begin{quote}
    The function $z \mapsto \iota (z + 1){(z - 1)}^{-1}$ maps the unit circle in $\mathbb{C}$ (except for the point $1$) to $\mathbb{R}$. Thus (b) illustrates the analogy between the unitary operators and the unit circle in $\mathbb{C}$, along with the analogy between the self-adjoint operators and $\mathbb{R}$.
\end{quote}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item The adjoint map of $(T + I){(T - I)}^{-1}$ is
              \begin{align*}
                  {\left((T + I){(T - I)}^{-1}\right)}^{*} & = {\left({(T - I)}^{-1}\right)}^{*}{(T + I)}^{*} \\
                                                           & = {\left({(T - I)}^{*}\right)}^{-1}(T^{*} + I)   \\
                                                           & = {(T^{*} - I)}^{-1}(T^{*} + I)                  \\
                                                           & = {(T^{*} - T^{*}T)}^{-1}(T^{*} + T^{*}T)        \\
                                                           & = {(I - T)}^{-1}{(T^{*})}^{-1}T^{*}(I + T)       \\
                                                           & = {(I - T)}^{-1}(I + T).
              \end{align*}

              Moreover, because $(I + T)(I - T) = (I - T)(I + T)$ so ${(I + T)}{(I - T)}^{-1} = {(I - T)}^{-1}(I + T)$. Hence the adjoint map of $(T + I){(T - I)}^{-1}$ is
              \[
                  (I + T){(I - T)}^{-1} = -(T + I){(T - I)}^{-1}.
              \]

              Hence ${(T + I)}{(T - I)}^{-1}$ is a skew operator.
        \item If $\mathbb{F} = \mathbb{C}$, the adjoint map of $\iota (T + I){(T - I)}^{-1}$ is
              \[
                  \bar{\iota}\left(-(T + I){(T - I)}^{-1}\right) = \iota (T + I){(T - I)}^{-1}
              \]

              so $\iota (T + I){(T - I)}^{-1}$ is a self-adjoint operator.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionD:exercise16
\begin{exercise}
    Suppose $\mathbb{F} = \mathbb{C}$ and $T \in \lmap{V}$ is self-adjoint. Prove that $(T + \iota I){(T - \iota I)}^{-1}$ is a unitary operator and $1$ is not an eigenvalue of this operator.
\end{exercise}

\begin{proof}
    Because $T$ is self-adjoint, every eigenvalue of $T$ is real, so ${(T - \iota I)}$ and ${(T + \iota I)}$ are invertible.

    The inverse of $(T + \iota I){(T - \iota I)}^{-1}$ is
    \[
        {(T - \iota I)}{(T + \iota I)}^{-1}.
    \]

    The adjoint of $(T + \iota I){(T - \iota I)}^{-1}$ is
    \[
        {(T^{*} + \iota I)}^{-1}(T^{*} - \iota I) = {(T + \iota I)}^{-1}(T - \iota I).
    \]

    Because $(T - \iota I)(T + \iota I) = (T + \iota I)(T - \iota I)$ so ${(T - \iota I)}{(T + \iota I)}^{-1} = {(T + \iota I)}^{-1}(T - \iota I)$.

    Hence the inverse and the adjoint of $(T + \iota I){(T - \iota I)}^{-1}$ are identical. Thus $(T + \iota I){(T - \iota I)}^{-1}$ is a unitary operator.

    \bigskip
    Assume $1$ is an eigenvalue of $(T + \iota I){(T - \iota I)}^{-1}$, then $1$ is also an eigenvalue of its inverse, ${(T + \iota I)}^{-1}(T - \iota I)$. Let $v$ be an eigenvector of ${(T + \iota I)}^{-1}(T - \iota I)$ corresponding to $1$, then ${(T + \iota I)}^{-1}(T - \iota I)v = v$. It follows that $(T - \iota I)v = {(T + \iota I)}v$, hence $v = 0$, which is a contradiction because an eigenvector is nonzero.

    Thus $1$ is not an eigenvalue of $(T + \iota I){(T - \iota I)}^{-1}$.
\end{proof}
\newpage

% chapter7:sectionD:exercise17
\begin{exercise}
    Explain why the characterizations of unitary matrices given by 7.57 hold.
\end{exercise}

\begin{quote}
    Suppose $Q$ is an $n$-by-$n$ matrix. The the following are equivalent.
    \begin{enumerate}[label={(\alph*)}]
        \item $Q$ is a unitary matrix.
        \item The rows of $Q$ form an orthonormal list in $\mathbb{F}^{n}$.
        \item $\norm{Qv} = \norm{v}$ for every $v\in\mathbb{F}^{n}$.
        \item $Q^{*}Q = QQ^{*} = I$, the $n$-by-$n$ matrix with $1$'s on the diagonal and $0$'s elsewhere.
    \end{enumerate}
\end{quote}

\begin{proof}
    Let $T\in\lmap{\mathbb{F}^{n}}$ be the linear operator defined by $Av = Qv$. The matrix of $T$ with respect to the standard basis of $\mathbb{F}^{n}$ is $Q$, so $Q$ is a unitary matrix if and $T$ is a unitary operator.

    Thus (a), (b), (c), (d) are equivalent, because
    \begin{enumerate}[label={(\alph*)}]
        \item $Q$ is a unitary matrix if and $T$ is a unitary operator.
        \item The rows of $Q$ form an orthonormal list in $\mathbb{F}^{n}$ if and only if $Q^{*}$ is a unitary matrix. $T^{*}$ is a unitary matrix if and only if $T^{*}$ is a unitary operator, $T^{*}$ is a unitary operator if and only if $T$ is a unitary operator.
        \item $\norm{Qv} = \norm{v}$ for every $v\in V$ if and only if $\norm{Tv} = \norm{v}$ for every $v\in V$. $\norm{Tv} = \norm{v}$ for every $v\in V$ if and only if $T$ is a unitary operator.
        \item $QQ^{*} = Q^{*}Q = I$ if and only if $TT^{*} = T^{*}T = I$. $TT^{*} = T^{*}T = I$ if and only if $T$ is a unitary operator.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionD:exercise18
\begin{exercise}
    A square matrix $A$ is called symmetric if it equals its transpose. Prove that if $A$ is a symmetric matrix with real entries, then there exists a unitary matrix $Q$ with real entries such that $Q^{*}AQ$ is a diagonal matrix.
\end{exercise}

\begin{proof}
    Let $n$ be the number of columns of $A$. Let $T\in\lmap{\mathbb{R}^{n}}$ such that the matrix of $T$ with respect to the standard basis of $\mathbb{R}^{n}$ is $A$.

    Because $A = A^{\top}$, $T$ is self-adjoint. According to the real spectral theorem, there exists an orthonormal list $v_{1}, \ldots, v_{n}$ of $\mathbb{R}^{n}$ to which $T$ has a diagonal matrix $D$.

    Let $Q = \mathcal{M}(I, (v_{1}, \ldots, v_{n}), (e_{1}, \ldots, e_{n}))$, then $Q$ is a unitary matrix and $D = Q^{*}AQ$.
\end{proof}
\newpage

% chapter7:sectionD:exercise19
\begin{exercise}
    Suppose $n$ is a positive integer. For this exercise, we adopt the notation that
    a typical element $z$ of $\mathbb{C}^{n}$ is denoted by $z = (z_{0}, z_{1}, \ldots, z_{n-1})$. Define linear functionals $\omega_{0}, \omega_{1}, \ldots, \omega_{n-1}$ on $\mathbb{C}^{n}$ by
    \[
        \omega_{j}(z_{0}, z_{1}, \ldots, z_{n-1}) = \frac{1}{\sqrt{n}}\sum^{n-1}_{m=0}z_{m}e^{-2\pi \iota jm/n}.
    \]

    The \textit{discrete Fourier transform} is the operator $\mathscr{F}: \mathbb{C}^{n}\to \mathbb{C}^{n}$ defined by
    \[
        \mathscr{F}z = (\omega_{0}(z), \omega_{1}(z), \ldots, \omega_{n-1}(z)).
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that $\mathscr{F}$ is a unitary operator on $\mathbb{C}^{n}$.
        \item Show that if $(z_{0}, \ldots, z_{n-1})\in\mathbb{C}^{n}$ and $z_{n}$ is defined to equal $z_{0}$, then
              \[
                  \mathscr{F}^{-1}(z_{0}, z_{1}, \ldots, z_{n-1}) = \mathscr{F}(z_{n}, z_{n-1}, \ldots, z_{1}).
              \]
        \item Show that $\mathscr{F}^{4} = I$.
    \end{enumerate}
\end{exercise}

\begin{quote}
    The discrete Fourier transform has many important applications in data analysis. The usual Fourier transform involves expressions of the form $\int^{\infty}_{-\infty}f(x)e^{-2\pi \iota tx}dx$ for complex-valued integrable functions $f$ defined on $\mathbb{R}$.
\end{quote}

\begin{proof}
    Let $e_{1}, \ldots, e_{n}$ be the standard basis of $\mathbb{C}^{n}$.
    \begin{align*}
        \innerprod{\mathscr{F}e_{j}, \mathscr{F}e_{k}} & = \innerprod{(\omega_{0}(e_{j}), \omega_{1}(e_{j}), \ldots, \omega_{n-1}(e_{j})), (\omega_{0}(e_{k}), \omega_{1}(e_{k}), \ldots, \omega_{n-1}(e_{k}))} \\
                                                       & = \sum^{n-1}_{m=0}\omega_{p}(e_{j})\conj{\omega_{p}(e_{k})}                                                                                            \\
                                                       & = \sum^{n-1}_{m=0}\left(\frac{1}{\sqrt{n}}e^{-2\pi\iota jm/n} \times \frac{1}{\sqrt{n}}e^{2\pi\iota km/n}\right)                                       \\
                                                       & = \frac{1}{n}\sum^{n-1}_{m=0}e^{-2\pi\iota (j - k)m/n}                                                                                                 \\
                                                       & = \delta_{j,k}
    \end{align*}

    so $\mathscr{F}e_{1}, \ldots, \mathscr{F}e_{n}$ is an orthonormal list in $\mathbb{C}^{n}$. Hence $\mathscr{F}$ is a unitary operator on $\mathbb{C}^{n}$.

    Since $z_{0} = z_{n}$.
    \begingroup
    \allowdisplaybreaks{}
    \begin{align*}
        \innerprod{(w_{0}, w_{1}, \ldots, w_{n-1}), \mathscr{F}^{*}(z_{0}, z_{1}, \ldots, z_{n-1})} & = \innerprod{\mathscr{F}(w_{0}, w_{1}, \ldots, w_{n-1}), (z_{0}, z_{1}, \ldots, z_{n-1})}                          \\
                                                                                                    & = \sum^{n-1}_{j=0}\omega_{j}(w_{0}, w_{1}, \ldots, w_{n-1})\conj{z_{j}}                                            \\
                                                                                                    & = \sum^{n-1}_{j=0}\left(\frac{1}{\sqrt{n}}\sum^{n-1}_{m=0}w_{m}e^{-2\pi\iota jm/n}\conj{z_{j}}\right)              \\
                                                                                                    & = \sum^{n-1}_{m=0}w_{m}\conj{\left(\sum^{n-1}_{j=0}\frac{1}{\sqrt{n}}z_{j}e^{2\pi\iota jm/n}\right)}               \\
                                                                                                    & = \sum^{n-1}_{m=0}w_{m}\conj{\left(\sum^{n-1}_{j=0}\frac{1}{\sqrt{n}}z_{j}e^{2\pi\iota (j - n)m/n}\right)}         \\
                                                                                                    & = \sum^{n-1}_{m=0}w_{m}\conj{\left(\sum^{n-1}_{j=0}\frac{1}{\sqrt{n}}z_{n-j}e^{2\pi\iota ((n - j) - n)m/n}\right)} \\
                                                                                                    & = \sum^{n-1}_{m=0}w_{m}\conj{\left(\sum^{n-1}_{j=0}\frac{1}{\sqrt{n}}z_{n-j}e^{-2\pi\iota jm/n}\right)}            \\
                                                                                                    & = \sum^{n-1}_{m=0}w_{m}\conj{\omega_{j}(z_{n}, z_{n-1}, \ldots, z_{1})}.
    \end{align*}
    \endgroup

    Therefore
    \begin{multline*}
        \innerprod{(w_{0}, w_{1}, \ldots, w_{n-1}), \mathscr{F}^{*}(z_{0}, z_{1}, \ldots, z_{n-1})} \\
        = \innerprod{(w_{0}, w_{1}, \ldots, w_{n-1}), (\omega_{0}(z_{n}, z_{n-1}, \ldots,z_{1}), \ldots, (\omega_{n-1}(z_{n}, z_{n-1}, \ldots,z_{1})))} \\
        = \innerprod{(w_{0}, w_{1}, \ldots, w_{n-1}), \mathscr{F}(z_{n}, z_{n-1}, \ldots, z_{1})}.
    \end{multline*}

    Together with $\mathscr{F}$ being a unitary operator, we have
    \[
        \mathscr{F}^{-1}(z_{0}, z_{1}, \ldots, z_{n-1}) = \mathscr{F}^{*}(z_{0}, z_{1}, \ldots, z_{n-1}) = \mathscr{F}(z_{n}, z_{n-1}, \ldots, z_{1}).
    \]

    By (b)
    \[
        \mathscr{F}^{4}(z_{0}, z_{1}, \ldots, z_{n-1}) = \mathscr{F}^{2}(z_{n}, z_{n-1}, \ldots, z_{1}) = (z_{0}, z_{1}, \ldots, z_{n-1})
    \]

    so $\mathscr{F}^{4} = I$.
\end{proof}
\newpage

% chapter7:sectionD:exercise20
\begin{exercise}
    Suppose $A$ is a square matrix with linearly independent columns. Prove that there exist unique matrices $R$ and $Q$ such that $R$ is lower triangular with only positive numbers on its diagonal, $Q$ is unitary, and $A = RQ$.
\end{exercise}

\begin{quote}
    You should try not using the QR-decomposition.

    Hint: Apply the Gram-Schmidt's procedure to the rows of $A$ in reversed order.
\end{quote}

\begin{proof}[Proof using QR-decomposition]
    According to the QR-decomposition theorem, there exists a upper-triangular matrix $U$ with only positive numbers on its diagonal and a unitary matrix $S$ such that $A^{*} = SU$. Therefore $A = U^{*}S^{*}$, where $S^{*}$ is a unitary matrix and $U^{*}$ is a lower-triangular matrix with only positive numbers on its diagonal.

    Assume $A = RQ$ where $R$ is lower triangular with only positive numbers on its diagonal, $Q$ is unitary, then $A^{*} = Q^{*}R^{*} = SU$. According to the QR-decomposition theorem, $Q^{*} = S$ and $R^{*} = U$, so $Q = S^{*}$ and $R = U^{*}$.

    Thus there exist unique matrices $R$ and $Q$ such that $R$ is lower triangular with only positive numbers on its diagonal, $Q$ is unitary, and $A = RQ$.
\end{proof}

\begin{proof}[Proof without QR-decomposition]
    $A$ is a square matrix and the columns of $A$ are linearly independent, so the rows of $A$ are linearly independent.

    Let $w_{1}, \ldots, w_{n}$ be the rows of $A$. Apply the Gram-Schmidt's procedure to $v_{1} = w_{n}, \ldots, v_{n} = w_{1}$ to get an orthonormal basis $e_{n}, \ldots, e_{1}$ of $\mathbb{F}^{n}$. Then
    \[
        \operatorname{span}(v_{k}, \ldots, v_{n}) = \operatorname{span}(e_{k}, \ldots, e_{n})
    \]

    and $\innerprod{v_{k}, e_{k}} > 0$ for every $k\in\{1,\ldots,n\}$.

    Let $Q$ be a unitary matrix whose $k$th row is $e_{n+1-k}$. Let $R$ be the $n$-by-$n$ matrix defined by
    \[
        R_{j,k} = \innerprod{v_{n+1-j}, e_{n+1-k}}.
    \]

    If $j < k$, then $\innerprod{v_{n+1-j}, e_{n+1-k}} = 0$ because $(n+1-j) > (n+1-k)$ and $e_{n+1-k}$ is orthogonal to $\operatorname{span}(e_{n+1-j}, \ldots, e_{n}) = \operatorname{span}(v_{n+1-j}, \ldots, v_{n})$, so $R$ is lower triangular. Moreover, $R_{j,j} = \innerprod{v_{n+1-j}, e_{n+1-j}} > 0$ so all entries on the diagonal of $R$ are positive.

    The $j$th rows of $RQ$ is
    \[
        \innerprod{v_{n+1-j}, e_{n}}e_{n} + \cdots + \innerprod{v_{n+1-j}, e_{n+1-j}}e_{n+1-j}
    \]

    which equals to $v_{n+1-j}$ and $w_{j}$. Therefore $A = RQ$.

    Suppose we have $A = \widetilde{R}\widetilde{Q}$ where $\widetilde{R}$ is lower triangular and $\widetilde{Q}$ is unitary. Let $q_{n}, \ldots, q_{1}$ be the rows of $\widetilde{Q}$, then $\operatorname{span}(v_{k}, \ldots, v_{n}) =  \operatorname{span}(q_{k}, \ldots, q_{n})$ and $\innerprod{v_{k}, q_{k}} > 0$ for every $k\in\{1,\ldots, n\}$. By Exercise~\ref{chapter6:sectionB:exercise10}, $q_{k} = e_{k}$ for every $k\in\{1,\ldots,n\}$. Hence $Q = \widetilde{Q}$ and $R = \widetilde{R}$ (since $e_{1},  \ldots, e_{n}$ form a basis).

    Thus there exist unique matrices $R$ and $Q$ such that $R$ is lower-triangular with only positive numbers on its diagonal, $Q$ is unitary, and $A = RQ$.
\end{proof}
\newpage

\section{Singular Value Decomposition}

% chapter7:sectionE:exercise1
\begin{exercise}
    Suppose $T\in \lmap{V, W}$. Show that $T = 0$ if and only if all singular values of $T$ are $0$.
\end{exercise}

\begin{proof}
    \begin{align*}
        T = 0 & \Longleftrightarrow Tv = 0\,\forall v\in V                     \\
              & \Longleftrightarrow \innerprod{Tv, Tv} = 0\,\forall v\in V     \\
              & \Longleftrightarrow \innerprod{T^{*}Tv, v} = 0\,\forall v\in V \\
              & \Longleftrightarrow T^{*}T = 0
    \end{align*}

    the last equivalence is true since $T^{*}T$ is self-adjoint. $T^{*}T$ is diagonalizable, so $T^{*}T = 0$ if and only if all eigenvalues of $T^{*}T$ are $0$, equivalently, all singular values of $T$ are $0$.
\end{proof}
\newpage

% chapter7:sectionE:exercise2
\begin{exercise}
    Suppose $T\in \lmap{V, W}$ and $s > 0$. Prove that $s$ is a singular value of $T$ if and only if there exist nonzero vectors $v \in V$ and $w \in W$ such that
    \[
        Tv = sw \quad\text{and}\quad T^{*}w = sv.
    \]
\end{exercise}

\begin{quote}
    The vectors $v, w$ satisfying both equations above are called a Schmidt pair. Erhard Schmidt introduced the concept of singular values in 1907.
\end{quote}

\begin{proof}
    $(\Rightarrow)$ $s$ is a singular value of $T$.

    Then $s^{2}$ is an eigenvalue of $T^{*}T$. So there exists a nonzero vector $v$ such that $T^{*}Tv = s^{2}v$.

    Let $w = \frac{1}{s}Tv$, then $w$ is nonzero, $Tv = sw$ and $T^{*}w = \frac{1}{s}T^{*}Tv = sv$.

    \bigskip
    $(\Leftarrow)$ There exist nonzero vectors $v \in V$ and $w \in W$ such that $Tv = sw$ and $T^{*}w = sv$.

    $T^{*}Tv = T^{*}(sw) = s^{2}v$. So $s^{2}$ is an eigenvalue of $T^{*}T$, and it follows that $s$ is a singular value of $T$.
\end{proof}
\newpage

% chapter7:sectionE:exercise3
\begin{exercise}
    Give an example of $T\in\lmap{\mathbb{C}^{2}}$ such that $0$ is the only eigenvalue of $T$ and the singular values of $T$ are $5$, $0$.
\end{exercise}

\begin{proof}
    Let $T(z, w) = (0, 5z)$, then the matrix of $T$ with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}0 & 5 \\ 0 & 0 \end{pmatrix}$. The only eigenvalue of $T$ is $0$ (because the minimal polynomial of $T$ is $z^{2}$).

    $T^{*}(z, w) = (5w, 0)$, and the matrix of $T^{*}$ with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}0 & 0 \\ 5 & 0\end{pmatrix}$.

    The matrix of $T^{*}T$ with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}25 & 0 \\ 0 & 0\end{pmatrix}$ so the eigenvalues of $T^{*}T$ are $25$ and $0$. Hence the singular values of $T$ are $5, 0$.
\end{proof}
\newpage

% chapter7:sectionE:exercise4
\begin{exercise}\label{chapter7:sectionE:exercise4}
    Suppose that $T\in\lmap{V, W}$, $s_{1}$ is the largest singular value of $T$, and $s_{n}$ is the smallest singular value of $T$. Prove that
    \[
        \left\{ \norm{Tv}: v\in V\text{ and }\norm{v} = 1\right\} = [s_{n}, s_{1}].
    \]
\end{exercise}

\begin{proof}
    Let $s_{1}, \ldots, s_{n}$ be the singular values of $T^{*}T$ and $e_{1}, \ldots, e_{n}$ be an orthonormal basis of $V$ such that $T^{*}Te_{k} = s_{k}^{2}e_{k}$ for every $k\in\{1,\ldots,n\}$ (this is possible due to the spectral theorem).

    Let $v$ be a vector in $V$ such that $\norm{v} = 1$. So there exist $a_{1}, \ldots, a_{n}\in\mathbb{F}$ such that
    \[
        \abs{a_{1}}^{2} + \cdots + \abs{a_{n}}^{2} = 1\qquad v = a_{1}e_{1} + \cdots + a_{n}e_{n}.
    \]

    \begin{align*}
        \innerprod{Tv, Tv} & = \innerprod{T^{*}Tv, v}                                                                           & \text{(the definition of adjoint map)} \\
                           & = \innerprod{s_{1}^{2}a_{1}e_{1} + \cdots + s_{n}^{2}a_{n}e_{n}, a_{1}e_{1} + \cdots + a_{n}e_{n}}                                          \\
                           & = s_{1}^{2}\abs{a_{1}}^{2} + \cdots + s_{n}^{2}\abs{a_{n}}^{2}
    \end{align*}

    Hence $s_{n}^{2}\leq \norm{Tv}^{2}\leq s_{1}^{2}$, where
    \begin{itemize}
        \item $\norm{Tv}^{2} = s_{1}^{2}$ if and only if $a_{k} = 0$ for every $k$ that satisfies $s_{k}\ne s_{1}$,
        \item $\norm{Tv}^{2} = s_{n}^{2}$ if and only if $a_{k} = 0$ for every $k$ that satisfies $s_{k}\ne s_{n}$.
    \end{itemize}

    Thus $\left\{ \norm{Tv}: v\in V\text{ and }\norm{v} = 1\right\} = [s_{n}, s_{1}]$.
\end{proof}
\newpage

% chapter7:sectionE:exercise5
\begin{exercise}
    Suppose $T\in\lmap{\mathbb{C}^{2}}$ is defined by $T(x, y) = (-4y, x)$. Find the singular values of $T$.
\end{exercise}

\begin{proof}
    $T^{*}(x, y) = (y, -4x)$, so $T^{*}T(x, y) = T^{*}(-4y, x) = (x, 16y)$. The matrix of $T^{*}T$ with respect to the standard basis of $\mathbb{C}^{2}$ is $\begin{pmatrix}0 & 0 \\ 0 & 16\end{pmatrix}$. Hence the singular values of $T$ are $4, 0$.
\end{proof}
\newpage

% chapter7:sectionE:exercise6
\begin{exercise}
    Find the singular values of the differentiation operator $D \in \lmap{\mathscr{P}_{2}(\mathbb{R})}$ defined by $Dp = p'$, where the inner product on $\mathscr{P}_{2}(\mathbb{R})$ is as in Example 6.34.
\end{exercise}

\begin{proof}
    An orthonormal basis of $\mathscr{P}_{2}(\mathbb{R})$ is
    \[
        \sqrt{\frac{1}{2}}, \sqrt{\frac{3}{2}}x, \sqrt{\frac{45}{8}}\left(x^{2} - \frac{1}{3}\right)
    \]

    and the matrices of $D$, $D^{*}$ with respect to this basis are
    \[
        \begin{pmatrix}
            0 & \sqrt{\frac{3}{2}} & 0                   \\
            0 & 0                  & \sqrt{\frac{45}{2}} \\
            0 & 0                  & 0
        \end{pmatrix}
        \qquad
        \begin{pmatrix}
            0                  & 0                   & 0 \\
            \sqrt{\frac{3}{2}} & 0                   & 0 \\
            0                  & \sqrt{\frac{45}{2}} & 0
        \end{pmatrix}
    \]

    so the matrix of $D^{*}D$ with respect to the basis is
    \[
        \begin{pmatrix}
            0 & 0           & 0            \\
            0 & \frac{3}{2} & 0            \\
            0 & 0           & \frac{45}{2}
        \end{pmatrix}.
    \]

    Hence the singular values of $D$ are $\sqrt{\frac{45}{2}}, \sqrt{\frac{3}{2}}, 0$.
\end{proof}
\newpage

% chapter7:sectionE:exercise7
\begin{exercise}\label{chapter7:sectionE:exercise7}
    Suppose that $T \in \lmap{V}$ is self-adjoint or that $\mathbb{F} = \mathbb{C}$ and $T \in \lmap{V}$ is normal. Let $\lambda_{1}, \ldots, \lambda_{n}$ be the eigenvalues of $T$, each included in this list as many times as the dimension of the corresponding eigenspace. Show that the singular values of $T$ are $\abs{\lambda_{1}}, \ldots, \abs{\lambda_{n}}$, after these numbers have been sorted into decreasing order.
\end{exercise}

\begin{proof}
    According to the complex spectral theorem, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ of which vectors are eigenvectors of $T$. Without loss of generality, suppose that $Te_{k} = \lambda_{k}e_{k}$ for every $k\in\{1,\ldots,n\}$.

    Because $T$ is normal, so $T^{*}e_{k} = \conj{\lambda_{k}}e_{k}$ for every $k\in\{1,\ldots,n\}$. Hence $T^{*}Te_{k} = \abs{\lambda_{k}}^{2}e_{k}$ for every $k\in\{1,\ldots,n\}$. Therefore the singular values of $T$ are $\abs{\lambda_{1}}, \ldots, \abs{\lambda_{n}}$ after sorted into decreasing order.
\end{proof}
\newpage

% chapter7:sectionE:exercise8
\begin{exercise}\label{chapter7:sectionE:exercise8}
    Suppose $T\in\lmap{V, W}$. Suppose $s_{1}\geq s_{2}\geq \cdots\geq s_{m} > 0$ and $e_{1}, \ldots, e_{m}$ is an orthonormal list in $V$ and $f_{1}, \ldots, f_{m}$ is an orthonormal list in $W$ such that
    \[
        Tv = s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{m}
    \]

    for every $v\in V$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $f_{1}, \ldots, f_{m}$ is an orthonormal basis of $\range{T}$.
        \item Prove that $e_{1}, \ldots, e_{m}$ is an orthonormal basis of ${(\kernel{T})}^{\bot}$.
        \item Prove that $s_{1}, \ldots, s_{m}$ are the positive singular values of $T$.
        \item Prove that if $k\in\{1,\ldots, m\}$, then $e_{k}$ is an eigenvector $T^{*}T$ with corresponding eigenvalue $s_{k}^{2}$.
        \item Prove that
              \[
                  TT^{*}w = s_{1}^{2}\innerprod{w, f_{1}}f_{1} + \cdots + s_{m}^{2}\innerprod{w, f_{m}}f_{m}
              \]

              for all $w\in W$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item The list $f_{1}, \ldots, f_{m}$ spans $\range{T}$ and it is also an orthonormal list so it is an orthonormal basis of $\range{T}$.
        \item $Tv = 0$ if and only if $\innerprod{v, e_{1}} = \cdots = \innerprod{v, e_{m}} = 0$.

              So $v\in\kernel{T}$ if and only if $v$ is orthogonal to $\operatorname{span}(e_{1}, \ldots, e_{m})$.

              Therefore $e_{1}, \ldots, e_{m}$ is an orthonormal basis of ${(\kernel{T})}^{\bot}$.
        \item For every $v\in V$ and $w\in W$
              \begin{align*}
                  \innerprod{v, T^{*}w} & = \innerprod{Tv, w}                                                                                                  \\
                                        & = \innerprod{s_{1}\innerprod{v, e_{1}}f_{1} + \cdots + s_{m}\innerprod{v, e_{m}}f_{m}, w}                            \\
                                        & = s_{1}\innerprod{v,e_{1}}\conj{\innerprod{w, f_{1}}} + \cdots + s_{m}\innerprod{v,e_{m}}\conj{\innerprod{w, f_{m}}} \\
                                        & = \innerprod{v, s_{1}\innerprod{w, f_{1}}e_{1} + \cdots + s_{m}\innerprod{w, f_{m}}e_{m}}
              \end{align*}

              so $T^{*}w = s_{1}\innerprod{w, f_{1}}e_{1} + \cdots + s_{m}\innerprod{w, f_{m}}e_{m}$, and
              \begin{align*}
                  T^{*}(Tv) & = T^{*}(s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{m})    \\
                            & = s_{1}\innerprod{v,e_{1}}T^{*}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}T^{*}f_{m} \\
                            & = s_{1}^{2}\innerprod{v,e_{1}}e_{1} + \cdots + s_{m}^{2}\innerprod{v,e_{m}}e_{m}.
              \end{align*}

              So the matrix of $T^{*}T$ with respect to $e_{1}, \ldots, e_{m}, \ldots, e_{\dim V}$ is a diagonal matrix with
              \[
                  s_{1}^{2}, \ldots, s_{m}^{2}, 0, \ldots, 0
              \]

              on the diagonal. Hence $s_{1}, \ldots, s_{m}$ are the positive singular values of $T$.
        \item From the proof of (c), we obtain that $T^{*}(Te_{k}) = s_{k}^{2}e_{k}$ for every $k\in\{1,\ldots,m\}$, so $e_{k}$ is an eigenvector of $T^{*}T$ with corresponding eigenvalue $s_{k}^{2}$.
        \item For every $w\in W$
              \begin{align*}
                  TT^{*}w & = T(s_{1}\innerprod{w, f_{1}}e_{1} + \cdots + s_{m}\innerprod{w, f_{m}}e_{m})     \\
                          & = s_{1}\innerprod{w,f_{1}}Te_{1} + \cdots + s_{m}\innerprod{w,f_{m}}Te_{m}        \\
                          & = s_{1}^{2}\innerprod{w,f_{1}}f_{1} + \cdots + s_{m}^{2}\innerprod{w,f_{m}}f_{m}.
              \end{align*}
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionE:exercise9
\begin{exercise}
    Suppose $T\in\lmap{V, W}$. Show that $T$ and $T^{*}$ have the same positive singular values.
\end{exercise}

\begin{proof}
    This is a direct corollary of Exercise~\ref{chapter7:sectionE:exercise8} (c) and (e) (the previous exercise).
\end{proof}
\newpage

% chapter7:sectionE:exercise10
\begin{exercise}
    Suppose $T\in\lmap{V, W}$ has singular values $s_{1}, \ldots, s_{n}$. Prove that if $T$ is an invertible linear map, then $T^{-1}$ has singular values
    \[
        \frac{1}{s_{n}}, \ldots, \frac{1}{s_{1}}
    \]
\end{exercise}

\begin{proof}
    Because $T$ is invertible, $s_{1}, \ldots, s_{n}$ are all positive numbers.

    By the SVD theorem, $T$ admits a singular value decomposition
    \[
        Tv = s_{1}\innerprod{v, e_{1}}f_{1} + \cdots + s_{n}\innerprod{v, e_{n}}f_{n}
    \]

    where $T^{*}Te_{k} = s_{k}^{2}e_{k}$ and $Te_{k} = s_{k}f_{k}$ for every $k\in\{1,\ldots,n\}$. On the other hand
    \[
        T^{-1}w = T^{\dagger}w = \frac{1}{s_{1}}\innerprod{w,f_{1}}e_{1} + \cdots + \frac{1}{s_{n}}\innerprod{w,f_{n}}f_{n}
    \]

    so the singular values of $T^{-1}$ are
    \[
        \frac{1}{s_{n}}, \ldots, \frac{1}{s_{1}}.\qedhere
    \]
\end{proof}
\newpage

% chapter7:sectionE:exercise11
\begin{exercise}\label{chapter7:sectionE:exercise11}
    Suppose that $T\in\lmap{V, W}$ and $v_{1}, \ldots, v_{n}$ is an orthonormal basis of $V$. Let $s_{1}, \ldots, s_{n}$ denote the singular values of $T$.
    \begin{enumerate}[label={(\alph*)}]
        \item Prove that $\norm{Tv_{1}}^{2} + \cdots + \norm{Tv_{n}}^{2} = s_{1}^{2} + \cdots + s_{n}^{2}$.
        \item Prove that if $W = V$ and $T$ is a positive operator, then
              \[
                  \innerprod{Tv_{1}, v_{1}} + \cdots + \innerprod{Tv_{n}, v_{n}} = s_{1} + \cdots + s_{n}.
              \]
    \end{enumerate}
\end{exercise}

\begin{quote}
    See the comment after Exercise~\ref{chapter7:sectionA:exercise5}.
\end{quote}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item By the SVD theorem, there exists an orthonormal list $e_{1}, \ldots, e_{m}$ ($m\leq n$) of $V$ and an orthonormal list $f_{1}, \ldots, f_{m}$ in $W$ such that
              \[
                  Tv = s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v, e_{m}}f_{m}
              \]

              and $Te_{k} = s_{k}f_{k}$ for every $k\in\{1,\ldots,n\}$ (yes, $n$ instead of $m$, because $s_{k} = 0$ if $k > m$). So
              \begin{align*}
                  \sum^{n}_{j=1}\norm{Tv_{j}}^{2} & = \sum^{n}_{j=1}\norm{s_{1}\innerprod{v_{j}, e_{1}}f_{1} + \cdots + s_{m}\innerprod{v_{j}, e_{m}}f_{m}}^{2} \\
                                                  & = \sum^{n}_{j=1}\left(\sum^{m}_{k=1}s_{k}^{2}\abs{\innerprod{v_{j}, e_{k}}}^{2}\right)                      \\
                                                  & = \sum^{m}_{k=1}s_{k}^{2}\left(\sum^{n}_{j=1}\abs{\innerprod{v_{j}, e_{k}}}^{2}\right)                      \\
                                                  & = \sum^{m}_{k=1}s_{k}^{2}\norm{e_{k}}^{2} = \sum^{m}_{k=1}s_{k}^{2} = \sum^{n}_{k=1}s_{k}^{2}.
              \end{align*}

              Hence $\norm{Tv_{1}}^{2} + \cdots + \norm{Tv_{n}}^{2} = s_{1}^{2} + \cdots + s_{n}^{2}$.
        \item If $T$ is a positive operator, then $\sqrt{T}$ is also a positive operator.

              Because $T$ is a positive operator, then $T$ is diagonalizable and all eigenvalues of $T$ are nonnegative. Let $\lambda_{1}\geq\lambda_{2}\geq\cdots\geq\lambda_{n}$ be the eigenvalues of $T$ (each are included as many as the dimension of the corresponding eigenspace of $T$). Hence the eigenvalues of $T^{*}T$ are $\lambda_{1}^{2}\geq\lambda_{2}^{2}\geq\cdots\geq \lambda_{n}^{2}$, which are precisely $s_{1}^{2}\geq s_{2}^{2}\geq\cdots\geq s_{n}^{2}$.

              Because $\lambda_{1}, \ldots, \lambda_{n}, s_{1}, \ldots, s_{n}$ are nonnegative numbers, it follows that $\lambda_{1} = s_{1}, \ldots, \lambda_{n} = s_{n}$. So $\sqrt{s_{1}}, \ldots, \sqrt{s_{n}}$ are the singular values of $\sqrt{T}$. By (a)
              \begin{align*}
                  \innerprod{Tv_{1}, v_{1}} + \cdots + \innerprod{Tv_{n}, v_{n}} & = \innerprod{\sqrt{T}v_{1}, \sqrt{T}v_{1}} + \cdots + \innerprod{\sqrt{T}v_{n}, \sqrt{T}v_{n}} \\
                                                                                 & = \norm{\sqrt{T}v_{1}}^{2} + \cdots + \norm{\sqrt{T}v_{n}}^{2}                                 \\
                                                                                 & = {(\sqrt{s_{1}})}^{2} + \cdots + {(\sqrt{s_{n}})}^{2}                                         \\
                                                                                 & = s_{1} + \cdots + s_{n}.
              \end{align*}
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionE:exercise12
\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Give an example of a finite-dimensional vector space and an operator $T$ on it such that the singular values of $T^{2}$ do not equal the squares of the singular values of $T$.
        \item Suppose $T\in\lmap{V}$ is normal. Prove that the singular values of $T^{2}$ equal the squares of the singular values of $T$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Here is an example.

              On $\mathbb{C}^{2}$, let $T(x, y) = (0, x)$, then $T^{2} = 0$.

              The singular values of $T$ are $1, 0$, meanwhile the singular values of $T^{2}$ are $0, 0$.
        \item $T$ is normal, so $T$ commutes with $T^{*}$, and
              \[
                  {(T^{2})}^{*}T^{2} = {(T^{*})}^{2}T^{2} = T^{*}T^{*}TT = T^{*}TT^{*}T = {(T^{*}T)}^{2}.
              \]

              $T^{*}T$ is self-adjoint and hence normal. According to the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ such that $e_{1}, \ldots, e_{n}$ are eigenvectors of $T^{*}T$. Let $T^{*}Te_{k} = \lambda_{k}e_{k}$ for every $k\in\{1,\ldots,n\}$, then
              \[
                  {(T^{2})}^{*}T^{2}e_{k} = {(T^{*}T)}^{2}e_{k} = \lambda_{k}^{2}e_{k}
              \]

              so $\lambda_{1}^{2}, \ldots, \lambda_{n}^{2}$ are the eigenvalues of ${(T^{2})}^{*}T^{2}$.

              Hence the singular values of $T^{2}$ equal the squares of the singular values of $T$.
    \end{enumerate}
\end{proof}
\newpage

% chapter7:sectionE:exercise13
\begin{exercise}
    Suppose $T_{1}, T_{2} \in \lmap{V}$. Prove that $T_{1}$ and $T_{2}$ have the same singular values if and only if there exist unitary operators $S_{1}, S_{2} \in \lmap{V}$ such that $T_{1} = S_{1}T_{2}S_{2}$.
\end{exercise}

\begin{proof}
    $(\Rightarrow)$ There exist unitary operators $S_{1}, S_{2} \in \lmap{V}$ such that $T_{1} = S_{1}T_{2}S_{2}$.
    \[
        T_{1}^{*}T_{1} = (S_{2}^{*}T_{2}^{*}S_{1}^{*})(S_{1}T_{2}S_{2}) = S_{2}^{*}T_{2}^{*}T_{2}S_{2}.
    \]

    $T_{1}^{*}T_{1}$ are self-adjoint and hence normal, so by the real and complex spectral theorems, there exists an orthonormal basis $e_{1}, \ldots, e_{n}$ of $V$ to which $T_{1}^{*}T_{1}$ have a diagonal matrix.

    Let $T_{1}^{*}T_{1}e_{k} = \lambda_{k}e_{k}$ for every $k\in\{1,\ldots,n\}$. Let $f_{k} = S_{2}e_{k}$ for every $k\in\{1,\ldots,n\}$, then $e_{k} = S_{2}^{*}f_{k}$ for every $k\in\{1,\ldots,n\}$.
    \begin{align*}
        (T_{2}^{*}T_{2})f_{k} & = (T_{2}^{*}T_{2}S_{2})e_{k}               \\
                              & = (S_{2}S_{2}^{*}T_{2}^{*}T_{2}S_{2})e_{k} \\
                              & = (S_{2}T_{1}^{*}T_{1})e_{k}               \\
                              & = S_{2}(\lambda_{k}e_{k})                  \\
                              & = \lambda_{k}f_{k}
    \end{align*}

    so $\lambda_{1}, \ldots, \lambda_{n}$ are the eigenvalues of $T_{1}^{*}T_{1}$ and $T_{2}^{*}T_{2}$. Therefore $T_{1}$ and $T_{2}$ have the same singular values.

    \bigskip
    $(\Leftarrow)$ $T_{1}$ and $T_{2}$ have the same singular values.

    Let $s_{1}\geq s_{2}\geq \cdots \geq s_{n}$ be the singular values of $T_{1}$ and $T_{2}$.

    According to the SVD theorem, there exist four orthonormal bases
    \[
        e_{1}, \ldots, e_{n}; f_{1}, \ldots, f_{n}; v_{1}, \ldots, v_{n}; w_{1}, \ldots, w_{n}
    \]

    of $V$ such that $T_{1}e_{k} = s_{k}f_{k}$, $T_{2}v_{1} = s_{k}w_{k}$, and $T_{1}^{*}T_{1}e_{k} = s_{k}^{2}e_{k}$, $T_{2}^{*}T_{2}v_{k} = s_{k}^{2}v_{k}$ for every $k\in\{1,\ldots, n\}$.

    Let $S_{2}$ be the unitary operator that maps $e_{k}$ to $v_{k}$ and $S_{1}$ be the unitary operator that maps $w_{k}$ to $f_{k}$. Then for every $k\in\{1,\ldots,n\}$,
    \[
        S_{1}T_{2}S_{2}e_{k} = S_{1}T_{2}v_{k} = S_{1}(s_{k}w_{k}) = s_{k}f_{k} = T_{1}e_{k}.
    \]

    Hence $T_{1} = S_{1}T_{2}S_{2}$.
\end{proof}
\newpage

% chapter7:sectionE:exercise14
\begin{exercise}\label{chapter7:sectionE:exercise14}
    Suppose $T\in\lmap{V, W}$. Let $s_{n}$ denote the smallest singular value of $T$. Prove that $s_{n}\norm{v}\leq \norm{Tv}$ for every $v\in V$.
\end{exercise}

\begin{proof}
    By the SVD theorem, there exist an orthonormal list $e_{1}, \ldots, e_{m}$ in $V$ and an orthonormal list $f_{1}, \ldots, f_{m}$ such that $Te_{k} = s_{k}f_{k}$, $T^{*}Te_{k} = s_{k}^{2}e_{k}$ for every $k\in\{1,\ldots,n\}$ and
    \[
        Tv = s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{k}.
    \]

    where $s_{1}\geq s_{2}\geq \cdots \geq s_{m}$ are the positive singular values of $T$.
    \begin{align*}
        \norm{Tv}^{2} & = \norm{s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{m}}^{2}                                              \\
                      & = s_{1}^{2}\abs{\innerprod{v, e_{1}}}^{2} + \cdots + s_{m}^{2}\abs{\innerprod{v, e_{m}}}^{2}                                     \\
                      & = s_{1}^{2}\abs{\innerprod{v, e_{1}}}^{2} + \cdots + s_{n}^{2}\abs{\innerprod{v, e_{n}}}^{2}   & \text{($s_{k} = 0$ if $k > m$)} \\
                      & \geq s_{n}^{2}\abs{\innerprod{v, e_{1}}}^{2} + \cdots + s_{n}^{2}\abs{\innerprod{v,e_{n}}}^{2}                                   \\
                      & = s_{n}^{2}\norm{v}^{2}                                                                        & \text{(Parseval's identity)}
    \end{align*}

    so $s_{n}\norm{v}\leq \norm{Tv}$ for every $v\in V$.
\end{proof}
\newpage

% chapter7:sectionE:exercise15
\begin{exercise}
    Suppose $T\in\lmap{V}$ and $s_{1}\geq \cdots\geq s_{n}$ are the singular values of $T$. Prove that if $\lambda$ is an eigenvalue of $T$, then $s_{1}\geq \abs{\lambda}\geq s_{n}$.
\end{exercise}

\begin{proof}[Short proof]
    Let $\lambda$ be an eigenvalue of $T$ and $v$ be a corresponding eigenvector, then $\norm{v_{0}} = 1$ where $v_{0} = v/\norm{v}$.

    Because $\norm{Tv_{0}} = \norm{\lambda v_{0}} = \abs{\lambda}$, then by Exercise~\ref{chapter7:sectionE:exercise4}, we conclude that $s_{1}\geq \abs{\lambda}\geq s_{n}$.
\end{proof}

\begin{proof}[Another proof]
    By the SVD theorem, there exist an orthonormal list $e_{1}, \ldots, e_{m}$ in $V$ and an orthonormal list $f_{1}, \ldots, f_{m}$ such that $Te_{k} = s_{k}f_{k}$, $T^{*}Te_{k} = s_{k}^{2}e_{k}$ for every $k\in\{1,\ldots,n\}$ and
    \[
        Tv = s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{k}.
    \]

    where $s_{1}\geq s_{2}\geq \cdots \geq s_{m}$ are the positive singular values of $T$.
    \begin{align*}
        \norm{Tv}^{2} & = \norm{s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{m}}^{2}                                              \\
                      & = s_{1}^{2}\abs{\innerprod{v, e_{1}}}^{2} + \cdots + s_{m}^{2}\abs{\innerprod{v, e_{m}}}^{2}                                     \\
                      & = s_{1}^{2}\abs{\innerprod{v, e_{1}}}^{2} + \cdots + s_{n}^{2}\abs{\innerprod{v, e_{n}}}^{2}   & \text{($s_{k} = 0$ if $k > m$)} \\
                      & \leq s_{1}^{2}\abs{\innerprod{v, e_{1}}}^{2} + \cdots + s_{1}^{2}\abs{\innerprod{v,e_{n}}}^{2}                                   \\
                      & = s_{1}^{2}\norm{v}^{2}                                                                        & \text{(Parseval's identity)}
    \end{align*}

    so $\norm{Tv}\leq s_{1}\norm{v}$ for every $v\in V$. Together with Exercise~\ref{chapter7:sectionE:exercise14}, we conclude that for every $v\in V$
    \[
        s_{1}\norm{v}\geq \norm{Tv}\geq s_{n}\norm{v}.
    \]

    Let $\lambda$ be an eigenvalue of $T$ and $v_{0}$ be a corresponding eigenvector, then
    \[
        s_{1}\norm{v_{0}}\geq \norm{\lambda v_{0}}\geq s_{n}\norm{v_{0}}.
    \]

    Because $v_{0}\ne 0$, $\norm{v_{0}} > 0$ and $\norm{\lambda v_{0}} = \abs{\lambda}\norm{v_{0}}$, it follows that $s_{1}\geq \abs{\lambda} \geq s_{n}$.
\end{proof}
\newpage

% chapter7:sectionE:exercise16
\begin{exercise}\label{chapter7:sectionE:exercise16}
    Suppose $T\in\lmap{V, W}$. Prove that ${(T^{*})}^{\dagger} = {(T^{\dagger})}^{*}$.
\end{exercise}

\begin{quote}
    Compare the result in this exercise to the analogous result for invertible
    linear maps.
\end{quote}

\begin{proof}
    By the SVD theorem, there exist an orthonormal list $e_{1}, \ldots, e_{m}$ in $V$ and an orthonormal list $f_{1}, \ldots, f_{m}$ such that $Te_{k} = s_{k}f_{k}$, $T^{*}Te_{k} = s_{k}^{2}e_{k}$ for every $k\in\{1,\ldots,n\}$ and
    \[
        Tv = s_{1}\innerprod{v,e_{1}}f_{1} + \cdots + s_{m}\innerprod{v,e_{m}}f_{k}
    \]

    where $s_{1}\geq s_{2}\geq \cdots \geq s_{m}$ are the positive singular values of $T$.

    Moreover,
    \begin{align*}
        T^{*}w       & = s_{1}\innerprod{w,f_{1}}e_{1} + \cdots + s_{m}\innerprod{w, f_{m}}e_{m},                    \\
        T^{\dagger}w & = \frac{1}{s_{1}}\innerprod{w,f_{1}}e_{1} + \cdots + \frac{1}{s_{m}}\innerprod{w,f_{m}}e_{m}.
    \end{align*}

    Apply these results to ${(T^{*})}^{\dagger}$ and ${(T^{\dagger})}^{*}$, we obtain that
    \begin{align*}
        {(T^{*})}^{\dagger}v & = \frac{1}{s_{1}}\innerprod{v,e_{1}}f_{1} + \cdots + \frac{1}{s_{m}}\innerprod{v,e_{m}}f_{m}, \\
        {(T^{\dagger})}^{*}v & = \frac{1}{s_{1}}\innerprod{v,e_{1}}f_{1} + \cdots + \frac{1}{s_{m}}\innerprod{v,e_{m}}f_{m}.
    \end{align*}

    Thus ${(T^{*})}^{\dagger} = {(T^{\dagger})}^{*}$.
\end{proof}
\newpage

% chapter7:sectionE:exercise17
\begin{exercise}
    Suppose $T \in \lmap{V}$. Prove that $T$ is self-adjoint if and only if $T^{\dagger}$ is self-adjoint.
\end{exercise}

\begin{proof}
    By Exercise~\ref{chapter6:sectionC:exercise23} and~\ref{chapter7:sectionE:exercise16}
    \begin{align*}
        T = T^{*} & \Longleftrightarrow T^{\dagger} = {(T^{*})}^{\dagger} & \text{(because ${(T^{\dagger})}^{\dagger} = T$)}             \\
                  & \Longleftrightarrow T^{\dagger} = {(T^{\dagger})}^{*} & \text{(because ${(T^{*})}^{\dagger} = {(T^{\dagger})}^{*}$)}
    \end{align*}

    hence $T$ is self-adjoint if and only if $T^{\dagger}$ is self-adjoint.
\end{proof}
\newpage

\section{Consequences of Singular Value Decomposition}

