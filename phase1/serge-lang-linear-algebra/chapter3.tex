\chapter{Linear Mappings}

\section{Mappings}
\setcounter{exercise}{0}

\begin{exercise}
    In Example 3, give $Df$ as a function of $x$ when $f$ is the function
    \begin{enumerate}[label={(\alph*)}]
        \item $f(x) = \sin x$
        \item $f(x) = e^{x}$
        \item $f(x) = \log x$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $Df(x) = \cos x$.
        \item $Df(x) = e^{x}$.
        \item $Df(x) = \frac{1}{x}$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $V$ be a vector space, and let $u$ be a fixed element of $V$, we let $T_{u}: V\to V$ be the map such that $T_{u}(v) = v + u$. Prove that
    \begin{itemize}
        \item If $u_{1}, u_{2}$ are elements of $V$, then $T_{u_{1} + u_{2}} = T_{u_{1}} \circ T_{u_{2}}$.
        \item If $u$ is an element of $V$, then $T_{u}: V\to V$ has an inverse mappingwhich is nothing but the translation $T_{-u}$.
    \end{itemize}
\end{exercise}

\begin{proof}
    \begin{itemize}
        \item For every $v\in V$
              \begin{align*}
                  T_{u_{1} + u_{2}}(v) & = v + (u_{1} + u_{2})            \\
                                       & = v + (u_{2} + u_{1})            \\
                                       & = (v + u_{2}) + u_{1}            \\
                                       & = T_{u_{2}}(v) + u_{1}           \\
                                       & = T_{u_{1}}(T_{u_{2}}(v))        \\
                                       & = (T_{u_{1}} \circ T_{u_{2}})(v)
              \end{align*}

              Hence $T_{u_{1} + u_{2}} = T_{u_{1}} \circ T_{u_{2}}$.
        \item According to the previous result
              \[
                  \begin{split}
                      T_{u}\circ T_{-u} = T_{u + (-u)} = T_{O} = \text{id}_{V} \\
                      T_{-u}\circ T_{u} = T_{(-u) + u} = T_{O} = \text{id}_{V}
                  \end{split}
              \]

              Hence $T_{u}$ has an inverse mapping, and it is $T_{-u}$.
    \end{itemize}
\end{proof}

\begin{exercise}
    In Example 5, give $L(X)$ when $X$ is the vector
    \begin{enumerate}[label={(\alph*)}]
        \item $(1, 2, -3)$
        \item $(-1, 5, 0)$
        \item $(2, 1, 1)$
    \end{enumerate}
\end{exercise}

\begin{proof}
    $L: \mathbb{R}^{3} \to \mathbb{R}$, $X\mapsto L(X) = 2x + 3y - z$

    \begin{enumerate}[label={(\alph*)}]
        \item $L(X) = 2 + 6 + 3 = 11$
        \item $L(X) = -2 + 15 - 0 = 13$
        \item $L(X) = 4 + 3 - 1 = 6$
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $F: \mathbb{R} \to \mathbb{R}^{2}$ be the mapping such that $F(t) = (e^{t}, t)$. What is $F(1), F(0), F(-1)$?
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            F(1) = (e, 1) \\
            F(0) = (1, 0) \\
            F(-1) = (1/e, -1)
        \end{split}
    \]
\end{proof}

\begin{exercise}
    Let $G: \mathbb{R} \to \mathbb{R}^{2}$ be the mapping such that $G(t) = (t, 2t)$. Let $F$ be as in Exercise 4. What is $(F + G)(1), (F + G)(2), (F + G)(0)$?
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            (F + G)(1) = F(1) + G(1) = (e, 1) + (1, 2) = (e + 1, 3) \\
            (F + G)(2) = F(2) + G(2) = (e^{2}, 2) + (2, 4) = (e^{2} + 2, 6) \\
            (F + G)(0) = F(0) + G(0) = (1, 0) + (0, 0) = (1, 0)
        \end{split}
    \]
\end{proof}

\begin{exercise}
    Let $F$ be as in Exercise 4. What is $(2F)(0), (\pi F)(1)$?
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            (2F)(0) = 2\cdot F(0) = 2\cdot (1, 0) = (2, 0) \\
            (\pi F)(1) = \pi\cdot F(1) = \pi\cdot (e, 1) = (\pi e, \pi)
        \end{split}
    \]
\end{proof}

\begin{exercise}
    Let $A = (1, 1, -1, 3)$. Let $F: \mathbb{R}^{4} \to \mathbb{R}$ be the mapping such that for any vector $X = (x_{1}, x_{2}, x_{3}, x_{4})$ we have $F(X) = X\cdot A + 2$. What is the value of $F(X)$ when
    \begin{enumerate}[label={(\alph*)}]
        \item $X = (1, 1, 0, -1)$?
        \item $X = (2, 3, -1, 1)$?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $F(X) = 1 + 1 - 3 + 2 = 1$.
        \item $F(X) = 2 + 3 + 1 + 3 = 9$.
    \end{enumerate}
\end{proof}

In Exercises 8 through 12, refer to Example 6. In each case, to prove that the image is equal to a certain set $S$, you must prove that the image is contained in $S$, and also that every element of $S$ is in the image.

\begin{exercise}
    Let $F: \mathbb{R}^{2} \to \mathbb{R}^{2}$ be the mapping defined by $F(x, y) = (2x, 3y)$. Describe the image of the points lying on the circle $x^{2} + y^{2} = 1$.
\end{exercise}

\begin{proof}
    If $(a, b)$ lies on the circle $x^{2} + y^{2} = 1$, then $a^{2} + b^{2} = 1$. The image of $(a, b)$ under $F$ is $(2a, 3b)$. Coordinates of the image satisify the equation
    \[
        \frac{x^{2}}{4} + \frac{y^{2}}{9} = 1
    \]

    If $(u, v)$ lies on the curve $\frac{x^{2}}{4} + \frac{y^{2}}{9} = 1$, then $\frac{u^{2}}{4} + \frac{v^{2}}{9} = 1$. On the other hand, $F(\frac{u}{2}, \frac{v}{3}) = (u, v)$.

    Hence the image under $F$ of the points lying on the circle $x^{2} + y^{2} = 1$ is the ellipse $\frac{x^{2}}{4} + \frac{y^{2}}{9} = 1$.
\end{proof}

\begin{exercise}
    Let $F: \mathbb{R}^{2} \to \mathbb{R}^{2}$ be the mapping defined by $F(x, y) = (xy, y)$. Describe the image under $F$ of the straight line $x = 2$.
\end{exercise}

\begin{proof}
    The image of $(2, u)$ under $F$ is $(2u, u)$, which lies on the straight line $x - 2y = 0$.

    Let $(2t, t)$ be a point on the straight line $x - 2y = 0$. $F(2, t) = (2t, t)$. So the point $(2, t)$ is mapped to $(2t, t)$ by $F$.

    Hence the image under $F$ of the straight line $x = 2$ is the straight line $x - 2y = 0$.
\end{proof}

\begin{exercise}
    Let $F$ be the mapping defined by $F(x, y) = (e^{x}\cos y, e^{x}\sin y)$. Describe the image under $F$ of the line $x = 1$. Describe more generally the image under $F$ of a line $x = c$, where $c$ is a constant.
\end{exercise}

\begin{proof}
    The image of $(1, u)$ under $F$ is $(e\cos u, e\sin u)$. This point lies on the circle $x^{2} + y^{2} = e^{2}$.

    Let $(a, b)$ be a point on the circle $x^{2} + y^{2} = e^{2}$, then $\frac{a^{2}}{e^{2}} + \frac{y^{2}}{e^{2}} = 1$. So there exists $\theta$ such that $\cos{\theta} = \frac{a}{e}$ and $\sin{\theta} = \frac{b}{e}$. Therefore $F(1, \theta) = (e\cos{\theta}, e\sin{\theta}) = (a, b)$.

    Hence the image under $F$ of the line $x = 1$ is the circle $x^{2} + y^{2} = e^{2}$.

    The image of $(c, u)$ under $F$ is $(e^{c}\cos u, e^{c}\sin u)$. This point lies on the circle $x^{2} + y^{2} = e^{2c}$.

    Let $(a, b)$ be a point on the circle $x^{2} + y^{2} = e^{2c}$, then $\frac{x^{2}}{e^{2c}} + \frac{y^{2}}{e^{2c}} = 1$. So there exists $\theta$ such that $\cos{\theta} = \frac{a}{e^{c}}$ and $\sin{\theta} = \frac{b}{e^{c}}$. Therefore, $F(c, \theta) = (e^{c}\cos{\theta}, e^{c}\sin{\theta}) = (a, b)$.

    Hence the image under $F$ of the line $x = c$ is the circle $x^{2} + y^{2} = e^{2c}$.
\end{proof}

\begin{exercise}
    Let $F$ be the mapping defined by $F(t, u) = (\cos t, \sin t, u)$. Describe geometrically the image of the $(t, u)$-plane under $F$.
\end{exercise}

\begin{proof}
    The image of $(t, u)$ is the cylinder surface $x^{2} + y^{2} = 1$, whose radii is $1$, axis is $x = y = 0$.

    If $(a, b, c)$ is a point on the cylinder surface $x^{2} + y^{2} = 1$, then $a^{2} + b^{2} = 1$. So there exists $\theta$ such that $a = \cos\theta$ and $b = \sin\theta$. Therefore $F(\theta, c) = (\cos\theta, \sin\theta, c) = (a, b, c)$.

    Hence the image of the $(t, u)$-plane under $F$ is the cylinder surface $x^{2} + y^{2} = 1$.
\end{proof}

\begin{exercise}
    Let $F$ be the mapping defined by $F(x, y) = (x/3, x/4)$. What is the image under $F$ of the ellipse
    \[
        \frac{x^{2}}{9} + \frac{y^{2}}{16} = 1.
    \]
\end{exercise}

\begin{proof}
    If $(x, y)$ lies on the given ellipse, then $-3 \le x \le 3$. Therefore, $-1\le x/3 \le 1$ and $\frac{-3}{4} \le x/4 \le \frac{3}{4}$. So the image of $(x, y)$ under $F$ is the straight line segment whose extremites are $(-1, -3/4)$ and $(1, 3/4)$.

    If $(a, b)$ is a point on the straight line segment whose extermites are $(-1, -3/4)$ and $(1, 3/4)$, then $(a, b)$ also lies on the straight line $3x - 4y = 0$. Hence $F(3a, b) = (a, 3a/4) = (a, b)$.

    Hence the image under $F$ of the given ellipse is the straight line segment whose extremites are $(-1, -3/4)$ and $(1, 3/4)$.
\end{proof}

\section{Linear Mappings}
\setcounter{exercise}{0}

\begin{exercise}
    Determine which of the following mappings $F$ are linear
    \begin{enumerate}[label={(\alph*)}]
        \item $F: \mathbb{R}^{3} \to \mathbb{R}^{2}$ defined by $F(x, y, z) = (x, z)$
        \item $F: \mathbb{R}^{4} \to \mathbb{R}^{4}$ defined by $F(X) = -X$
        \item $F: \mathbb{R}^{3} \to \mathbb{R}^{3}$ defined by $F(X) = X + (0, -1, 0)$
        \item $F: \mathbb{R}^{2} \to \mathbb{R}^{2}$ defined by $F(x, y) = (2x + y, y)$
        \item $F: \mathbb{R}^{2} \to \mathbb{R}^{2}$ defined by $F(x, y) = (2x, y - x)$
        \item $F: \mathbb{R}^{2} \to \mathbb{R}^{2}$ defined by $F(x, y) = (y, x)$
        \item $F: \mathbb{R}^{2} \to \mathbb{R}$ defined by $F(x, y) = xy$
        \item Let $U$ be an open subset of $\mathbb{R}^{3}$, and let $V$ be the vector space of differentiable functions on $U$. Let $V'$ be the vector space of vector fields on $U$. Then $\text{grad}: V \to V'$ is a mapping. Is it linear?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item
              \begin{align*}
                  F(x_{1} + x_{2}, y_{1} + y_{2}, z_{1} + z_{2}) & = (x_{1} + x_{2}, z_{1} + z_{2})                  \\
                                                                 & = (x_{1}, z_{1}) + (x_{2}, z_{2})                 \\
                                                                 & = F(x_{1}, y_{1}, z_{1}) + F(x_{2}, y_{2}, z_{2}) \\
                  F(cx_{1}, cy_{1}, cz_{1}) = (cx_{1}, cz_{1})   & = c(x_{1}, z_{1})                                 \\
                                                                 & = cF(x_{1}, y_{1}, z_{1})
              \end{align*}

              Hence $F$ is a linear mapping.
        \item
              \begin{align*}
                  F(X + Y) & = -(X + Y) = (-X) + (-Y) = F(X) + F(Y) \\
                  F(cX)    & = -(cX) = c\cdot (-X) = cF(X)
              \end{align*}

              Hence $F$ is a linear mapping.
        \item $F(0, 0, 0) = (0, 0, 0) + (0, -1, 0) = (0, -1, 0) \ne (0, 0, 0)$. So $F$ is not a linear mapping.
        \item
              \begin{align*}
                  F(x_{1} + x_{2}, y_{1} + y_{2}) & = (2(x_{1} + x_{2}) + (y_{1} + y_{2}), y_{1} + y_{2}) \\
                                                  & = (2x_{1} + y_{1}, y_{1}) + (2x_{2} + y_{2}, y_{2})   \\
                                                  & = F(x_{1}, y_{1}) + F(x_{2}, y_{2})                   \\
                  F(cx_{1}, cy_{1})               & = (2cx_{1} + cy_{1}, cy_{1})                          \\
                                                  & = c(2x_{1} + y_{1}, y_{1})                            \\
                                                  & = cF(x_{1}, y_{1})
              \end{align*}

              Hence $F$ is a linear mapping.
        \item
              \begin{align*}
                  F(x_{1} + x_{2}, y_{1} + y_{2}) & = (2(x_{1} + x_{2}), (y_{1} + y_{2}) - (x_{1} + x_{2})) \\
                                                  & = (2x_{1}, y_{1} - x_{1}) + (2x_{2}, y_{2} - x_{2})     \\
                                                  & = F(x_{1}, y_{1}) + F(x_{2}, y_{2})                     \\
                  F(cx_{1}, cy_{1})               & = (2cx_{1}, cy_{1} - cx_{1})                            \\
                                                  & = c(2x_{1}, y_{1} - x_{1})                              \\
                                                  & = cF(x_{1}, y_{1})
              \end{align*}

              Hence $F$ is a linear mapping.

        \item
              \begin{align*}
                  F(x_{1} + x_{2}, y_{1} + y_{2}) & = (y_{1} + y_{2}, x_{1} + x_{2})    \\
                                                  & = (y_{1}, x_{1}) + (y_{2}, x_{2})   \\
                                                  & = F(x_{1}, y_{1}) + F(x_{2}, y_{2}) \\
                  F(cx_{1}, cy_{1})               & = (cy_{1}, cx_{1})                  \\
                                                  & = c(y_{1}, x_{1})                   \\
                                                  & = cF(x_{1}, y_{1})
              \end{align*}
        \item $F(cx, cy) = c^{2}xy = c^{2}F(x, y) \ne cF(x, y)$ if $c\ne 0, 1$ and $(x, y)\ne (0, 0)$.

              So $F$ is not a linear mapping.
        \item Let $f, g$ be elements of $V$.
              \begin{align*}
                  \text{grad}(f + g) & = \begin{bmatrix}
                                             \frac{\partial}{\partial x}(f + g) \\
                                             \frac{\partial}{\partial y}(f + g) \\
                                             \frac{\partial}{\partial z}(f + g)
                                         \end{bmatrix}
                  = \begin{bmatrix}
                        \frac{\partial}{\partial x}f + \frac{\partial}{\partial x}g \\
                        \frac{\partial}{\partial y}f + \frac{\partial}{\partial y}g \\
                        \frac{\partial}{\partial z}f + \frac{\partial}{\partial z}g
                    \end{bmatrix}
                  = \begin{bmatrix}
                        \frac{\partial}{\partial x}f \\
                        \frac{\partial}{\partial y}f \\
                        \frac{\partial}{\partial z}f
                    \end{bmatrix} +
                  \begin{bmatrix}
                      \frac{\partial}{\partial x}g \\
                      \frac{\partial}{\partial y}g \\
                      \frac{\partial}{\partial z}g
                  \end{bmatrix}
                  = \text{grad}(f) + \text{grad}(g)                         \\
                  \text{grad}(cf)    & = \begin{bmatrix}
                                             \frac{\partial}{\partial x}(cf) \\
                                             \frac{\partial}{\partial y}(cf) \\
                                             \frac{\partial}{\partial z}(cf)
                                         \end{bmatrix}
                  = c\begin{bmatrix}
                         \frac{\partial}{\partial x}f \\
                         \frac{\partial}{\partial y}f \\
                         \frac{\partial}{\partial z}f
                     \end{bmatrix}
                  = c\cdot\text{grad}(f)
              \end{align*}

              Hence $\text{grad}: V \to V'$ is a linear mapping.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $T: V \to W$ be a linear map from one vector space into another. Show that $T(O) = O$.
\end{exercise}

\begin{proof}
    $T(O) = T(0\cdot O) = 0\cdot T(O) = O$.
\end{proof}

\begin{exercise}
    Let $T: V \to W$ be a linear map. Let $u, v$ be elements of $V$, and let $Tu = w$. If $Tv = O$, show that $T(u + v)$ is also equal to $w$.
\end{exercise}

\begin{proof}
    Since $T: V\to W$ is a linear map, $T(u + v) = T(u) + T(v)$. $Tu = w, Tv = O$ so $T(u + v) = w + O = w$.
\end{proof}

\begin{exercise}
    Let $T: V \to W$ be a linear map. Let $U$ be the subset of elements $u\in V$ such that $T(u) = O$. Let $w\in W$ and suppose there is some element $v_{0}\in V$ such that $T(v_{0}) = w$. Show that the set of elements $v\in V$ satisfying $T(v) = w$ is precisely $v_{0} + U$.
\end{exercise}

\begin{proof}
    Denote by $U_{0}$ the set of $v\in V$ satisfying $T(v) = w$.

    Let $x = v_{0} + u$ for some $u$ in $V$. So $Tx = Tv_{0} + Tu = w + O = w$. So $v_{0} + U \subseteq U_{0}$.

    Let $y$ be an element of $U_{0}$. $T(y - v_{0}) = T(y) - T(v_{0}) = w - w = O$, so $y - v_{0}\in U$. Therefore, $y = v_{0} + (y - v_{0}) \in v_{0} + U$, and $U_{0} \subseteq v_{0} + U$.

    Hence $U_{0} = v_{0} + U$. So the set of $v\in V$ satisfying $T(v) = w$ is precisely $v_{0} + U$.
\end{proof}

\begin{exercise}
    Let $T: V \to W$ be a linear map. Let $v$ be an element of $V$. Show that $T(-v) = -T(v)$.
\end{exercise}

\begin{proof}
    $T(-v) = T((-1)v) = (-1)\cdot T(v) = -T(v)$.
\end{proof}

\begin{exercise}
    Let $V$ be a vector space, and $f: V \to \mathbf{R}, g: V \to \mathbf{R}$ two linear mappings. Let $F: V \to \mathbf{R}^{2}$ be the mapping defined by $F(v) = (f(v), g(v))$. Show that $F$ is linear. Generalize.
\end{exercise}

\begin{proof}
    \begin{align*}
        F(v_{1} + v_{2}) & = (f(v_{1} + v_{2}), g(v_{1} + v_{2}))        \\
                         & = (f(v_{1}) + f(v_{2}), g(v_{1}) + g(v_{2}))  \\
                         & = (f(v_{1}), g(v_{1})) + (f(v_{2}), g(v_{2})) \\
                         & = F(v_{1}) + F(v_{2})                         \\
        F(c\cdot v_{1})  & = (f(c\cdot v_{1}), g(c\cdot v_{1}))          \\
                         & = (c\cdot f(v_{1}), c\cdot g(v_{1}))          \\
                         & = c(f(v_{1}), g(v_{1}))                       \\
                         & = c\cdot F(v_{1})
    \end{align*}

    Hence $F$ is linear.

    Generalization: $f_{i}: V \to \mathbf{R}$ are linear mappings, where $i = 1, \ldots, n$. $F: V \to \mathbf{R}^{n}$ is defined as $F(v) = (f_{1}(v), \ldots, f_{n}(v))$. Then $F$ is linear.

    \begin{align*}
        F\left(\sum^{n}_{i=1}v_{i}\right) & = \left(f_{1}\left(\sum^{n}_{i=1}v_{i}\right), \ldots, f_{n}\left(\sum^{n}_{i=1}v_{i}\right)\right) \\
                                          & = \left(\sum^{n}_{i=1}f_{1}(v_{i}), \ldots, \sum^{n}_{i=1}f_{n}(v_{i})\right)                       \\
                                          & = \sum^{n}_{i=1}(f_{1}(v_{i}), \ldots, f_{n}(v_{i}))                                                \\
                                          & = \sum^{n}_{i=1}F(v_{i})                                                                            \\
        F(cv)                             & = (f_{1}(cv), \ldots, f_{n}(cv))                                                                    \\
                                          & = (c\cdot f_{1}(v), \ldots, c\cdot f_{n}(v))                                                        \\
                                          & = c (f_{1}(v), \ldots, f_{n}(v))                                                                    \\
                                          & = c\cdot F(v)
    \end{align*}

    Hence $F$ is linear.
\end{proof}

\begin{exercise}
    Let $V, W$ be two vector spaces and let $F: V \to W$ be a linear map. Let $U$ be the subset of $V$ consisting of all elements $v$ such that $F(v) = O$. Prove that $U$ is a subspace of $V$.
\end{exercise}

\begin{proof}
    Since $F(O) = O$, $U$ contains $O$, so $U$ is nonempty.

    Suppose that $x, y\in U$, and $c$ is a scalar.
    \begin{align*}
        F(x + y) & = F(x) + F(y) = O + O = O    \\
        F(cx)    & = c\cdot F(x) = c\cdot O = O
    \end{align*}

    So $x + y, cx\in U$. In other words, $U$ is closed under vector addition and scalar multiplication. Therefore, $U$ is a subspace of $V$.
\end{proof}

\begin{exercise}
    Which of the mappings in Exercises 4, 7, 8, 9, of \S{1} are linear?
\end{exercise}

\begin{proof}
    Mapping in Exercise 8 is linear. The others are not linear.
\end{proof}

\begin{exercise}
    Let $V$ be a vector space over $\mathbf{R}$, and let $v, w\in V$. The \textbf{line passing through $v$ and parallel to $w$} is defined to be the set of all elements $v + tw$ with $t\in\mathbb{R}$. The \textbf{line segment} between $v$ and $v + w$ is defined to be the set of all elements
    \[
        v + tw \quad\text{with}\quad 0 \leq t \leq 1.
    \]

    Let $L: V \to U$ be a linear map. Show that the image under $L$ of a line segment in $V$ is a line segment in $U$. Between what points?

    Show that the image of a line under $L$ is either a line or a point.
\end{exercise}

\begin{proof}
    Let $u$ be an element of $\{ v + tw \mid 0 \leq t \leq 1 \}$, then there exists $0 \leq t \leq 1$ such that $u = v + tw$.
    \[
        L(u) = L(v + tw) = L(v) + L(tw) = L(v) + t\cdot L(w)
    \]

    So $L(u)$ is an element of $\{ L(v) + t\cdot L(w) \mid 0\leq t\leq 1 \}$.

    Let $u'$ be an element of $\{ L(v) + t\cdot L(w) \mid 0\leq t\leq 1 \}$, then there exists $0\leq t\leq 1$ such that $u' = L(v) + t\cdot L(w)$.
    \[
        u' = L(v) + t\cdot L(w) = L(v) + L(tw) = L(v + tw)
    \]

    So there exists an element of $\{ v + tw \mid 0 \leq t\leq 1 \}$ such that its image under $L$ is $u'$.

    Thus the image under $L$ of a line segment in $V$ is a line segment in $U$, between $L(v)$ and $L(v + w)$.
\end{proof}

Let $V$ be a vector space, and let $v_{1}, v_{2}$ be two elements of $V$ which are linearly independent. The set of elements of $V$ which can be written in the form $t_{1}v_{1} + t_{2}v_{2}$ with numbers $t_{1}, t_{2}$ satisfying
\[
    0 \leq t_{1} \leq 1 \quad\text{and}\quad 0 \leq t_{2} \leq 1
\]

is called the \textbf{parallelogram} spanned by $v_{1}, v_{2}$.

\begin{exercise}
    Let $V$ and $W$ be vector spaces, and let $F: V \to W$ be a linear map. Let $v_{1}, v_{2}$ be linearly independent elements of $V$, and assume that $F(v_{1}), F(v_{2})$ are linearly independent. Show that the image under $F$ of the parallelogram spanned by $v_{1}$ and $v_{2}$ is the parallelogram spanned by $F(v_{1}), F(v_{2})$.
\end{exercise}

\begin{proof}
    Let $v$ be an element of the parallelogram spanned by $v_{1}, v_{2}$. According to the Definition of parallelogram, there exist $0\leq t_{1}, t_{2} \leq 1$ such that $v = t_{1}v_{1} + t_{2}v_{2}$.
    \[
        F(v) = F(t_{1}v_{1} + t_{2}v_{2}) = F(t_{1}v_{1}) + f(t_{2}v_{2}) = t_{1}\cdot F(v_{1}) + t_{2}\cdot F(v_{2})
    \]

    So the image of $v$ under $F$ is in the parallelogram spanned by $F(v_{1}), F(v_{2})$.

    Let $v'$ be an element of the parallelogram spanned by $F(v_{1}), F(v_{2})$. According to the Definition of parallelogram, there exist $0\leq t_{1}, t_{2} \leq 1$ such that $v' = t_{1}\cdot F(v_{1}) + t_{2}\cdot F(v_{2})$.
    \[
        v' = t_{1}\cdot F(v_{1}) + t_{2}\cdot F(v_{2}) = F(t_{1}v_{1}) + F(t_{2}v_{2}) = F(t_{1}v_{1} + t_{2}v_{2})
    \]

    So there exists an element in the parallelogram spanned by $v_{1}, v_{2}$ whose image under $F$ is $v'$.

    Hence the image under $F$ of the parallelogram spanned by $v_{1}, v_{2}$ is the parallelogram spanned by $F(v_{1}), F(v_{2})$.
\end{proof}

\begin{exercise}
    Let $F$ be a linear map from $\mathbb{R}^{2}$ into itself such that
    \[
        F(E_{1}) = (1, 1) \quad\text{and}\quad F(E_{2}) = (-1, 2).
    \]

    Let $S$ be the square whose corners are at $(0, 0), (1, 0), (1, 1)$, and $(0, 1)$. Show that the image of this square under $F$ is a parallelogram.
\end{exercise}

\begin{proof}
    $S$ is also the parallelogram spanned by $E_{1} = (1, 0)$ and $E_{2} = (0, 1)$.

    $F(E_{1}) = (1, 1)$ and $F(E_{2}) = (-1, 2)$ are linearly independent. According to Exercise 10, the image of the given square is a parallelogram.
\end{proof}

\begin{exercise}
    Let $A, B$ be two non-zero vectors in the plane such that there is no constant $c\ne 0$ such that $B = cA$. Let $T$ be a linear mapping of the plane into itself such that $T(E_{1}) = A$ and $T(E_{2}) = B$. Describe the image under $T$ of the rectangle whose corners are $(0, 1)$, $(3, 0)$, $(0, 0)$, and $(3, 1)$.
\end{exercise}

\begin{proof}
    $A, B$ are linearly independent.

    The given rectangle is also the parallelogram spanned by $(0, 1)$ and $(3, 0)$.

    $T((0, 1)) = T(E_{2}) = B$, $T((3, 0)) = T(3\cdot E_{1}) = 3\cdot T(E_{1}) = 3A$. Hence the image under $T$ of the given rectangle is the parallelogram spanned by $3A$ and $B$.
\end{proof}

\begin{exercise}
    Let $A, B$ be two non-zero vectors in the plane such that there is no constant $c\ne 0$ such that $B = cA$. Describe geometrically the set of points $tA + uB$ for values of $t$ and $u$ such that $0 \leq t \leq 5$ and $0 \leq u \leq 2$.
\end{exercise}

\begin{proof}
    $A, B$ are linearly independent.

    For $0\leq t\leq 5$ and $0\leq u\leq 2$
    \[
        tA + uB = \frac{t}{5}\cdot 5A + \frac{u}{2}\cdot 2B
    \]

    Since $0\leq \frac{t}{5} \leq 1$ and $0 \leq \frac{u}{2} \leq 1$, we conclude that the given set is the parallelogram spanned by $5A$ and $2B$.
\end{proof}

\begin{exercise}
    Let $T_{u}: V \to V$ be the translation by a vector $u$. For which vectors $u$ is $T_{u}$ a linear map? Proof?
\end{exercise}

\begin{proof}
    If $T_{u}$ is a linear map, then $T_{u}(O) = O$. On the other hand, $T_{u}(O) = O + u = u$. So $u = O$.

    If $u = O$
    \begin{align*}
        T_{O}(v + w)    & = v + w + O = (v + O) + (w + O) = T_{O}(v) + T_{O}(w) \\
        T_{O}(c\cdot v) & = c\cdot v + O = c(v + O) = c\cdot T_{O}(v)
    \end{align*}

    so $T_{O}$ is a linear map.

    Hence $T_{u}$ is a linear map if and only if $u = O$.
\end{proof}

\begin{exercise}
    Let $V, W$ be two vector spaces, and $F: V \to W$ a linear map. Let $w_{1}, \ldots, w_{n}$ be elements of $W$ which are linearly independent, and let $v_{1}, \ldots, v_{n}$ be elements of $V$ such that $F(v_{i}) = w_{i}$ for $i = 1,\ldots, n$. Show that $v_{1}, \ldots, v_{n}$ are linearly independent.
\end{exercise}

\begin{proof}
    Let $x_{1}, \ldots, x_{n}$ be scalars such that $x_{1}v_{1} + \cdots + x_{n}v_{n} = O$.
    \begin{align*}
        O & = F(O)                                               \\
          & = F(x_{1}v_{1} + \cdots + x_{n}v_{n})                \\
          & = F(x_{1}v_{1}) + \cdots + F(x_{n}v_{n})             \\
          & = x_{1}\cdot F(v_{1}) + \cdots + x_{n}\cdot F(v_{n}) \\
          & = x_{1}w_{1} + \cdots + x_{n}w_{n}
    \end{align*}

    Since $w_{1},\ldots, w_{n}$ are linearly independent, we obtain that $x_{1} = \cdots = x_{n} = 0$. Therefore, $v_{1}, \ldots, v_{n}$ are linearly independent.
\end{proof}

\begin{exercise}
    Let $V$ be a vector space and $F: V \to \mathbf{R}$ a linear map. Let $W$ be the subset of $V$ consisting of all elements $v$ such that $F(v) = 0$. Assume that $W \ne V$, and let $v_{0}$ be an element of $V$ which does not lie in $W$. Show that every element of $V$ can be written as a sum $w + cv_{0}$, with some $w$ in $W$ and some number $c$.
\end{exercise}

\begin{proof}
    Let $v$ be an element of $V$.

    Since $F(v_{0}) \ne 0$, there exist $c\in\mathbb{R}$ such that $F(v) = c\cdot F(v_{0})$. Let $w = v - cv_{0}$.
    \[
        F(w) = F(v - cv_{0}) = F(v) - F(cv_{0}) = F(v) - c\cdot F(v_{0}) = 0
    \]

    so $w\in W$.

    Thus, $v = (v - cv_{0}) + cv_{0} = w + cv_{0}$.
\end{proof}

\begin{exercise}
    In Exercise 16, show that $W$ is a subspace of $V$. Let $\{ v_{1}, \ldots, v_{n} \}$ be a basis of $W$. Show that $\{ v_{0}, v_{1}, \ldots, v_{n} \}$ is a basis of $V$.
\end{exercise}

\begin{proof}
    Let $x_{0}, x_{1}, \ldots, x_{n}$ be real numbers such that $x_{0}v_{0} + x_{1}v_{1} + \cdots + x_{n}v_{n} = O$.
    \begin{align*}
        O & = F(O)                                                                     \\
          & = F(x_{0}v_{0} + x_{1}v_{1} + \cdots + x_{n}v_{n})                         \\
          & = x_{0}\cdot F(v_{0}) + x_{1}\cdot F(v_{1}) + \cdots + x_{n}\cdot F(v_{n}) \\
          & = x_{0}\cdot F(v_{0}) + O + \cdots + O                                     \\
          & = x_{0}\cdot F(v_{0})
    \end{align*}

    Since $F(v_{0}) \ne 0$, we obtain that $x_{0} = 0$. So $x_{1}v_{1} + \cdots + x_{n}v_{n} = O$. Because $v_{1}, \ldots, v_{n}$ are linearly independent, it follows that $x_{1} = \cdots = x_{n} = 0$. So $x_{0} = x_{1} = \cdots = x_{n} = 0$, which means $v_{0}, v_{1}, \ldots, v_{n}$ are linearly independent.

    Let $v\in V$. According to Exercise 16, $v = cv_{0} + w$, where $w\in W$. Since $\{ v_{1}, \ldots, v_{n} \}$ is a basis of $W$, there exist $c_{1}, \ldots, c_{n}$ such that $w = c_{1}v_{1} + \cdots + c_{n}v_{n}$, so
    \[
        v = cv_{0} + c_{1}v_{1} + \cdots + c_{n}v_{n}
    \]

    Hence $\{ v_{0}, v_{1}, \ldots, v_{n} \}$ generates $V$ and is linearly independent. Thus it is a basis of $V$.
\end{proof}

\begin{exercise}
    Let $L: \mathbb{R}^{2} \to \mathbb{R}^{2}$ be a linear map, having the following effect on the indicated vectors
    \begin{enumerate}[label={(\alph*)}]
        \item $L(3, 1) = (1, 2)$ and $L(-1, 0) = (1, 1)$
        \item $L(4, 1) = (1, 1)$ and $L(1, 1) = (3, -2)$
        \item $L(1, 1) = (2, 1)$ and $L(-1, 1) = (6, 3)$
    \end{enumerate}

    In each case compute $L(1, 0)$.
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $L(1, 0) = -L(-1, 0) = (-1, -1)$.
        \item $L(1, 0) = \frac{1}{3}(L(4, 1) - L(1, 1)) = \frac{1}{3}(-2, 3) = (\frac{-2}{3}, 1)$.
        \item $L(1, 0) = \frac{1}{2}(L(1, 1) - L(-1, 1)) = \frac{1}{2}(-4, -2) = (-2, -1)$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $L$ be as in $(a), (b), (c)$, of Exercise 18. Find $L(0, 1)$.
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $L(0, 1) = L(3, 1) + 3L(-1, 0) = (4, 5)$.
        \item $L(0, 1) = \frac{1}{3}(4L(1, 1) - L(4, 1)) = \frac{1}{3}(11, -9) = (\frac{11}{3}, -3)$.
        \item $L(0, 1) = \frac{1}{2}(L(1, 1) + L(-1, 1)) = \frac{1}{2}(8, 4) = (4, 2)$.
    \end{enumerate}
\end{proof}

\section{The Kernal and Image of a Linear Map}
\setcounter{exercise}{0}

\begin{exercise}
    Let $A, B$ be two vectors in $\mathbf{R}^{2}$ forming a basis of $\mathbf{R}^{2}$. Let $F: \mathbf{R}^{2} \to \mathbf{R}^{n}$ be a linear map. Show that either $F(A), F(B)$ are linearly independent, or the image of $F$ has dimension $1$, or the image of $F$ is $\{ O \}$.
\end{exercise}

\begin{proof}
    $\dim\mathbf{R}^{2} = \dim\ker F + \dim\im F$

    $\dim\ker F$ can be either $0$, $1$, or $2$.

    If $\dim\ker F = 0$, then $F$ is injective, and $F(A), F(B)$ are linearly independent. If $\dim\ker F = 1$, then $\dim\im F = 1$. If $\dim\ker F = 2$, then $\ker F = \mathbf{R}^{2}$, which means the image of $F$ is $\{ O \}$.

    Thus either $F(A), F(B)$ are linearly independent, or the image of $F$ has dimension $1$, or the image of $F$ is $\{ O \}$.
\end{proof}

\begin{exercise}
    Let $A$ be a non-zero vector in $\mathbf{R}^{2}$. Let $F: \mathbf{R}^{2} \to W$ be a linear map such that $F(A) = O$. Show that the image of $F$ is either a straight line or $\{ O \}$.
\end{exercise}

\begin{proof}
    According to Rank-Nullity theorem, $\dim\mathbf{R}^{2} = \dim\ker F + \dim\im F$. So $\dim\ker F \leq 2$.

    Since $A$ is a non-zero vector and $F(A) = O$, then $\dim\ker F \geq 1$. So $\dim\ker F$ is either $1$ or $2$.

    If $\dim\ker F = 1$, then there exists a vector $B$ in $\mathbf{R}^{2}$ such that $F(B) \ne O$. So the image of $F$ is $\{ t\cdot F(B) \mid t\in\mathbf{R} \}$, which is a straight line.

    If $\dim\ker F = 2$, then $\ker F = \mathbf{R}^{2}$. So the image of $F$ is $\{ O \}$.

    Thus the image of $F$ is either a straight line or $\{ O \}$.
\end{proof}

\begin{exercise}
    Determine the dimension of the subspace of $\mathbf{R}^{4}$ consisting of all $X\in\mathbf{R}^{4}$ such that
    \[
        x_{1} + 2x_{2} = 0 \quad\text{and}\quad x_{3} - 15x_{4} = 0
    \]
\end{exercise}

\begin{proof}
    Let $v = (a_{1}, a_{2}, a_{3}, a_{4})$ be a vector of the given subspace. Since $a_{1} + 2a_{2} = a_{3} - 15a_{4} = 0$, we write
    \begin{align*}
        v & = (-2a_{2}, a_{2}, 15a_{4}, a_{4})                \\
          & = (-2a_{2}, a_{2}, 0, 0) + (0, 0, 15a_{4}, a_{4}) \\
          & = a_{2}(-2, 1, 0, 0) + a_{4}(0, 0, 15, 1)
    \end{align*}

    So $(-2, 1, 0, 0)$ and $(0, 0, 15, 1)$ generate the given subspace. Suppose that there exist a linear relation
    \[
        b_{1}(-2, 1, 0, 0) + b_{2}(0, 0, 15, 1) = (0, 0, 0, 0)
    \]

    then in terms of coordinates, $-2b_{1} = b_{1} = 15b_{2} = b_{2} = 0$. Therefore $b_{1} = b_{2} = 0$, from which we conclude that $(-2, 1, 0, 0)$ and $(0, 0, 15, 1)$ are linearly independent. So $(-2, 1, 0, 0)$ and $(0, 0, 15, 1)$ constitutes a basis of the given subspace. Hence the dimension of the given subspace is $2$.
\end{proof}

\begin{exercise}
    Let $L: V \to W$ be a linear map. Let $w$ be an element of $W$. Let $v_{0}$ be an element of $V$ such that $L(v_{0}) = w$. Show that any solution of the equation $L(X) = w$ is of type $v_{0} + U$, where $u$ is an element of the kernel of $L$.
\end{exercise}

\begin{proof}
    Let $X$ be a solution of the equation $L(X) = w$. By linearity
    \[
        L(X - v_{0}) = L(X) - L(v_{0}) = w - w = O.
    \]

    So $X - v_{0}$ is an element of $\ker L$. Hence $X = v_{0} + u$, where $u = X - v_{0} \in \ker L$.

    Let $Y$ be an element of $V$ of type $v_{0} + u$, where $u\in\ker L$. By linearity
    \[
        L(Y) = L(v_{0} + u) = L(v_{0}) + L(u) = w + O = w.
    \]

    So $Y$ is a solution of the equation $L(X) = w$.
\end{proof}

\begin{exercise}
    Let $V$ be the vector space of functions which have derivatives of all orders, and let $D: V \to V$ be the derivative. What is the kernel of $D$?
\end{exercise}

\begin{proof}
    The kernel of $D$ is the subspace of all constant functions.
\end{proof}

\begin{exercise}
    Let $D^{2}$ be the second derivative (i.e.\@ the iteration of $D$ taken twice). What is the kernel of $D^{2}$? In general, what is the kernel of $D^{n}$ ($n$-th derivative)?
\end{exercise}

\begin{proof}
    The kernel of $D^{2}$ is the subspace of all polynomial functions of degree less than $2$.

    The kernel of $D^{n}$ is the subspace of all polynomial functions of degree less than $n$.
\end{proof}

\begin{exercise}
    Let $V$ be again the vector of functions which have derivates of all orders. Let $W$ be the subspace of $V$ consisting of those functions $f$ such that
    \[
        f'' + 4f = 0 \quad\text{and}\quad f(\pi) = 0.
    \]

    Determine the dimension of $W$.
\end{exercise}

\begin{proof}
    % unsolved
\end{proof}

\begin{exercise}
    Let $V$ be the vector space of all infinitely differentiable functions. We write the functions as functions of variable $t$, and let $D = d/dt$. Let $a_{1}, \ldots, a_{m}$ be numbers. Let $g$ be an element of $V$. Describe how the problem of finding a solution of the differential equation
    \[
        a_{m}\frac{d^{m}f}{dt^{m}} + a_{m-1}\frac{d^{m-1}f}{dt^{m-1}} + \cdots + a_{0}f = g
    \]

    can be interpreted as fitting the asbtract situation described in Exercise 4.
\end{exercise}

\begin{proof}
    If $f^{*}$ is a particular solution to the differential equation, and $\tilde{f}$ is the general solution to the homogeneous differential equation
    \[
        a_{m}\frac{d^{m}f}{dt^{m}} + a_{m-1}\frac{d^{m-1}f}{dt^{m-1}} + \cdots + a_{0}f = 0
    \]

    then every solution of the given differential equation is of the form $f^{*} + \tilde{f}$.
\end{proof}

\begin{exercise}
    Again let $V$ be the space of all infinitely differentiable functions, and let $D: V \to V$ be the derivative.
    \begin{enumerate}[label={(\alph*)}]
        \item Let $L = D - I$ where $I$ is the identity mapping. What is the kernel of $L$?
        \item Same question if $L = D - aI$, where $a$ is a number.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item If $L(f) = 0$, then $D(f) = f$. So $f = c\cdot e^{x}$, where $c$ is a number. When $f = c\cdot e^{x}$, $L(f) = c\cdot e^{x} - c\cdot e^{x} = 0$.

              Hence $\ker L = \{ f \mid f(x) = c\cdot e^{x} \}$.
        \item If $L(f) = 0$, then $D(f) = a\cdot f$. So $f = c\cdot e^{ax}$, where $c$ is a number. When $f = c\cdot e^{ax}$, $L(f) = ca\cdot e^{ax} - a\cdot c\cdot e^{ax} = 0$.

              Hence $\ker L = \{ f \mid f(x) = c\cdot e^{ax} \}$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item What is the dimension of the subspace of $K^{n}$ consisting of those vectors $A = (a_{1}, \ldots, a_{n})$ such that $a_{1} + \cdots + a_{n} = 0$?
        \item What is the dimension of the subspace of the space of $n\times n$ matrices $(a_{i.i})$ such that
              \[
                  a_{1.1} + \cdots + a_{n.n} = \sum^{n}_{i=1}a_{i.i} = 0?
              \]
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $F$ be the linear map from $K^{n}$ to $K$ such that $F(a_{1}, \ldots, a_{n}) = a_{1} + \cdots + a_{n}$. The image of $K$ has dimension $1$.

              $\dim\ker F = \dim K^{n} - \dim\im F = n - 1$. Hence the dimension of the given subspace is $n - 1$.
        \item Let $F$ be the linear map from $\text{Mat}_{n\times n}(K)$ to $K$ such that $F((a_{i.j})) = \sum^{n}_{i=1}a_{i.i}$. The image of $K$ has dimension $1$.

              $\dim\ker F = \dim\text{Mat}_{n\times n}(K) - \dim\im F = n^{2} - 1$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $A = {(a_{i.j})}$ be an $n\times n$ matrix. Define the \textbf{trace} of $A$ to be the sum of the diagonal elements, that is
    \[
        \tr{A} = \sum^{n}_{i=1}a_{i.i}.
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that the trace is a linear map of the space of $n\times n$ matrices into $K$.
        \item If $A, B$ are $n\times n$ matrices, show that $\tr{AB} = \tr{BA}$.
        \item If $B$ is invertible, show that $\tr{B^{-1}AB} = \tr{A}$.
        \item If $A, B$ are $n\times n$ matrices, show that the association
              \[
                  (A, B) \mapsto \tr{AB} = \anglebracket{A, B}
              \]

              satisfies the three conditions of a scalar product.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $\tr{A + B} = \displaystyle\sum^{n}_{i=1}(a_{i.i} + b_{i.i}) = \displaystyle\sum^{n}_{i=1} a_{i.i} + \displaystyle\sum^{n}_{i=1} b_{i.i} = \tr{A} + \tr{B}$.

              $\tr{cA} = \displaystyle\sum^{n}_{i=1}ca_{i.i} = c\displaystyle\sum^{n}_{i=1} a_{i.i} = c\cdot\tr{A}$.

              So the trace is a linear map of the space of $\text{Mat}_{n\times n}(K)$.
        \item \begin{align*}
                  \tr{AB} & = \sum^{n}_{i=1}\left(\sum^{n}_{j=1}a_{i.j}b_{j.i}\right) \\
                          & = \sum^{n}_{j=1}\left(\sum^{n}_{i=1}b_{j.i}a_{i.j}\right) \\
                          & = \tr{BA}
              \end{align*}
        \item $\tr{B^{-1}AB} = \tr{ABB^{-1}} = \tr{A}$.
        \item $\tr{(A_{1} + A_{2})B} = \tr{A_{1}B + A_{2}B} = \tr{A_{1}B} + \tr{A_{2}B}$

              $\tr{A(B_{1}, B_{2})} = \tr{AB_{1} + AB_{2}} = \tr{AB_{1}} + \tr{AB_{2}}$.

              $\tr{(cA)B} = \tr{A(cB)} = \tr{c(AB)} = c\tr{AB}$.

              Therefore the association is bilinear.

              $\tr{AB} = \tr{BA}$. Therefore the association is symmetric.

              Hence the association satisfies three conditions of a scalar product.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $S$ be the set of symmetric $n\times n$ matrices. Show that $S$ is a vector space. What is the dimension of $S$? Exhibit a basis for $S$, when $n = 2$ and $n = 3$.
\end{exercise}

\begin{proof}
    The zero $n\times n$ matrix is a symmetric matrix, so $S$ is not empty. If $A, B$ are symmetric matrices, then $A + B$ and $cA$ are also symmetric, since $a_{i.j} + b_{i.j} = a_{j.i} + b_{j.i}$ and $ca_{i.j} = ca_{j.i}$. Hence $S$ is a subspace of the space of $n\times n$ matrices.

    When $n = 2$, a basis for $S$ is
    \[
        \begin{bmatrix}
            1 & 0 \\
            0 & 0
        \end{bmatrix},
        \begin{bmatrix}
            0 & 0 \\
            0 & 1
        \end{bmatrix},
        \begin{bmatrix}
            0 & 1 \\
            1 & 0
        \end{bmatrix}.
    \]

    When $n = 3$, a basis for $S$ is
    \[
        \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix},
        \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 0
        \end{bmatrix},
        \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 1
        \end{bmatrix},
        \begin{bmatrix}
            0 & 1 & 0 \\
            1 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix},
        \begin{bmatrix}
            0 & 0 & 1 \\
            0 & 0 & 0 \\
            1 & 0 & 0
        \end{bmatrix},
        \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 0 & 1 \\
            0 & 1 & 0
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Let $A$ be a real symmetric $n\times n$ matrix. Show that
    \[
        \tr{AA} \geq 0,
    \]

    and if $A\ne O$, then $\tr{AA} > 0$.
\end{exercise}

\begin{proof}
    Let $A = {(a_{i.j})} \in \text{Mat}_{n\times n}(\mathbf{R})$.

    \begin{align*}
        \tr{AA} & = \sum^{n}_{i=1}\left( \sum^{n}_{j=1} a_{i.j}a_{j.i} \right) \\
                & = \sum^{n}_{i=1}\left( \sum^{n}_{j=1} {a_{i.j}}^{2} \right)  \\
                & \geq 0
    \end{align*}

    Equality holds iff $a_{i.j} = 0$ for all $1\leq i, j \leq n$.
\end{proof}

\begin{exercise}
    (Field of characteristic other than $2$) An $n\times n$ matrix $A$ is called \textbf{skew-symmetric} if $\prescript{t}{}{A} = -A$. Show that any $n\times n$ matrix $A$ can be rewritten as a sum
    \[
        A = B + C,
    \]

    where $B$ is symmetric and $C$ is skew-symmetric.
\end{exercise}

\begin{proof}
    \[
        A = \frac{A + \prescript{t}{}{A}}{2} + \frac{A - \prescript{t}{}{A}}{2}
    \]

    where $B = \dfrac{A + \prescript{t}{}{A}}{2}$ is symmetric, and $C = \dfrac{A - \prescript{t}{}{A}}{2}$ is skew-symmetric.
\end{proof}

\begin{exercise}
    (Field of characteristic other than $2$) Let $M$ be the space of all $n\times n$ matrices. Let
    \[
        P: M \to M
    \]

    be the map such that
    \[
        P(A) = \frac{A + \prescript{t}{}{A}}{2}.
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that $P$ is linear.
        \item Show that the kernel of $P$ consists of the space of skew-symmetric matrices.
        \item What is the dimension of the kernel of $P$?
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $P(A + B) = \dfrac{(A + B) + \prescript{t}{}{(A + B)}}{2} = \dfrac{A + \prescript{t}{}{A}}{2} + \dfrac{B + \prescript{t}{}{B}}{2} = P(A) + P(B)$.

              $P(cA) = \dfrac{cA + \prescript{t}{}{cA}}{2} = \dfrac{cA + c\cdot\prescript{t}{}{A}}{2} = cP(A)$.

              Hence $P$ is linear.
        \item $P(A) = O$ iff $A + \prescript{t}{}{A} = O$. Equivalently, $\prescript{t}{}{A} = -A$. Hence the kernel of $P$ is the space of skew-symmetric matrices.
        \item $\dim\ker P = n(n-1)/2$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $M$ be the space of all $n\times n$ matrices. Let
    \[
        F: M\to M
    \]

    be the map such that
    \[
        F(A) = \frac{A - \prescript{t}{}{A}}{2}
    \]

    \begin{enumerate}[label={(\alph*)}]
        \item Show that $F$ is linear.
        \item Describe the kernel of $F$, and determine its dimension.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $F(A + B) = \dfrac{A + B - \prescript{t}{}(A + B)}{2} = \dfrac{A - \prescript{t}{}{A}}{2} + \dfrac{B - \prescript{t}{}{B}}{2} = F(A) + F(B)$.

              $F(cA) = \dfrac{cA - \prescript{t}{}(cA)}{2} = \dfrac{cA - c\cdot\prescript{t}{}{A}}{2} = cF(A)$.

              Hence $F$ is linear.
        \item $F(A) = O$ iff $A = \prescript{t}{}{A}$, which means $A$ is symmetric. The kernel of $F$ is the space of $n\times n$ symmetric matrices, its dimension is $n(n+1)/2$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $U, W$ be vector spaces. We let $U\times W$ be the set of all pairs $(u, w)$ with $u\in U$ and $w\in W$. If $(u_{1}, w_{1}), (u_{2}, w_{2})$ are such pairs, define their sum
              \[
                  (u_{1}, w_{1}) + (u_{2}, w_{2}) = (u_{1} + u_{2}, w_{1} + w_{2})
              \]

              If $c$ is a number, define $c(u, w) = (cu, cw)$. Show that $U\times W$ is a vector space with these definitions. What is the zero element?
        \item If $U$ has dimension $n$ and $W$ has dimension $m$, what is the dimension of $U\times W$? Exhibit a basis of $U\times W$ in terms of a basis for $U$ and a basis for $W$.
        \item If $U$ is a subspace of a vector space $V$, show that the subset of $V\times V$ consisting of all elements $(u, u)$ with $u\in U$ is a subspace.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item Component-wised addition is associative, has identity element, each element has an inverse, and commutative, these follow the properties of fields.

              \begin{align*}
                  c((u_{1}, w_{1}) + (u_{2}, w_{2})) & = c(u_{1} + u_{2}, w_{1} + w_{2})     \\
                                                     & = (cu_{1} + cu_{2}, cw_{1} + cw_{2})  \\
                                                     & = (cu_{1}, cw_{1}) + (cu_{2}, cw_{2}) \\
                                                     & = c(u_{1}, w_{1}) + c(u_{2}, w_{2}),  \\
                  (c_{1} + c_{2})(u, w)              & = (c_{1}u + c_{2}u, c_{1}w + c_{2}w)  \\
                                                     & = (c_{1}u, c_{1}w) + (c_{2}u, c_{2}w) \\
                                                     & = c_{1}(u, w) + c_{2}(u, w)           \\
                  c_{1}(c_{2}(u, w))                 & = c_{1}(c_{2}u, c_{2}w),              \\
                                                     & = (c_{1}(c_{2}u), c_{1}(c_{2}w))      \\
                                                     & = ((c_{1}c_{2})u, (c_{1}c_{2})w)      \\
                                                     & = c_{1}c_{2}(u, w),                   \\
                  1(u, w)                            & = (1u, 1w)                            \\
                                                     & = (u, w).
              \end{align*}

              So $U\times W$ is a vector space. The zero element is $(O_{U}, O_{W})$.
        \item Let $\{ u_{1}, \ldots, u_{n} \}$ be a basis of $U$, $\{ w_{1}, \ldots, w_{m} \}$ be a basis of $W$.

              A basis of $U\times W$ is $\{ (u_{1}, O_{W}), \ldots, (u_{n}, O_{W}), (O_{U}, w_{1}), \ldots, (O_{U}, w_{m}) \}$.

              $\dim (U\times W) = \dim U \cdot \dim W = nm$.
        \item The subset contains $(O_{V}, O_{V})$. $(u_{1}, u_{1}), (u_{2}, u_{2})$ are elements of the subset, then $(u_{1} + u_{2}, u_{1} + u_{2})$ and $(cu_{1}, cu_{2})$ are also elements of the subset. Hence the subset is a subspace of $V\times V$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    (To be done after you have done Exercise 17.) Let $U, W$ be subspaces of a vector space $V$. Show that
    \[
        \boxed{\dim U + \dim W = \dim (U + W) + \dim(U\cap W).}
    \]
\end{exercise}

\begin{proof}
    The map $L: U\times W \to V$ given by $L(u, w) = u - w$ is a linear map, since $L(u_{1} + u_{2}, w_{1} + w_{2}) = (u_{1} + u_{2}) - (w_{1} + w_{2}) = (u_{1} - w_{1}) + (u_{2} - w_{2}) = L(u_{1}, w_{1}) + L(u_{2}, w_{2})$ and $L(cu, cw) = cu - cw = c(u - w) = cL(u, w)$.

    The image of $L$ is precisely $U + W$. The kernel of $L$ is precisely is the space of $(t, t)$ where $t\in U\cap W$.

    According to Rank-Nullity theorem, $\dim (U\times W) = \dim\ker L + \dim\im L$. So $\dim U + \dim W = \dim (U\times W) = \dim (U\cap W) + \dim (U + W)$.
\end{proof}

\section{Composition and Inverse of Linear Mappings}
\setcounter{exercise}{0}

\begin{exercise}
    Let $L: \mathbf{R}^{2} \to \mathbf{R}^{2}$ be a linear map such that $L\ne O$ but $L^{2} = L \circ L = O$. Show that there exists a basis $\{ A, B \}$ of $\mathbf{R}^{2}$ such that
    \[
        L(A) = B \qquad\text{and}\qquad L(B) = O.
    \]
\end{exercise}

\begin{proof}
    Because $L\ne O$, there exists $A\in\mathbf{R}^{2}$ such that $L(A) \ne O$. Consider the linear relation $aA + bL(A) = O$. $O = L(O) = L(aA + bL(A)) = aL(A) + bL^{2}(A) = aL(A)$. Since $L(A) \ne O$, then $a = 0$. From $a = 0$, we obtain that $bL(A) = O$. Once again, $L(A)\ne O$, so $b = 0$. Therefore, $A$ and $B = L(A)$ is a desired basis of $\mathbf{R}^{2}$.
\end{proof}

\begin{exercise}
    Let $\dim V > \dim W$. Let $L: V \to W$ be a linear map. Show that the kernel of $L$ is not $\{ O \}$.
\end{exercise}

\begin{proof}
    $\dim\ker L = \dim V - \dim W > \dim V - \dim V = 0$. Therefore, $\ker L$ is not $\{ O \}$.
\end{proof}

\begin{exercise}
    Let $F: U\to V$ be a linear map, and assume that this map has an inverse mapping $G: V\to U$. Then $G$ is a linear map.
\end{exercise}

\begin{proof}
    Let $v_{1} = F(u_{1}), v_{2} = F(u_{2})$. Because $G$ is an inverse mapping of $F$, then $G(v_{1}) = u_{1}$ and $G(v_{2}) = u_{2}$.
    \begin{align*}
        G(v_{1} + v_{2}) & = G(F(u_{1}) + F(u_{2})) = G(F(u_{1} + u_{2})) = u_{1} + u_{2} = G(v_{1}) + G(v_{2}) \\
        G(cv_{1})        & = G(cF(u_{1})) = G(F(cu_{1})) = cu_{1} = cG(v_{1})
    \end{align*}

    Hence $G$ is a linear map.
\end{proof}

\begin{exercise}
    Let $\dim V = \dim W$. Let $L: V \to W$ be a linear map whose kernel is $\{ O \}$. Show that $L$ has an inverse linear map.
\end{exercise}

\begin{proof}
    Let $\{ v_{1}, \ldots, v_{n} \}$ be a basis of $V$. Let $w_{i} = L(v_{i})$, where $i = \overline{1, n}$. Consider the linear relation $c_{1}w_{1} + \cdots + c_{n}w_{n} = O$, from this, we obtain that $L(c_{1}v_{1} + \cdots + c_{n}v_{n}) = O$. Since $\ker L = O$ and $v_{1}, \ldots, v_{n}$ are linearly independent, it follows that $c_{1} = \cdots = c_{n} = 0$ and $w_{1}, \ldots, w_{n}$ are linearly independent. On the other hand, $\dim V = \dim W$, from which we conclude that $\{ w_{1}, \ldots, w_{n} \}$ is a basis of $W$.

    Let $J: W \to V$ be a linear map given by $J(x_{1}w_{1} + \cdots x_{n}w_{n}) = x_{1}v_{1} + \cdots + x_{n}v_{n}$.
    \begin{align*}
        L(J(x_{1}w_{1} + \cdots + x_{n}w_{n})) & = L(x_{1}v_{1} + \cdots + x_{n}v_{n}) \\
                                               & = x_{1}w_{1} + \cdots + x_{n}w_{n},   \\
        J(L(x_{1}v_{1} + \cdots + x_{n}v_{n})) & = J(x_{1}w_{1} + \cdots + x_{n}w_{n}) \\
                                               & = x_{1}v_{1} + \cdots + x_{n}v_{n}
    \end{align*}

    Hence $J$ is an inverse linear map of $L$.
\end{proof}

\begin{exercise}
    Let $F, G$ be invertible linear maps of a vector space $V$ onto itself. Show that
    \[
        {(F \circ G)}^{-1} = G^{-1} \circ F^{-1}.
    \]
\end{exercise}

\begin{proof}
    First, we prove that if a linear map $L: V\to W$ has an inverse map, then its inverse map is unique. Suppose that $J, J': W\to V$ are inverse maps of $L$. According to the definition of inverse map, for all $w\in W$, $L(J(w)) = L(J'(w))$. $L$ has an inverse map means $L$ is a one-to-one map. Therefore, $J(w) = J'(w)$ for all $w\in W$. Thus $J$ and $J'$ are identical.

    Back to the problem. ${(F\circ G)}^{-1}$ is the inverse map of $F\circ G$
    \begin{align*}
        (F\circ G)\circ (G^{-1}\circ F^{-1}) & = (F \circ (G \circ G^{-1})) \circ F^{-1} \\
                                             & = (F \circ \text{id}_{V}) \circ F^{-1}    \\
                                             & = F \circ F^{-1}                          \\
                                             & = \text{id}_{V}                           \\
        (G^{-1}\circ F^{-1})\circ (F\circ G) & = (G^{-1}\circ (F^{-1}\circ F))\circ G    \\
                                             & = (G^{-1}\circ\text{id}_{V})\circ G       \\
                                             & = G^{-1}\circ G                           \\
                                             & = \text{id}_{V}
    \end{align*}

    So $G^{-1}\circ F^{-1}$ is the inverse map of $F\circ G$. Thus ${(F\circ G)}^{-1} = G^{-1}\circ F^{-1}$.
\end{proof}

\begin{exercise}
    Let $L: \mathbf{R}^{2} \to \mathbf{R}^{2}$ be the linear map defined by
    \[
        L(x, y) = (x+y, x-y)
    \]

    Show that $L$ is invertible.
\end{exercise}

\begin{proof}
    $L(x, y) = (0, 0)$ iff $x + y = x - y = 0$. Equivalently, $x = y = 0$. So the kernel of $L$ is $\{ (0, 0) \}$. According to Exercise 4, $L$ has an inverse linear map. In other words, $L$ is invertible.
\end{proof}

\begin{exercise}
    Let $L: \mathbf{R}^{2} \to \mathbf{R}^{2}$ be the linear map defined by
    \[
        L(x, y) = (2x + y, 3x - 5y)
    \]

    Show that $L$ is invertible.
\end{exercise}

\begin{proof}
    $L(x, y) = (0, 0)$ iff $2x + y = 3x - 5y = 0$. Solve for $x, y$ from this system of linear equations, we obtain $x = y = 0$, which means the kernel of $L$ is $\{ (0, 0) \}$. According to Exercise 4, $L$ is invertible.
\end{proof}

\begin{exercise}
    Let $L: \mathbf{R}^{3} \to \mathbf{R}^{3}$ be the linear maps as indicated. Show that $L$ is invertible in each case.
    \begin{enumerate}[label={(\alph*)}]
        \item $L(x, y, z) = (x - y, x + z, x + y + 2z)$
        \item $L(x, y, z) = (2x - y + z, x + y, 3x + y + z)$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $L(x, y, z) = (0, 0, 0)$ iff
              \[
                  \begin{cases}
                      x - y = 0 \\
                      x + z = 0 \\
                      x + y + 2z = 0
                  \end{cases}
              \]

              Solve for $x, y, z$, we obtain that $x = y = z = 0$, so the kernel of $L$ is $\{ (0, 0, 0) \}$. According to Exercise 4, $L$ is invertible.
        \item $L(x, y, z) = (0, 0, 0)$ iff
              \[
                  \begin{cases}
                      2x - y + z = 0 \\
                      x + y = 0      \\
                      3x + y + z = 0
                  \end{cases}
              \]

              Solve for $x, y, z$, we obtain that $x = y = z = 0$, so the kernel of $L$ is $\{ (0, 0, 0) \}$. According to Exercise 4, $L$ is invertible.
    \end{enumerate}
\end{proof}

\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $L: V \to V$ be a linear mapping such that $L^{2} = O$. Show that $I - L$ is invertible. ($I$ is the identity mapping on $V$.)
        \item Let $L: V \to V$ be a linear map such that $L^{2} + 2L + I = O$. Show that $L$ is invertible.
        \item Let $L: V \to V$ be a linear map such that $L^{3} = O$. Show that $I - L$ is invertible.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $(I + L)(I - L) = (I + L) + (I + L)(-L) = I + L + (-L) - L^{2} = I$, $(I - L)(I + L) = (I - L) + (L - L^{2}) = I - L^{2} = I$. So $I + L$ is the inverse map of $I - L$. Hence $I - L$ is invertible.
        \item $I = L(-L-2I) = (-L-2I)L$. So $-L-2I$ is the inverse map of $L$. Hence $L$ is invertible.
        \item $(I - L)(I + L + L^{2}) = (I - L) + (L - L^{2}) + (L^{2} - L^{3}) = I - L^{3} = I$. $(I + L + L^{2})(I - L) = (I + L + L^{2}) - (L + L^{2} + L^{3}) = I - L^{3} = I$. So $I + L + L^{2}$ is the inverse map of $I - L$. Hence $I - L$ is invertible.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $V$ be a vector space. Let $P: V \to V$ be a linear map such that $P^{2} = P$. Show that
    \[
        V = \ker P + \im P \quad\text{and}\quad \ker P \cap \im P = \{ O \},
    \]

    in other words, $V$ is the direct sum of $\ker P$ and $\im P$.
\end{exercise}

\begin{proof}
    Let $v$ be an element of $V$. $v = (v - Pv) + Pv$. $P(Pv) = Pv$ and $P(v - Pv) = Pv - P^{2}v = 0$. So $V = \ker P + \im P$.

    Suppose that $w\in \ker P\cap \im P$. Then there exists $u\in V$ such that $w = Pu$. Because $w\in \ker P$, then $Pw = O$. Therefore $O = Pw = P^{2}u = Pu = w$. So $w = O$. Hence $\ker P\cap \im P = \{ O \}$.

    Thus $V = \ker P \oplus \im P$.
\end{proof}

\begin{exercise}
    Let $V$ be a vector space, and let $P, Q$ be linear maps of $V$ into itself. Assume that they satisfy the following conditions:
    \begin{enumerate}[label={(\alph*)}]
        \item $P + Q = I$ (identity mapping).
        \item $PQ = QP = O$.
        \item $P^{2} = P$ and $Q^{2} = Q$.
    \end{enumerate}

    Show that $V$ is equal to the direct sum of $\im P$ and $\im Q$.
\end{exercise}

\begin{proof}
    Let $v$ be an element of $V$.

    $v = (v - Pv) + Pv$. On the other hand, $v - Pv = Iv - Pv = Qv \in \im Q$ and $Pv \im P$. So $V = \im P + \im Q$.

    Let $w\in\im P\cap\im Q$. Then there exist $x, y\in V$ such that $Px = Qy = w$. So $O = QPx = PQy = Qw = Pw$. Moreover, $w = Iw = (P + Q)w = Pw + Qw = O + O = O$. Hence $\im P\cap\im Q = \{ O \}$.

    Thus $V = \im P \oplus \im Q$.
\end{proof}

\begin{exercise}
    Notations being as in Exercise 11, show that the image of $P$ is equal to the kernel of $Q$.
\end{exercise}

\begin{proof}
    Let $v\in\im P$. Then there exists $u\in V$ such that $v = Pu$. So $Qv = QPu = O$ because $QP = O$. So $\im P \subseteq \ker Q$.

    Let $w\in\ker Q$. $O = Qw = (I - P)w = w - Pw$, so $w = Pw\in\im P$. So $\ker Q\subseteq \im P$.

    Thus $\im P = \ker Q$.
\end{proof}

\begin{exercise}
    Let $T: V\to V$ be a linear map such that $T^{2} = I$. Let
    \[
        P = \frac{1}{2}(I + T)\quad\text{and}\quad Q = \frac{1}{2}(I - T).
    \]

    Prove:
    \[
        P + Q = I;\quad P^{2} = P;\quad Q^{2} = Q;\quad PQ = QP = O.
    \]
\end{exercise}

\begin{proof}
    \[
        P + Q = \frac{1}{2}(I + T) + \frac{1}{2}(I - T) = I.
    \]

    Let $v$ be an element of $V$
    \[
        \begin{split}
            P^{2}v = P(Pv) = P(\frac{1}{2}v + \frac{1}{2}Tv) = \frac{1}{4}v + \frac{1}{4}Tv + \frac{1}{4}Tv + \frac{1}{4}T^{2}v = \frac{1}{2}v + \frac{1}{2}Tv = Pv, \\
            Q^{2}v = Q(Qv) = Q(\frac{1}{2}v - \frac{1}{2}Tv) = \frac{1}{4}v - \frac{1}{4}Tv - \frac{1}{4}Tv + \frac{1}{4}T^{2}v = \frac{1}{2}v - \frac{1}{2}Tv = Qv.
        \end{split}
    \]

    So $P^{2} = P$ and $Q^{2} = Q$. Moreover, $PQ = P(I - P) = P - P^{2} = O$, and $QP = (I - P)P = P - P^{2} = O$.
\end{proof}

\begin{exercise}
    Let $F: V \to W$ and $G: W \to U$ be isomorphisms of vector spaces over $K$. Show that $G\circ F$ is invertible, and that
    \[
        {(G \circ F)}^{-1} = F^{-1} \circ G^{-1}.
    \]
\end{exercise}

\begin{proof}
    Because $F, G$ are isomorphisms, each of them has inverse linear mapping.
    \[
        \begin{split}
            (G\circ F)\circ (F^{-1}\circ G^{-1}) = (G\circ (F\circ F^{-1}))\circ G^{-1} = (G\circ \text{id}_{W}) \circ G^{-1} = G\circ G^{-1} = \text{id}_{U}, \\
            (F^{-1}\circ G^{-1})\circ (G\circ F) = (F^{-1}\circ (G^{-1}\circ G))\circ F = (F^{-1}\circ \text{id}_{W})\circ F = F^{-1}\circ F = \text{id}_{V}
        \end{split}
    \]

    Hence ${(G\circ F)}^{-1} = F^{-1}\circ G^{-1}$.
\end{proof}

\begin{exercise}
    Let $F: V\to W$ and $G: W\to U$ be isomorphisms of vector spaces over $K$. Show that $G\circ F: V\to U$ is an isomorphism.
\end{exercise}

\begin{proof}
    $(G\circ F)(x) = (G\circ F)(y)$ implies $F(x) = F(y)$ because $G$ is one-to-one. $F(x) = F(y)$ implies $x = y$ because $F$ is one-to-one. Therefore $G\circ F$ is one-to-one.

    $u\in U$. Because $G$ is onto, there exists $w\in W$ such that $u = Gw$. Because $F$ is onto, there exists $v\in V$ such that $w = Fv$. So there exists $v\in V$ such that $(G\circ F)(v) = u$. Therefore $G\circ F$ is onto.

    \begin{align*}
        (G\circ F)(v_{1} + v_{2}) & = G(F(v_{1} + v_{2})) = G(F(v_{1}) + F(v_{2})) = G(F(v_{1})) + G(F(v_{2})) = (G\circ F)(v_{1}) + (G\circ F)(v_{2}), \\
        (G\circ F)(cv_{1})        & = G(F(cv_{1})) = G(cF(v_{1})) = cG(F(v_{1})) = c(G\circ F)(v_{1})
    \end{align*}

    Thus $G\circ U: V\to U$ is an isomorphism.
\end{proof}

\begin{exercise}
    Let $V, W$ be two vector spaces over $K$, of dimension $n$. Show that $V$ and $W$ are isomorphic.
\end{exercise}

\begin{proof}
    Let $\{ v_{1}, \ldots, v_{n} \}$ be a basis of $V$, $\{ w_{1}, \ldots, w_{n} \}$ a basis of $W$. Let $F: V\to W$ be defined as
    \[
        F(x_{1}v_{1} + \cdots + x_{n}v_{n}) = x_{1}w_{1} + \cdots + x_{n}w_{n}
    \]

    $F$ is an isomorphism, since $\im F = W$ and $\ker F = \{ O \}$. Hence $V$ and $W$ are isomorphic.
\end{proof}

\begin{exercise}
    Let $A$ be a linear map of a vector space into itself, and assume that
    \[
        A^{2} - A + I = O
    \]

    (where $I$ is the identity map). Show that $A^{-1}$ exists and is equal to $I - A$. Generalize.
\end{exercise}

\begin{proof}
    $A^{2} - A + I = O$ implies $I = A(I - A) = (I - A)A$. So $A$ is invertible, and $A^{-1} = I - A$.

    Generalization: $I + \sum^{n}_{k=1}{(-1)}^{k}A^{k} = O$, then $A$ is invertible and $A^{-1} = I + \sum^{n-1}_{k=1}{(-1)}^{k}A^{k}$.
\end{proof}

\begin{exercise}
    Let $A, B$ be linear maps of a vector space into itself. Assume that $AB = BA$. Show that
    \[
        {(A + B)}^{2} = A^{2} + 2AB + B^{2}
    \]

    and
    \[
        (A + B)(A - B) = A^{2} - B^{2}.
    \]
\end{exercise}

\begin{proof}
    ${(A + B)}^{2} = (A + B)(A + B) = A(A + B) + B(A + B) = A^{2} + AB + BA + B^{2} = A^{2} + 2AB + B^{2}$.

    $(A + B)(A - B) = A(A - B) + B(A - B) = A^{2} - AB + BA - B^{2} = A^{2} - B^{2}$.
\end{proof}

\begin{exercise}
    Let $A, B$ be linear maps of a vector space into itself. If the kernel of $A$ is $\{ O \}$ and the kernel of $B$ is $\{ O \}$, show that the kernel of $AB$ is also $\{ O \}$.
\end{exercise}

\begin{proof}
    Suppose that $(AB)v = O$. $(AB)v = A(Bv)$. Because the kernel of $A$ is $\{ O \}$, so $Bv = O$. Because the kernel of $B$ is $\{ O \}$, so $v = O$. Therefore $(AB)v = O$ implies $v = O$. Thus the kernel of $AB$ is also $\{ O \}$.
\end{proof}

\begin{exercise}
    More generally, let $A: V\to W$ and $B: W\to U$ be linear maps. Assume that the kernel of $A$ is $\{ O \}$ and the kernel of $B$ is $\{ O \}$. Show that the kernal of $BA$ is $\{ O \}$.
\end{exercise}

\begin{proof}
    Suppose that $(BA)v = O$. $(BA)v = B(Av)$. Because the kernel of $B$ is $\{ O \}$, so $Av = O$. Because the kernel of $A$ is $\{ O \}$, so $v = O$. Therefore $(BA)v = O$ implies $v = O$. Thus the kernel of $BA$ is $\{ O \}$.
\end{proof}

\begin{exercise}
    Let $A: V \to W$ and $B: W \to U$ be linear maps. Assume that $A$ is surjective and that $B$ is surjective. Show that $BA$ is surjective.
\end{exercise}

\begin{proof}
    Let $u$ be an element of $U$. Because $B$ is surjective, there exists $w\in W$ such that $Bw = u$. Because $A$ is surjective, there exists $v\in V$ such that $Av = w$. Therefore, there exists $v\in V$ such that $(BA)v = B(Av) = Bw = u$. Thus $BA$ is surjective.
\end{proof}

\section{Geometric Applications}
\setcounter{exercise}{0}

\begin{exercise}
    Show that the image under a linear map of a convex set is convex.
\end{exercise}

\begin{proof}
    Let $L: V\to W$ be a linear map and $S$ a convex set in $V$. Let $P', Q'$ be two points in the image of $S$ under $L$. Then there exist $P, Q$ in $V$ such that $P' = L(P), Q' = L(Q)$. The image of $tP + (1-t)Q$ under $L$ is $tL(P) + (1-t)L(Q)$, so all points on the line segment $L(P)L(Q)$ are in the image of $S$ under $L$. Thus the image of $S$ under $L$ is convex.
\end{proof}

\begin{exercise}
    Let $S_{1}$ and $S_{2}$ be convex sets in $V$. Show that the intersection $S_{1}\cap S_{2}$ is convex.
\end{exercise}

\begin{proof}
    If $S_{1}\cap S_{2} = \varnothing$, then it is convex.

    Otherwise, let $P, Q$ be two points in $S_{1}\cap S_{2}$. In other words, $P, Q$ are in $S_{1}$ and $S_{2}$. Because $S_{1}$ is convex, all points on the line segment $PQ$ are in $S_{1}$. Because $S_{2}$ is convex, all points on the line segment $PQ$ are in $S_{2}$. Therefore all points on the line segment $PQ$ are in $S_{1}\cap S_{2}$.
\end{proof}

\begin{exercise}
    Let $L: \mathbf{R}^{n}\to \mathbf{R}$ be a linear map. Let $S$ be the set of all points $A$ in $\mathbf{R}^{n}$ such that $L(A)\geq 0$. Show that $S$ is convex.
\end{exercise}

\begin{proof}
    Let $P, Q$ be two points in $S$. Every point on the line segment $PQ$ is of the form $tP + (1 - t)Q$ where $0\leq t \leq 1$.
    \[
        L(tP + (1-t)Q) = L(tP) + L((1-t)Q) = tL(P) + (1-t)L(Q) \geq 0 + 0 = 0
    \]

    Therefore $tP + (1-t)Q$ is also in $S$. Thus $S$ is convex.
\end{proof}

\begin{exercise}
    Let $L: \mathbf{R}^{n}\to \mathbf{R}$ be a linear map and $c$ a number. Show that the set $S$ consisting of all points $A$ in $\mathbf{R}^{n}$ such that $L(A) > c$ is convex.
\end{exercise}

\begin{proof}
    Let $P, Q$ be two points in $S$. Every point on the line segment $PQ$ is of the form $tP + (1 - t)Q$ where $0\leq t \leq 1$. If $t = 0$, $L(tP + (1-t)Q) = L(Q) > c$. If $t = 1$, $L(tP + (1-t)Q) = L(P) > c$. Otherwise, $0 < t < 1$
    \[
        L(tP + (1-t)Q) = L(tP) + L((1-t)Q) = tL(P) + (1-t)L(Q) > tc + (1-t)c = c
    \]

    In three cases, $tP + (1-t)Q$ is also in $S$. Thus $S$ is convex.
\end{proof}

\begin{exercise}
    Let $A$ be a non-zero vector in $\mathbf{R}^{n}$ and $c$ a number. Show that the set of points $X$ such that $X\cdot A \geq c$ is convex.
\end{exercise}

\begin{proof}
    Let $P, Q$ be points in $S$. All points on the line segment $PQ$ are of the form $tP + (1-t)Q$ where $0 \leq t \leq 1$.
    \[
        (tP + (1-t)Q)\cdot A = tP\cdot A + (1-t)Q\cdot A \geq tc + (1-t)c = c
    \]

    Hence the given set is convex.
\end{proof}

\begin{exercise}
    Let $L: V\to W$ be a linear map. Let $S'$ be a convex set in $W$. Let $S$ be the set of all elements $P$ in $V$ such that $L(P)$ is in $S'$. Show that $S$ is convex.
\end{exercise}

\begin{proof}
    Let $P, Q$ be points in $S$. All points on the line segment $PQ$ are of the form $tP + (1-t)Q$ where $0 \leq t \leq 1$. $L(tP + (1-t)Q) = tL(P) + (1-t)L(Q)$. Because $S'$ is convex and $L(P), L(Q)$ are in $S'$, then $tL(P) + (1-t)L(Q)$ are in $S'$ where $0\leq t\leq 1$. Due to the definition of $S$, $tP + (1-t)Q$ are also in $S$, where $0\leq t\leq 1$. Thus $S$ is convex.
\end{proof}

\begin{exercise}
    Show that a parallelogram is convex.
\end{exercise}

\begin{proof}
    Let's consider the parallelogram spanned by $v, w$. Let $t_{1}v + t_{2}w$ and $s_{1}v + s_{2}w$ be two points in the parallelogram, where $0\leq t_{1}, t_{2}, s_{1}, s_{2} \leq 1$. All points between these two points are of the form $t(t_{1}v + t_{2}w) + (1 - t)(s_{1}v + s_{2}w)$ where $0\leq t \leq 1$.
    \[
        t(t_{1}v + t_{2}w) + (1 - t)(s_{1}v + s_{2}w) = (tt_{1} + (1-t)s_{1})v + (tt_{2} + (1-t)s_{2})w
    \]

    $0\leq tt_{1} + (1 - t)s_{1} \leq t + (1-t) = 1$, $0\leq tt_{2} + (1-t)s_{2}\leq t + (1-t) = 1$. So all points between any two points in the parallelogram are also in the parallelogram.

    Hence every parallelogram is convex.
\end{proof}

\begin{exercise}
    Let $S$ be a convex set in $V$ and let $u$ be an element of $V$. Let $T_{u}: V\to V$ be the translation by $u$. Show that the image $T_{u}(S)$ is convex.
\end{exercise}

\begin{proof}
    Let $a, b$ be two elements in $T_{u}(S)$. $T_{u}(a - u) = a, T_{u}(b - u) = b$. The set of all points between $a$ and $b$ are $ta + (1-t)b$ where $0\leq t\leq 1$. $t(a-u) + (1-t)(b-u) = ta + (1-t)b - u$ is in $S$. So $T_{u}(ta + (1-t)b - u) = ta + (1-t)b$ is in $T_{u}(S)$ for all $0\leq t\leq 1$.

    Hence $T_{u}(S)$ is convex.
\end{proof}

\begin{exercise}
    Let $S$ be a convex set in the vector space $V$ and let $c$ be a number. Let $cS$ denote the set of all elements $cv$ with $v$ in $S$. Show that $cS$ is convex.
\end{exercise}

\begin{proof}
    Let $w_{1} = cv_{1}, w_{2} = cv_{2}$ be elements of $cS$. All elements between $w_{1}$ and $w_{2}$ are of the form $tw_{1} + (1-t)w_{2} = c(tv_{1} + (1-t)v_{2})$ where $0\leq t\leq 1$. Because $S$ is convex, $tv_{1} + (1-t)v_{2}$ is an element of $S$, so $c(tv_{1} + (1-t)v_{2})$ is an element of $cS$. Therefore all elements between $w_{1}$ and $w_{2}$ are in $cS$. Thus $cS$ is convex.
\end{proof}

\begin{exercise}
    Let $v, w$ be linearly independent elements of a vector space $V$. Let $F: V\to W$ be a linear map. Assume that $F(v), F(w)$ are linearly independent. Show that the image under $F$ of the parallelogram spanned by $v$ and $w$ is either a point or a line segment.
\end{exercise}

\begin{proof}
    All points in the parallelogram spanned by $v$ and $w$ are of the form $t_{1}v + t_{2}w$ where $0 \leq t_{1}, t_{2} \leq 1$. $F(t_{1}v + t_{2}w) = t_{1}F(v) + t_{2}F(w)$.

    Since $F(v), F(w)$ are linearly dependent, $F(v) = cF(w)$ for some number $c$ or $F(w) = cF(v)$ for some number $c$. Without loss of generality, suppose that $F(w) = cF(v)$. Then $F(t_{1}v + t_{2}w) = (t_{1} + ct_{2})F(v)$.

    \begin{enumerate}[label={\textbf{Case \arabic*.}},itemindent=1cm]
        \item $c\geq 0$.
              \[
                  F(t_{1}v + t_{2}w) = \dfrac{(1-t_{1}) + c(1-t_{2})}{1+c}F(O) + \dfrac{t_{1} + ct_{2}}{1+c}F((1+c)v)
              \]

              If $F(v) = O$, the image under $F$ of the parallelogram spanned by $v$ and $w$ is the point $O$.

              If $F(v)\ne O$, the image under $F$ of the parallelogram spanned by $v$ and $w$ is the line segment between $F(O)$ and $F((1+c)v)$.
        \item $c < 0$.
              \[
                  F(t_{1}v + t_{2}w) = \frac{(1-t_{1}) + c(1-t_{2})}{1-c}F(O) + \frac{t_{1} + ct_{2}}{1 - c}F((1 - c)v)
              \]

              If $F(v) = O$, the image under $F$ of the parallelogram spanned by $v$ and $w$ is the point $O$.

              If $F(v)\ne O$, the image under $F$ of the parallelogram spanned by $v$ and $w$ is the line segment between $F(O)$ and $F((1-c)v)$.
    \end{enumerate}

    In conclusion, the image under $F$ of the parallelogram spanned by $v$ and $w$ is either a point or a line segment.
\end{proof}
