\chapter{Matrices}

\section{The Space of Matrices}

\subsection{Exercises on Matrices}

\setcounter{exercise}{0}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            1  & 2 & 3 \\
            -1 & 0 & 2
        \end{bmatrix}
        \quad\text{and}\quad
        B = \begin{bmatrix}
            -1 & 5 & -2 \\
            2  & 2 & -1
        \end{bmatrix}.
    \]

    Find $A + B, 3B, -2B, A + 2B, 2A - B, A - 2B, B - A$.
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            A + B = \begin{bmatrix}
                0 & 7 & 1 \\
                1 & 2 & 1
            \end{bmatrix} \\
            3B = \begin{bmatrix}
                -3 & 15 & -6 \\
                6  & 6  & -3
            \end{bmatrix} \\
            -2B = \begin{bmatrix}
                2  & -10 & 4 \\
                -4 & -4  & 2
            \end{bmatrix} \\
            A + 2B = \begin{bmatrix}
                -1 & 12 & -1 \\
                3  & 4  & 0
            \end{bmatrix} \\
            2A - B = \begin{bmatrix}
                3  & -1 & 8 \\
                -4 & -2 & 5
            \end{bmatrix} \\
            A - 2B = \begin{bmatrix}
                3  & -8 & 7 \\
                -5 & -4 & 4
            \end{bmatrix} \\
            B - A = \begin{bmatrix}
                -2 & 3 & -5 \\
                3  & 2 & -3
            \end{bmatrix}
        \end{split}
    \]
\end{proof}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            1 & -1 \\
            2 & 2
        \end{bmatrix}
        \quad\text{and}\quad
        B = \begin{bmatrix}
            -1 & 1  \\
            0  & -3
        \end{bmatrix}.
    \]

    Find $A + B, 3B, -2B, A + 2B, A - B, B - A$
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            A + B = \begin{bmatrix}
                0 & 0  \\
                2 & -1
            \end{bmatrix} \\
            3B = \begin{bmatrix}
                -3 & 3 \\
                0  & 9
            \end{bmatrix} \\
            -2B = \begin{bmatrix}
                2 & -2 \\
                0 & 6
            \end{bmatrix} \\
            A + 2B = \begin{bmatrix}
                -1 & 1  \\
                2  & -4
            \end{bmatrix} \\
            A - B = \begin{bmatrix}
                2 & -2 \\
                2 & 5
            \end{bmatrix} \\
            B - A = \begin{bmatrix}
                -2 & 2  \\
                -2 & -5
            \end{bmatrix}
        \end{split}
    \]
\end{proof}

\begin{exercise}
    In Exercise 1, find $\prescript{t}{}{A}$ and $\prescript{t}{}{A}$.
\end{exercise}

\begin{proof}
    \[
        \prescript{t}{}{A} = \begin{bmatrix}
            1 & -1 \\
            2 & 0  \\
            3 & 2
        \end{bmatrix}
        \qquad
        \prescript{t}{}{B} = \begin{bmatrix}
            -1 & 2  \\
            5  & 2  \\
            -2 & -1
        \end{bmatrix}
    \]
\end{proof}

\begin{exercise}
    In Exercise 2, find $\prescript{t}{}{A}$ and $\prescript{t}{}{B}$.
\end{exercise}

\begin{proof}
    \[
        \prescript{t}{}{A} = \begin{bmatrix}
            1  & 2 \\
            -1 & 2
        \end{bmatrix}
        \qquad
        \prescript{t}{}{B} = \begin{bmatrix}
            -1 & 0  \\
            1  & -3
        \end{bmatrix}
    \]
\end{proof}

\begin{exercise}
    If $A, B$ are arbitrary $m\times n$ matrices, show that
    \[
        \prescript{t}{}{(A + B)} = \prescript{t}{}{A} + \prescript{t}{}{B}.
    \]
\end{exercise}

\begin{proof}
    The $ij$-entry of $\prescript{t}{}{(A + B)}$ is the $ji$-entry of $A + B$, and it is equal to $a_{ji} + b_{ji}$. On the other hand, $a_{ji}$ is the $ij$-entry of $\prescript{t}{}{A}$, $b_{ji}$ is the $ij$-entry of $\prescript{t}{}{B}$. Hence $\prescript{t}{}{(A + B)} = \prescript{t}{}{A} + \prescript{t}{}{B}$.
\end{proof}

\begin{exercise}
    If $c$ is a number, show that
    \[
        \prescript{t}{}{(cA)} = c\cdot\prescript{t}{}{A}.
    \]
\end{exercise}

\begin{proof}
    The $ij$-entry of $\prescript{t}{}{(cA)}$ is the $ji$-entry of $cA$, and it is equal to $c\cdot a_{ji}$. $a_{ji}$ is the $ij$-entry of $\prescript{t}{}{A}$. Hence $\prescript{t}{}{(cA)} = c\cdot\prescript{t}{}{A}$.
\end{proof}

\begin{exercise}
    If $A = (a_{ij})$ is a square matrix, then the elements $a_{ii}$ are called the \textbf{diagonal} elements. How do the diagonal elements of $A$ and $\prescript{t}{}{A}$ differ?
\end{exercise}

\begin{proof}
    The diagonal entries of $A$ and $\prescript{t}{}{A}$ are the same.
\end{proof}

\begin{exercise}
    Find $\prescript{t}{}{(A + B)}$ and $\prescript{t}{}{A} + \prescript{t}{}{B}$ in Exercise 2.
\end{exercise}

\begin{proof}
    \[
        \prescript{t}{}{(A + B)} = \prescript{t}{}{A} + \prescript{t}{}{B} = \begin{bmatrix}
            0 & 2  \\
            0 & -1
        \end{bmatrix}
    \]
\end{proof}

\begin{exercise}
    Find $A + \prescript{t}{}{A}$ and $B + \prescript{t}{}{B}$ in Exercise 2.
\end{exercise}

\begin{proof}
    \[
        \begin{split}
            A + \prescript{t}{}{A} = \begin{bmatrix}
                2 & 1 \\
                1 & 4
            \end{bmatrix} \\
            B + \prescript{t}{}{B} = \begin{bmatrix}
                -2 & 1  \\
                1  & -6
            \end{bmatrix}
        \end{split}
    \]
\end{proof}

\begin{exercise}
    Show that for any square matrix $A$, the matrix $A + \prescript{t}{}{A}$ is symmetric.
\end{exercise}

\begin{proof}
    According to the definition of transpose matrix, if matrix $B$ is the transpose of matrix $A$, then matrix $A$ is the transpose of matrix $B$. Hence $A = \prescript{t}{}{\prescript{t}{}{A}}$. According to Exercise 5
    \[
        \prescript{t}{}{(A + \prescript{t}{}{A})} = \prescript{t}{}{A} + \prescript{t}{}{\prescript{t}{}{A}} = \prescript{t}{}{A} + A = A + \prescript{t}{}{A}.
    \]

    Hence the matrix $A + \prescript{t}{}{A}$ is symmetric.
\end{proof}

\begin{exercise}
    Write down the row vectors and column vectors of the matrices $A, B$ in Exercise 1.
\end{exercise}

\begin{proof}
    The row vectors of $A$ are
    \[
        \begin{bmatrix}
            1 & 2 & 3
        \end{bmatrix},\quad
        \begin{bmatrix}
            -1 & 0 & 2
        \end{bmatrix}
    \]

    The column vectors of $A$ are
    \[
        \begin{bmatrix}
            1 \\
            -1
        \end{bmatrix},\quad
        \begin{bmatrix}
            2 \\
            0
        \end{bmatrix},\quad
        \begin{bmatrix}
            3 \\
            2
        \end{bmatrix}
    \]

    The row vectors of $B$ are
    \[
        \begin{bmatrix}
            -1 & 5 & -2
        \end{bmatrix},\quad
        \begin{bmatrix}
            2 & 2 & -1
        \end{bmatrix}
    \]

    The column vectors of $B$ are
    \[
        \begin{bmatrix}
            -1 \\
            2
        \end{bmatrix},\quad
        \begin{bmatrix}
            5 \\
            2
        \end{bmatrix},\quad
        \begin{bmatrix}
            -2 \\
            -1
        \end{bmatrix}
    \]
\end{proof}

\begin{exercise}
    Write down the row vectors and column vectors of the matrices $A, B$ in Exercise 2.
\end{exercise}

\begin{proof}
    The row vectors of $A$ are
    \[
        \begin{bmatrix}
            1 & -1
        \end{bmatrix},\quad
        \begin{bmatrix}
            2 & 2
        \end{bmatrix}
    \]

    The column vectors of $A$ are
    \[
        \begin{bmatrix}
            1 \\
            2
        \end{bmatrix},\quad
        \begin{bmatrix}
            -1 \\
            2
        \end{bmatrix}
    \]

    The row vectors of $B$ are
    \[
        \begin{bmatrix}
            -1 & 1
        \end{bmatrix},\quad
        \begin{bmatrix}
            0 & -3
        \end{bmatrix}
    \]

    The column vectors of $B$ are
    \[
        \begin{bmatrix}
            -1 \\
            0
        \end{bmatrix},\quad
        \begin{bmatrix}
            1 \\
            -3
        \end{bmatrix}
    \]
\end{proof}

\subsection{Exercises on Dimensions}

\setcounter{exercise}{0}

\begin{exercise}
    What is the dimension of the space of $2\times 2$ matrices? Give a basis for this space.
\end{exercise}

\begin{proof}
    $\dim\text{Mat}_{2\times 2}(\mathbb{R}) = 4$.

    A basis for this space includes the following matrices
    \[
        \begin{bmatrix}
            1 & 0 \\
            0 & 0
        \end{bmatrix},\quad
        \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix},\quad
        \begin{bmatrix}
            0 & 0 \\
            1 & 0
        \end{bmatrix},\quad
        \begin{bmatrix}
            0 & 0 \\
            0 & 1
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    What is the dimension of the space of $m\times n$ matrices? Give a basis for this space.
\end{exercise}

\begin{proof}
    $\dim\text{Mat}_{m\times n}(\mathbb{R}) = mn$.

    A basis for this space includes $mn$ matrices: each matrix has $ij$-entry equals to $1$ and others equal $0$.
\end{proof}

\begin{exercise}
    What is the dimension of the space of $n\times n$ matrices of all of whose components are $0$ expect possibly the diagonal components.
\end{exercise}

\begin{proof}
    The dimension of the space of $n\times n$ diagonal matrices is $n$.

    A basis for this space includes $n$ matrices: the $i$-th matrix is the $n\times n$ matrix whose $ii$-entry is equal to $1$ and other entries are equal $0$ ($1\le i\le n$).
\end{proof}

\begin{exercise}
    What is the dimension of the space of $n\times n$ matrices which are \textbf{upper-triangular}, i.e.\@ of the following type:
    \[
        \begin{bmatrix}
            a_{1.1} & a_{1.2} & \cdots & a_{1.n} \\
            0       & a_{2.2} & \cdots & a_{2.n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & 0       & \cdots & a_{n.n}
        \end{bmatrix}?
    \]
\end{exercise}

\begin{proof}
    Denote by $E_{i.j}$ the $n\times n$ matrix whose $ij$-entry is equal to $1$ and other entries are equal to $0$.
    \[
        \begin{bmatrix}
            a_{1.1} & a_{1.2} & \cdots & a_{1.n} \\
            0       & a_{2.2} & \cdots & a_{2.n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & 0       & \cdots & a_{n.n}
        \end{bmatrix}
        = \sum_{1\le i \le j \le n} a_{i.j}E_{i.j}
    \]

    So ${\{ E_{i.j} \}}_{1\le i \le j \le n}$ is a generating set of the space of $n\times n$ upper-triangular matrices. On the other hand, $E_{1.1}, \ldots, E_{i.j}, \ldots, E_{n.n}$ are linearly independent, since $\sum_{1\le i\le j\le n}x_{i.j}E_{i.j} = O$ implies $x_{i.j} = 0$ for every $1\le i\le j\le n$.

    Hence ${\{ E_{i.j} \}}_{1\le i \le j \le n}$ is a basis of the space of $n\times n$ upper-triangular matrices, and the dimension of this space is $\frac{n(n+1)}{2}$.
\end{proof}

\begin{exercise}
    What is the dimension of the space of symmetric $2\times 2$ matrices (i.e. $2\times 2$ matrices $A$ such that $A = \prescript{t}{}{A}$)? Exhibit a basis for this space.
\end{exercise}

\begin{proof}
    \[
        A = \begin{bmatrix}
            x & y \\
            y & z
        \end{bmatrix}
        = x\begin{bmatrix}
            1 & 0 \\
            0 & 0
        \end{bmatrix}
        + y\begin{bmatrix}
            0 & 1 \\
            1 & 0
        \end{bmatrix}
        + z\begin{bmatrix}
            0 & 0 \\
            0 & 1
        \end{bmatrix}
    \]

    A basis for the dimension of the space of symmetric $2\times 2$ matrices contains the following matrices
    \[
        \begin{bmatrix}
            1 & 0 \\
            0 & 0
        \end{bmatrix},\quad
        \begin{bmatrix}
            0 & 1 \\
            1 & 0
        \end{bmatrix},\quad
        \begin{bmatrix}
            0 & 0 \\
            0 & 1
        \end{bmatrix}
    \]

    and this space has dimension of $3$.
\end{proof}

\begin{exercise}
    More generally, what is the dimension of the space of symmetric $n\times n$ matrices? What is a basis for this space?
\end{exercise}

\begin{proof}
    Denote by $S_{i.j}$ the $n\times n$ matrix whose $ij$-entry, $ji$-entry are equal to $1$ and other entries are equal to $0$.

    Let
    \[
        A = \begin{bmatrix}
            a_{1.1} & a_{1.2} & \cdots & a_{1.n} \\
            a_{1.2} & a_{2.2} & \cdots & a_{2.n} \\
            \vdots  & \vdots  &        & \vdots  \\
            a_{1.n} & a_{2.n} & \cdots & a_{n.n}
        \end{bmatrix}
    \]

    be a symmetric matrix of size $n\times n$.
    \[
        A = \sum^{n}_{i=1}a_{i.i}S_{i.i} + \sum_{1\le i < j \le n}a_{i.j}S_{i.j}.
    \]

    So ${\{ S_{i.j} \}}_{1\le i\le j\le n}$ is a generating set of the space of symmetric $n\times n$ matrices. On the other hand, $S_{1.1}, \ldots, S_{i.j}, \ldots, S_{n.n}$ are linearly independent.

    Hence the dimension of the space of symmetric $n\times n$ matrices is $\frac{n(n+1)}{2}$. A basis for this space is ${\{ S_{i.j} \}}_{1\le i\le j\le n}$.
\end{proof}

\begin{exercise}
    What is the dimension of the space of diagonal $n\times n$ matrices? What is a basis for this space?
\end{exercise}

\begin{proof}
    Let
    \[
        A = \begin{bmatrix}
            a_{1}  & 0      & \cdots & 0      \\
            0      & a_{2}  & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & a_{n}
        \end{bmatrix}.
    \]

    Then
    \[
        A = \sum^{n}_{i=1}a_{i}\cdot E_{i.i}.
    \]

    The dimension of the space of diagonal $n\times n$ matrices is $n$. A basis for this space is $\{ E_{1.1}, E_{2.2}, \ldots, E_{n.n} \}$.
\end{proof}

\begin{exercise}
    Let $V$ be a subspace of $\mathbf{R}^{2}$. What are the possible dimensions for $V$?
\end{exercise}

\begin{proof}
    $\dim V \le 2$.
    \begin{itemize}
        \item $\dim V = 0$. $V$ is the zero subspace (contains $O$ only).
        \item $\dim V = 1$. Example: $V = \{ (x, 0) \vert x\in\mathbb{R} \}$.
        \item $\dim V = 2$. $V = \mathbf{R}^{2}$.
    \end{itemize}
\end{proof}

\begin{exercise}
    Let $V$ be a subspace of $\mathbf{R}^{3}$. What are the possible dimensions for $V$?
\end{exercise}

\begin{proof}
    $\dim V \le 2$.
    \begin{itemize}
        \item $\dim V = 0$. $V$ is the zero subspace (contains $O$ only).
        \item $\dim V = 1$. Example: $V = \{ (x, 0, 0) \vert x\in\mathbb{R} \}$.
        \item $\dim V = 2$. Example: $V = \{ (x, y, 0) \vert x, y\in\mathbb{R} \}$.
        \item $\dim V = 3$. $V = \mathbf{R}^{3}$.
    \end{itemize}
\end{proof}

\section{Linear Equations}

\begin{equation*}
    \begin{split}
        a_{1.1}x_{1} + \cdots + a_{1.n}x_{n} = 0 \\
        \phantom{a_{2.1}x_{1} + }\cdots          \\
        a_{m.1}x_{1} + \cdots + a_{m.n}x_{n} = 0 \\
    \end{split}
    \tag{(**)}
\end{equation*}

\begin{exercise}
    Let (**) be a system of homogeneous linear equations in a field $K$, and assume that $m = n$. Assume also that the column vectors of coefficients are linearly independent. Show that the only solution is the trivial solution.
\end{exercise}

\begin{proof}
    Assume that (**) has a non-trivial solution $(x_{1}, \ldots, x_{n}) = (y_{1},\ldots y_{n})$. Then
    \[
        y_{1}\begin{bmatrix}
            a_{1.1} \\
            \vdots  \\
            a_{n.1}
        \end{bmatrix}
        + \cdots +
        y_{n}\begin{bmatrix}
            a_{1.n} \\
            \vdots  \\
            a_{n.n}
        \end{bmatrix}
        = \begin{bmatrix}
            0      \\
            \vdots \\
            0
        \end{bmatrix}
    \]

    So the column vectors of coefficients are linearly dependent. But this contradicts the hypothesis that the column vectors of coefficients are linearly independent. So the assumption must be false.

    Hence the only solution of (**) is the trivial solution.
\end{proof}

\begin{exercise}
    Let (**) be a system of homogeneous linear equations in a field $K$, in $n$ unknowns. Show that the set of solutions $X = (x_{1},\ldots,x_{n})$ is a vector space over $K$.
\end{exercise}

\begin{proof}
    The set of solutions is not empty, since it contains $O = (0,\ldots, 0)$. This set is a subset of $K^{n}$.

    Let $X = (x_{1}, \ldots, x_{n})$ and $Y = (y_{1}, \ldots, y_{n})$ be solutions of (**) and $c$ an element of $K$.

    For every $1\le i \le m$
    \[
        \begin{split}
            a_{i.1}x_{1} + \cdots + a_{i.n}x_{n} = 0, \\
            a_{i.1}y_{1} + \cdots + a_{i.n}y_{n} = 0.
        \end{split}
    \]

    So for every $1\le i \le m$
    \[
        \begin{split}
            a_{i.1}(x_{1} + y_{1}) + \cdots + a_{i.n}(x_{n} + y_{n}) = 0, \\
            a_{i.1}(cx_{1}) + \cdots + a_{i.n}(cx_{n}) = 0.
        \end{split}
    \]

    Hence $(X + Y)$ and $cX$ are also solutions of (**). Thus the set of solutions of (**) is a vector space over $K$.
\end{proof}

\begin{exercise}
    Let $A^{1}, \ldots, A^{n}$ be column vectors of size $m$. Assume that they have coefficients in $\mathbf{R}$, and that they are linearly independent over $\mathbf{R}$. Show that they are linearly independent over $\mathbf{C}$.
\end{exercise}

\begin{proof}
    Let $z_{1} = x_{1} + iy_{1}, \ldots, z_{n} = x_{n} + iy_{n}$ be complex numbers which are not all zero such that $z_{1}A^{1} + \cdots + z_{n}A^{n} = 0$. Equivalently
    \[
        ( x_{1}A^{1} + \cdots + x_{n}A^{n} ) + i( y_{1}A^{1} + \cdots + y_{n}A^{n} ) = O.
    \]

    On the other hand, the coefficients of $A^{1}, \ldots, A^{n}$ are real numbers. Therefore
    \[
        \begin{split}
            x_{1}A^{1} + \cdots + x_{n}A^{n} = O, \\
            y_{1}A^{1} + \cdots + y_{n}A^{n} = O.
        \end{split}
    \]

    Since $A^{1}, \ldots, A^{n}$ are linearly independent over $\mathbf{R}$, we obtain that $x_{1} = \cdots = x_{n} = 0, y_{1} = \cdots = y_{n} = 0$. So $z_{1} = \cdots = z_{n} = 0$. Thus, $A^{1}, \ldots, A^{n}$ are linearly independent over $\mathbf{C}$.
\end{proof}

\begin{exercise}
    Let (**) be a system of homogeneous linear equations with coefficients in $\mathbf{R}$. If this system has a non-trivial solution in $\mathbf{C}$, show that it has a non-trivial solution in $\mathbf{R}$.
\end{exercise}

\begin{proof}
    Let $(z_{1}, \ldots, z_{n})$ be a non-trivial solution of (**) in $\mathbf{C}$, and $z_{k} = x_{k} + iy_{k}$ for every $1\le k\le n$. Let $A^{1}, \ldots, A^{n}$ be the column vectors of coefficients of (**). Since $z_{1}A^{1} + \cdots + z_{n}A^{n} = O$, we obtain
    \[
        (x_{1}A^{1} + \cdots + x_{n}A^{n}) + i(y_{1}A^{1} + \cdots + y_{n}A^{n}) = O
    \]

    The coefficients of two matrices $x_{1}A^{1} + \cdots + x_{n}A^{n}$ and $y_{1}A^{1} + \cdots + y_{n}A^{n}$ are real numbers. Therefore
    \[
        \begin{split}
            x_{1}A^{1} + \cdots + x_{n}A^{n} = O, \\
            y_{1}A^{1} + \cdots + y_{n}A^{n} = O.
        \end{split}
    \]

    Since $(z_{1},\ldots, z_{n})\ne O$, then at least one vector amongs $(x_{1},\ldots, x_{n}), (y_{1}, \ldots, y_{n})$ is non-zero. Hence (**) has a non-trivial solution in $\mathbf{R}$.
\end{proof}

\section{Multiplication of Matrices}
\setcounter{exercise}{0}

\begin{exercise}
    Let $I$ be the identity $n\times n$ matrix. Let $A$ be an $n\times r$ matrix. What is $IA$? If $A$ is an $m\times n$ matrix, what is $AI$?
\end{exercise}

\begin{proof}
    Denote by $e_{i.j}$ the $ij$-entry of $I$. Then $e_{i.j} = 1$ if $i = j$, $e_{i.j} = 0$ otherwise.

    When $A$ is an $n\times r$ matrix, $IA$ is also an $n\times r$ matrix. For every $1\le i\le n, 1\le j\le r$, the $ij$-entry of $IA$ is
    \[
        \sum^{n}_{k=1}e_{i.k}a_{k.j} = a_{i.j}
    \]

    So $IA = A$.

    When $A$ is an $m\times n$ matrix, $AI$ is also an $m\times n$ matrix. For every $1\le i\le m, 1\le j\le n$, the $ij$-entry of $AI$ is
    \[
        \sum^{n}_{k=1}a_{i.k}e_{k.j} = a_{i.j}
    \]

    So $AI = A$.
\end{proof}

\begin{exercise}
    Let $O$ be the matrix all of whose coordinates are $0$. Let $A$ be a matrix of a size such that the product $AO$ is defined. What is $AO$?
\end{exercise}

\begin{proof}
    Let $O$ be the matrix of size $m\times n$. To define $AO$, let $A$ be the matrix of size $p\times m$, for some positive integer $p$. Then $AO$ is of size $p\times n$. Every entry of $AO$ is $0$. Hence $AO$ is the $p\times n$ zero matrix.
\end{proof}

\begin{exercise}
    In each of the following cases, find $(AB)C$ and $A(BC)$
    \begin{enumerate}[label={(\alph*)}]
        \item $A = \begin{bmatrix}2 & 1 \\ 3 & 1\end{bmatrix}, B = \begin{bmatrix}-1 & 1 \\ 1 & 0\end{bmatrix}, C = \begin{bmatrix}1 & 4 \\ 2 & 3\end{bmatrix}$
        \item $A = \begin{bmatrix}2 & 1 & -1 \\ 3 & 1 & 2\end{bmatrix}, B = \begin{bmatrix}1 & 1 \\ 2 & 0 \\ 3 & -1\end{bmatrix}, C = \begin{bmatrix}1 \\ 3\end{bmatrix}$
        \item $A = \begin{bmatrix}2 & 4 & 1 \\ 3 & 0 & -1\end{bmatrix}, B = \begin{bmatrix}1 & 1 & 0 \\ 2 & 1 & -1 \\ 3 & 1 & 5\end{bmatrix}, C = \begin{bmatrix}1 & 2 \\ 3 & 1 \\ -1 & 4\end{bmatrix}$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $AB = \begin{bmatrix}-1 & 2 \\ -2 & 3\end{bmatrix}, (AB)C = \begin{bmatrix}3 & 2 \\ 4 & 1\end{bmatrix}$.

              $BC = \begin{bmatrix}1 & -1 \\ 1 & 4\end{bmatrix}, A(BC) = \begin{bmatrix}3 & 2 \\ 4 & 1\end{bmatrix}$.
        \item $AB = \begin{bmatrix}1 & 3 \\ 11 & 1\end{bmatrix}, (AB)C = \begin{bmatrix}10 \\ 14\end{bmatrix}$.

              $BC = \begin{bmatrix}4 \\ 2 \\ 0\end{bmatrix}, A(BC) = \begin{bmatrix}10 \\ 14\end{bmatrix}$.
        \item $AB = \begin{bmatrix}13 & 7 & 1 \\ 0 & 2 & -5\end{bmatrix}, (AB)C = \begin{bmatrix}33 & 37 \\ 11 & -18\end{bmatrix}$.

              $BC = \begin{bmatrix}4 & 3 \\ 6 & 1 \\ 1 & 27\end{bmatrix}, A(BC) = \begin{bmatrix}33 & 37 \\ 11 & -18\end{bmatrix}$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $A, B$ be square matrices of the same size, and assume that $AB = BA$. Show that ${(A + B)}^{2} = A^{2} + 2AB + B^{2}$, and $(A + B)(A - B) = A^{2} - B^{2}$ using the properties of matrices stated in Theorem 3.1.
\end{exercise}

\begin{proof}
    \begin{align*}
        {(A + B)}^{2} & = (A + B)(A + B)          \\
                      & = (A + B)A + (A + B)B     \\
                      & = A^{2} + BA + AB + B^{2} \\
                      & = A^{2} + 2AB + B^{2}
    \end{align*}
    \begin{align*}
        (A + B)(A - B) & = (A + B)A - (A + B)B       \\
                       & = A^{2} + BA - (AB + B^{2}) \\
                       & = A^{2} + BA - AB - B^{2}   \\
                       & = A^{2} - B^{2}
    \end{align*}
\end{proof}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            1 & 2  \\
            3 & -1
        \end{bmatrix},\qquad
        B = \begin{bmatrix}
            2 & 0 \\
            1 & 1
        \end{bmatrix}.
    \]

    Find $AB$ and $BA$.
\end{exercise}

\begin{proof}
    \[
        AB = \begin{bmatrix}
            4 & 2  \\
            5 & -1
        \end{bmatrix},\qquad
        BA = \begin{bmatrix}
            2 & 4 \\
            4 & 1
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Let
    \[
        C = \begin{bmatrix}
            7 & 0 \\
            0 & 7
        \end{bmatrix}.
    \]

    Let $A, B$ be as in Exercise 5. Find $CA, AC, CB$, and $BC$. State the general rule including this exercise as a special case.
\end{exercise}

\begin{proof}
    \[
        CA = \begin{bmatrix}
            7  & 14 \\
            21 & -7
        \end{bmatrix},\qquad
        AC = \begin{bmatrix}
            7  & 14 \\
            21 & -7
        \end{bmatrix}.
    \]
    \[
        CB = \begin{bmatrix}
            14 & 0 \\
            7  & 7
        \end{bmatrix},\qquad
        BC = \begin{bmatrix}
            14 & 0 \\
            7  & 7
        \end{bmatrix}.
    \]

    The general rule: Multiplication of a square matrix whose entries on the diagonal are equal to $c$ and other entries are equal to $0$ and a square matrix $A$ of the same size is commutative, the $ij$-entry of the product is the $ij$-entry of $A$ times $c$.
\end{proof}

\begin{exercise}
    Let $X = (1, 0, 0)$ and let
    \[
        A = \begin{bmatrix}
            3 & 1 & 5 \\
            2 & 0 & 1 \\
            1 & 1 & 7
        \end{bmatrix}.
    \]

    What is $XA$?
\end{exercise}

\begin{proof}
    $XA = (3, 1, 5)$.
\end{proof}

\begin{exercise}
    Let $X = (0, 1, 0)$, and let $A$ be an arbitrary $3\times 3$ matrix. How would you describe $XA$? What if $X = (0, 0, 1)$? Generalize to similar statements concerning $n\times n$ matrices, and their products with unit vectors.
\end{exercise}

\begin{proof}
    If $X = (0, 1, 0)$, then $XA$ is the 2nd row of $A$. If $X = (0, 0, 1)$, then $XA$ is the 3rd row of $A$.

    The general statement: product of an unit vector whose $i$-the component is $1$ and a matrix $A$ (such that the product is defined) is the $i$-th row vector of $A$.
\end{proof}

\begin{exercise}
    (Simplified) If the product are defined, prove that $\prescript{t}{}{(AB)} = \prescript{t}{}{B}\prescript{t}{}{A}$ and $\prescript{t}{}{(ABC)} = \prescript{t}{}{C}\prescript{t}{}{B}\prescript{t}{}{A}$.
\end{exercise}

\begin{proof}
    Let $A\in\text{Mat}_{m\times n}(K), B\in\text{Mat}_{n\times p}(K), C\in\text{Mat}_{p\times q}(K)$.

    $\prescript{t}{}{(AB)}$ is a $p\times m$ matrix, $\prescript{t}{}{B}\prescript{t}{}{A}$ is a $p\times m$ matrix.

    According to the definition of tranposition, the $ij$-entry of $\prescript{t}{}{(AB)}$ is equal to the $ji$-entry of $AB$. The $ji$-entry of $AB$ is equal to
    \[
        \sum^{n}_{r=1}a_{j.r}b_{r.i}.
    \]

    The $ji$-entry of $\prescript{t}{}{B}\prescript{t}{}{A}$ is equal to
    \[
        \sum^{n}_{r=1}b_{r.i}a_{j.r}.
    \]

    Hence $\prescript{t}{}{(AB)} = \prescript{t}{}{B}\prescript{t}{}{A}$.
    \[
        \prescript{t}{}{(ABC)} = \prescript{t}{}{C}\prescript{t}{}{(AB)} = \prescript{t}{}{C}(\prescript{t}{}{B}\prescript{t}{}{A}) = \prescript{t}{}{C}\prescript{t}{}{B}\prescript{t}{}{A}.
    \]
\end{proof}

\begin{exercise}
    Let $M$ be an $n\times n$ matrix such that $\prescript{t}{}{M} = M$. Given two row vectors in $n$-space, say $A$ and $B$ define $\anglebracket{A, B}$ to be $AM\prescript{t}{}{B}$. (Identify a $1\times 1$ matrix with a number.) Show that the conditions of a scalar product are satisfied, except possibly the condition concerning positivity. Give an example of a matrix $M$ and vectors $A, B$ such that $AM\prescript{t}{}{B}$ is negative (taking $n = 2$).
\end{exercise}

\begin{proof}
    Since $\anglebracket{A, B}$ is a number, then $\prescript{t}{}{\anglebracket{A, B}} = \anglebracket{A, B}$.

    According to Exercise 9,
    \begin{align*}
        \anglebracket{A, B} & = \prescript{t}{}{\anglebracket{A, B}}                                    \\
                            & = \prescript{t}{}{AM\prescript{t}{}{B}}                                   \\
                            & = \prescript{t}{}{\prescript{t}{}{B}}\prescript{t}{}{M}\prescript{t}{}{A} \\
                            & = BM\prescript{t}{}{A}                                                    \\
                            & = \anglebracket{B, A}.
    \end{align*}

    So $\anglebracket{\cdot, \cdot}$ is symmetric.

    \begin{align*}
        \anglebracket{xA + B, C} & = (xA + B)M\prescript{t}{}{C}                    \\
                                 & = (xA)M\prescript{t}{}{C} + BM\prescript{t}{}{C} \\
                                 & = x(AM\prescript{t}{}{C}) + BM\prescript{t}{}{C} \\
                                 & = x\anglebracket{A, C} + \anglebracket{B, C}.
    \end{align*}

    So $\anglebracket{\cdot, \cdot}$ is bilinear.

    However, $\anglebracket{\cdot, \cdot}$ is not necessarily positive-definite. Example:
    \[
        A = \begin{bmatrix}1 & 0\end{bmatrix}, B = \begin{bmatrix}1 & 0\end{bmatrix}, M = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}.
    \]

    We obtain
    \[
        AM\prescript{t}{}{B} = -1 < 0.
    \]
\end{proof}

\begin{exercise}
    \begin{enumerate}[label={(\alph*)}]
        \item Let $A$ be the matrix
              \[
                  \begin{bmatrix}
                      0 & 1 & 1 \\
                      0 & 0 & 1 \\
                      0 & 0 & 0
                  \end{bmatrix}.
              \]

              Find $A^{2}, A^{3}$. Generalize to $4\times 4$ matrices.
        \item Let $A$ be the matrix
              \[
                  \begin{bmatrix}
                      1 & 1 & 1 \\
                      0 & 1 & 1 \\
                      0 & 0 & 1
                  \end{bmatrix}.
              \]

              Compute $A^{2}, A^{3}, A^{4}$.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item
              \[
                  A^{2} = \begin{bmatrix}
                      0 & 0 & 1 \\
                      0 & 0 & 0 \\
                      0 & 0 & 0
                  \end{bmatrix},\qquad
                  A^{3} = \begin{bmatrix}
                      0 & 0 & 0 \\
                      0 & 0 & 0 \\
                      0 & 0 & 0
                  \end{bmatrix}.
              \]
        \item
              \[
                  A^{2} = \begin{bmatrix}
                      1 & 2 & 3 \\
                      0 & 1 & 2 \\
                      0 & 0 & 1
                  \end{bmatrix},\qquad
                  A^{3} = \begin{bmatrix}
                      1 & 3 & 6 \\
                      0 & 1 & 3 \\
                      0 & 0 & 1
                  \end{bmatrix},\qquad
                  A^{4} = \begin{bmatrix}
                      1 & 4 & 10 \\
                      0 & 1 & 4  \\
                      0 & 0 & 1
                  \end{bmatrix}.
              \]
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $X$ be the indicated column vector, and $A$ the indicated matrix. Find $AX$ as a column vector.
    \begin{enumerate}[label={(\alph*)}]
        \item $X = \begin{bmatrix}3 \\ 2 \\ 1\end{bmatrix}, A = \begin{bmatrix}1 & 0 & 1 \\ 2 & 0 & 1 \\ 2 & 0 & -1\end{bmatrix}$
        \item $X = \begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix}, A = \begin{bmatrix}2 & 1 & 5 \\ 0 & 1 & 1\end{bmatrix}$
        \item $X = \begin{bmatrix}x_{1} \\ x_{2} \\ x_{3}\end{bmatrix}, A = \begin{bmatrix}0 & 1 & 0 \\ 0 & 0 & 0\end{bmatrix}$
        \item $X = \begin{bmatrix}x_{1} \\ x_{2} \\ x_{3}\end{bmatrix}, A = \begin{bmatrix}0 & 0 & 0 \\ 1 & 0 & 0\end{bmatrix}$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $AX = \begin{bmatrix}4 \\ 7 \\ 5\end{bmatrix}$
        \item $AX = \begin{bmatrix}3 \\ 1\end{bmatrix}$
        \item $AX = \begin{bmatrix}x_{2} \\ 0\end{bmatrix}$
        \item $AX = \begin{bmatrix}0 \\ x_{1}\end{bmatrix}$
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            2 & 1 & 3 \\
            4 & 1 & 5
        \end{bmatrix}.
    \]

    Find $AX$ for each of the following values of $X$.
    \begin{enumerate}[label={(\alph*)}]
        \item $X = \begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}$
        \item $X = \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}$
        \item $X = \begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $AX = \begin{bmatrix}2 \\ 4\end{bmatrix}$
        \item $AX = \begin{bmatrix}4 \\ 6\end{bmatrix}$
        \item $AX = \begin{bmatrix}3 \\ 5\end{bmatrix}$
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            3 & 7  & 5 \\
            1 & -1 & 4 \\
            2 & 1  & 8
        \end{bmatrix}.
    \]

    Find $AX$ for each of the values of $X$ given in Exercise 13.
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item $AX = \begin{bmatrix}3 \\ 1 \\ 2\end{bmatrix}$
        \item $AX = \begin{bmatrix}12 \\ 3 \\ 9\end{bmatrix}$
        \item $AX = \begin{bmatrix}5 \\ 4 \\ 8\end{bmatrix}$
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let
    \[
        X = \begin{bmatrix}0 \\ 1 \\ 0 \\ 0\end{bmatrix}
        \quad\text{and}\quad
        A = \begin{bmatrix}
            a_{1.1} & \cdots & a_{1.4} \\
            \vdots  &        & \vdots  \\
            a_{m.1} & \cdots & a_{m.4}
        \end{bmatrix}.
    \]

    What is $AX$?
\end{exercise}

\begin{proof}
    $AX$ is the 2nd column of $A$.
\end{proof}

\begin{exercise}
    Let $X$ be a column vector having all its components equal to $0$ except the $i$-th component which is equal to $1$. Let $A$ be an arbitrary matrix, whose size is such that we can form the product $AX$. What is $AX$?
\end{exercise}

\begin{proof}
    $AX$ is the $i$-th column of $A$.
\end{proof}

\begin{exercise}
    Let $A = {(a_{i.j})}$, $i = 1,\ldots, m$ and $j = 1,\ldots, n$, be an $m\times n$ matrix. Let $B = {(b_{j.k})}$, $j = 1,\ldots, n$ and $k = 1,\ldots, s$, be an $n\times s$ matrix. Let $AB = C$. Show that the $k$-th column $C^{k}$ can be written
    \[
        C^{k} = b_{1.k}A^{1} + \cdots + b_{n.k}A^{n}.
    \]

    (This will be useful in finding the determinant of a product.)
\end{exercise}

\begin{proof}
    $C$ is an $m\times s$ matrix.

    For every $1\le r\le n$, the $r.k$-entry of $C$ is equal to
    \[
        \sum^{n}_{j=1}a_{r.j}b_{j.k} = b_{1.k}a_{r.1} + \cdots + b_{n.k}a_{r.n}.
    \]

    So
    \[
        C^{k} = b_{1.k}A^{1} + \cdots + b_{n.k}A^{n}.
    \]
\end{proof}

\begin{exercise}
    Let $A$ be a square matrix.
    \begin{enumerate}[label={(\alph*)}]
        \item If $A^{2} = O$ show that $I - A$ is invertible.
        \item If $A^{3} = O$ show that $I - A$ is invertible.
        \item In general, if $A^{n} = O$ for some positive integer $n$, show that $I - A$ is invertible.
        \item Suppose that $A^{2} + 2A + I = O$. Show that $A$ is invertible.
        \item Suppose that $A^{3} - A + I = O$. Show that $A$ is invertible.
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item
              \[
                  \begin{split}
                      (I - A)(I + A) = I^{2} - AI + IA - A^{2} = I - A + A - O = I, \\
                      (I + A)(I - A) = I^{2} + AI - IA - A^{2} = I + A - A - O = I.
                  \end{split}
              \]

              Hence $I - A$ is invertible.
        \item
              \[
                  \begin{split}
                      (I - A)(I + A + A^{2}) = I(I + A + A^{2}) - A(I + A + A^{2}) = I - A^{3} = I, \\
                      (I + A + A^{2})(I - A) = (I + A + A^{2})I - (I + A + A^{2})A = I - A^{3} = I.
                  \end{split}
              \]

              Hence $I - A$ is invertible.
        \item
              \[
                  \begin{split}
                      (I - A)(I + A + \cdots + A^{n-1}) = I(I + A + \cdots + A^{n-1}) - A(I + A + \cdots + A^{n-1}) = I - A^{n} = I, \\
                      (I + A + \cdots + A^{n-1})(I - A) = (I + A + \cdots + A^{n-1})I - (I + A + \cdots + A^{n-1})A = I - A^{n} = I.
                  \end{split}
              \]

              Hence $I - A$ is invertible.
        \item
              \[
                  \begin{split}
                      A^{2} + 2A + I = AA + 2AI + I = A(A + 2I) + I, \\
                      A^{2} + 2A + I = AA + 2IA + I = (A + 2I)A + I.
                  \end{split}
              \]

              So $A\cdot (-A-2I) = (-A-2I)\cdot A = I$. Hence $A$ is invertible.
        \item
              \[
                  \begin{split}
                      A^{3} - A + I = A^{2}A - IA + I = (A^{2} - I)A + I, \\
                      A^{3} - A + I = AA^{2} - AI + I = A(A^{2} - I) + I.
                  \end{split}
              \]

              So $A(I - A^{2}) = (I - A^{2})A = I$. Hence $A$ is invertible.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $a, b$ be numbers, and let
    \[
        A = \begin{bmatrix}
            1 & a \\
            0 & 1
        \end{bmatrix}
        \quad\text{and}\quad
        B = \begin{bmatrix}
            1 & b \\
            0 & 1
        \end{bmatrix}.
    \]

    What is $AB$? What is $A^{n}$ where $n$ is a positive integer?
\end{exercise}

\begin{proof}
    \[
        AB = \begin{bmatrix}
            1 & b + a \\
            0 & 1
        \end{bmatrix}.
    \]
    \[
        A^{n} = \begin{bmatrix}
            1 & nb \\
            0 & 1
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Show that the matrix $A$ in Exercise 19 has an inverse. What is this inverse?
\end{exercise}

\begin{proof}
    \[
        A^{-1} = \begin{bmatrix}
            1 & -a \\
            0 & 1
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Show that if $A, B$ are $n\times n$ matrices which have inverses, then $AB$ has an inverse.
\end{exercise}

\begin{proof}
    Let $A^{-1}, B^{-1}$ be the inverses of $A, B$, respectively.
    \[
        \begin{split}
            (AB)(B^{-1}A^{-1}) = (A(BB^{-1}))A^{-1} = (AI)A^{-1} = AA^{-1} = I, \\
            (B^{-1}A^{-1})(AB) = (B^{-1}(A^{-1}A))B = (B^{-1}I)B = B^{-1}B = I.
        \end{split}
    \]
\end{proof}

\begin{exercise}
    Determine all $2\times 2$ matrices $A$ such that $A^{2} = O$.
\end{exercise}

\begin{proof}
    Let $A = \begin{bmatrix}a & b \\ c & d\end{bmatrix}$. $A^{2} = \begin{bmatrix}a^{2} + bc & b(a + d) \\ c(a + d) & d^{2} + bc\end{bmatrix}$. $A^{2} = O$ implies at least one of the following
    \begin{itemize}
        \item $b = 0$.

              Then $a^{2} = d^{2} = 0$, and $a = d = 0$.
        \item $c = 0$.

              Then $a^{2} = d^{2} = 0$, and $a = d = 0$.
        \item $a + d = 0$.
    \end{itemize}

    All $2\times 2$ matrices $A$ such that $A^{2} = O$ are of one of the following form
    \[
        \begin{bmatrix}
            0 & 0 \\
            x & 0
        \end{bmatrix},\qquad
        \begin{bmatrix}
            0 & x \\
            0 & 0
        \end{bmatrix},\qquad
        \begin{bmatrix}
            a & b  \\
            c & -a
        \end{bmatrix}
    \]

    where $x$ is an arbitrary element of $K$, and $a^{2} + bc = 0$.
\end{proof}

\begin{exercise}
    Let $A = \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$. Show that $A^{2} = \begin{bmatrix}\cos{2\theta} & -\sin{2\theta} \\ \sin{2\theta} & \cos{2\theta}\end{bmatrix}$.

    Determine $A^{n}$ by induction for any positive integer $n$.
\end{exercise}

\begin{proof}
    \[
        A^{2} = \begin{bmatrix}
            \cos^{2}{\theta} - \sin^{2}{\theta}                 & -\cos{\theta}\sin{\theta} + (-\sin{\theta}\cos{\theta}) \\
            \sin{\theta}\cos{\theta} + \cos{\theta}\sin{\theta} & -\sin^{2}{\theta} + \cos^{2}{\theta}
        \end{bmatrix}
        = \begin{bmatrix}
            \cos{2\theta} & -\sin{2\theta} \\
            \sin{2\theta} & \cos{2\theta}
        \end{bmatrix}.
    \]

    Assume that $A^{n-1} = \begin{bmatrix}\cos{(n-1)\theta} & -\sin{(n-1)\theta} \\ \sin{(n-1)\theta} & \cos{(n-1)\theta}\end{bmatrix}$.
    \begin{align*}
        A^{n} = A^{n-1}A & = \begin{bmatrix}
                                 \cos{(n-1)\theta} & -\sin{(n-1)\theta} \\
                                 \sin{(n-1)\theta} & \cos{(n-1)\theta}
                             \end{bmatrix}
        \begin{bmatrix}
            \cos{\theta} & -\sin{\theta} \\
            \sin{\theta} & \cos{\theta}
        \end{bmatrix}                                                                                                                        \\
                         & = \begin{bmatrix}
                                 \cos{(n-1)\theta}\cos{\theta} - \sin{(n-1)\theta}\sin{\theta} & -\sin{(n-1)\theta}\cos{\theta} - \sin{\theta}\cos{(n-1)\theta} \\
                                 \sin{(n-1)\theta}\cos{\theta} + \sin{\theta}\cos{(n-1)\theta} & -\sin{(n-1)\theta}\sin{\theta} + \cos{(n-1)\theta}\cos{\theta}
                             \end{bmatrix} \\
                         & = \begin{bmatrix}
                                 \cos{n\theta} & -\sin{n\theta} \\
                                 \sin{n\theta} & \cos{n\theta}
                             \end{bmatrix}.
    \end{align*}

    Hence, by the principle of mathematical induction,
    \[
        A^{n} = \begin{bmatrix}
            \cos{n\theta} & -\sin{n\theta} \\
            \sin{n\theta} & \cos{n\theta}
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Find a $2\times 2$ matrix $A$ such that $A^{2} = -I = \begin{bmatrix}-1 & 0 \\ 0 & -1\end{bmatrix}$.
\end{exercise}

\begin{proof}
    From Exercise 23, the following matrix satisfies $A^{2} = -I$.
    \[
        A = \begin{bmatrix}
            0 & -1 \\
            1 & 0
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Let $A$ be an $n\times n$ matrix. Define the \textbf{trace} of $A$ to be the sum of the diagonal elements. Thus if $A = {(a_{i.j})}$, then
    \[
        \tr(A) = \sum^{n}_{i=1} a_{i.i}.
    \]

    For instance, if
    \[
        A = \begin{bmatrix}
            1 & 2 \\
            3 & 4
        \end{bmatrix},
    \]

    then $\tr{A} = 1 + 4 = 5$. If
    \[
        A = \begin{bmatrix}
            1 & -1 & 5 \\
            2 & 1  & 3 \\
            1 & -4 & 7
        \end{bmatrix},
    \]

    then $\tr{A} =9$. Compute the trace of the following matrices:
    \begin{enumerate}[label={(\alph*)}]
        \item $\begin{bmatrix}1 & 7 & 3 \\ -1 & 5 & 2 \\ 2 & 3 & -4\end{bmatrix}$
        \item $\begin{bmatrix}3 & -2 & 4 \\ 1 & 4 & 1 \\ -7 & -3 & -3\end{bmatrix}$
        \item $\begin{bmatrix}-2 & 1 & 1 \\ 3 & 4 & 4 \\ -5 & 2 & 6\end{bmatrix}$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item The trace in this case is equal to $2$.
        \item The trace in this case is equal to $4$.
        \item The trace in this case is equal to $8$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Let $A, B$ be the indicaed matrices. Show that
    \[
        \tr{AB} = \tr{BA}.
    \]
    \begin{enumerate}[label={(\alph*)}]
        \item $A = \begin{bmatrix}1 & -1 & 1 \\ 2 & 4 & 1 \\ 3 & 0 & 1\end{bmatrix}, B = \begin{bmatrix}3 & 1 & 2 \\ 1 & 1 & 0 \\ -1 & 2 & 1\end{bmatrix}$
        \item $A = \begin{bmatrix}1 & 7 & 3 \\ -1 & 5 & 2 \\ 2 & 3 & -4\end{bmatrix}, B = \begin{bmatrix}3 & -2 & 4 \\ 1 & 4 & 1 \\ -7 & -3 & 2\end{bmatrix}$
    \end{enumerate}
\end{exercise}

\begin{proof}
    \begin{enumerate}[label={(\alph*)}]
        \item
              \[
                  AB = \begin{bmatrix}
                      1 & 2 & 3 \\
                      9 & 8 & 5 \\
                      8 & 5 & 7
                  \end{bmatrix},\qquad
                  BA = \begin{bmatrix}
                      11 & 1 & 6 \\
                      3  & 3 & 2 \\
                      6  & 9 & 2
                  \end{bmatrix}
              \]

              $\tr{AB} = \tr{BA} = 16$.
        \item
              \[
                  AB = \begin{bmatrix}
                      -11 & 17 & 17 \\
                      -12 & 16 & 5  \\
                      37  & 20 & 3
                  \end{bmatrix},\qquad
                  BA = \begin{bmatrix}
                      13       & 23  & -11 \\
                      -1       & 30  & 7   \\
                      0  ^ -58 & -35
                  \end{bmatrix}
              \]

              $\tr{AB} = \tr{BA} = 8$.
    \end{enumerate}
\end{proof}

\begin{exercise}
    Prove in general that if $A$ is an $m\times n$ matrix and $B$ is an $n\times m$ matrix, then
    \[
        \tr{AB} = \tr{BA}.
    \]
\end{exercise}

\begin{proof}
    $A = {(a_{i.j})}_{m\times n}$, $B = {(b_{i.j})}_{n\times m}$. According to the definition of matrix multiplication, $AB$ is an $m\times m$ matrix, $BA$ is an $n\times n$ matrix. So $\tr{AB}$ and $\tr{BA}$ are defined.

    $ii$-entry of $AB$ is equal to
    \[
        \sum^{n}_{j=1}a_{i.j}b_{j.i}
    \]

    So
    \[
        \tr{AB} = \sum^{m}_{i=1}\left(\sum^{n}_{j=1}a_{i.j}b_{j.i}\right) = \sum_{\stackrel{1\le i\le m}{1\le j\le n}}a_{i.j}b_{j.i}.
    \]

    $jj$-entry of $BA$ is equal to
    \[
        \sum^{m}_{i=1}b_{j.i}a_{i.j}
    \]

    So
    \[
        \tr{BA} = \sum^{n}_{j=1}\left(\sum^{m}_{i=1}b_{j.i}a_{i.j}\right) = \sum_{\stackrel{1\le i\le m}{1\le j\le n}}a_{i.j}b_{j.i}.
    \]

    Therefore $\tr{AB} = \tr{BA}$.
\end{proof}

\begin{exercise}
    For any square matrix $A$, show that $\tr{A} = \tr{\prescript{t}{}{A}}$.
\end{exercise}

\begin{proof}
    Let
    \[
        A = \begin{bmatrix}
            a_{1.1} & \cdots & a_{1.n} \\
            \vdots  &        & \vdots  \\
            a_{n.1} & \cdots & a_{n.n}
        \end{bmatrix}
    \]

    then
    \[
        \prescript{t}{}{A} = \begin{bmatrix}
            a_{1.1} & \cdots & a_{n.1} \\
            \vdots  &        & \vdots  \\
            a_{1.n} & \cdots & a_{n.n}
        \end{bmatrix}.
    \]

    So $\tr{A} = \sum^{n}_{i=1} a_{i.i} = \tr{\prescript{t}{}{A}}$.
\end{proof}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 2 & 0 \\
            0 & 0 & 3
        \end{bmatrix}.
    \]

    Find $A^{2}, A^{3}, A^{4}$.
\end{exercise}

\begin{proof}
    \[
        A^{2} = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 4 & 0 \\
            0 & 0 & 9
        \end{bmatrix},\quad
        A^{3} = \begin{bmatrix}
            1 & 0 & 0  \\
            0 & 8 & 0  \\
            0 & 0 & 27
        \end{bmatrix},\quad
        A^{4} = \begin{bmatrix}
            1 & 0  & 0  \\
            0 & 16 & 0  \\
            0 & 0  & 81
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Let $A$ be a diagonal matrix, with diagonal elements $a_{1}, \ldots, a_{n}$. What is $A^{2}, A^{3}, A^{k}$ for any positive integer $k$.
\end{exercise}

\begin{proof}
    $A^{2}$ is a diagonal matrix, with diagonal elements ${a_{1}}^{2}, \ldots, {a_{n}}^{2}$.

    $A^{3}$ is a diagonal matrix, with diagonal elements ${a_{1}}^{3}, \ldots, {a_{n}}^{3}$.

    Assume that $A^{m-1}$ is a diagonal matrix, with diagonal elements ${a_{1}}^{m-1}, \ldots, {a_{n}}^{m-1}$. Then $A^{m} = A^{m-1}A$, is a diagonal matrix, with diagonal elements ${a_{1}}^{m}, \ldots, {a_{n}}^{m}$.

    Hence, due to the principle of mathematical induction, $A^{k}$ is a diagonal matrix, with diagonal elements ${a_{1}}^{k}, \ldots, {a_{n}}^{k}$.
\end{proof}

\begin{exercise}
    Let
    \[
        A = \begin{bmatrix}
            0 & 1 & 6 \\
            0 & 0 & 4 \\
            0 & 0 & 0
        \end{bmatrix}.
    \]

    Find $A^{3}$.
\end{exercise}

\begin{proof}
    \[
        A^{2} = \begin{bmatrix}
            0 & 0 & 4 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix},\quad
        A^{3} = \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Let $A$ be an invertible $n\times n$ matrix. Show that
    \[
        \prescript{t}{}{{(A^{-1})}} = {(\prescript{t}{}{A})}^{-1}.
    \]

    We many therefore write $\prescript{t}{}{A}^{-1}$ without fear of confusion.
\end{exercise}

\begin{proof}
    $AA^{-1} = A^{-1}A = I_{n}$.

    According to Exercise 9
    \[
        \begin{split}
            \prescript{t}{}{(A^{-1})}\prescript{t}{}{A} = \prescript{t}{}{AA^{-1}} = \prescript{t}{}{I_{n}} = I_{n} \\
            \prescript{t}{}{A}\prescript{t}{}{(A^{-1})} = \prescript{t}{}{A^{-1}A} = \prescript{t}{}{I_{n}} = I_{n}
        \end{split}
    \]

    Hence $\prescript{t}{}{A}$ is invertible, and ${(\prescript{t}{}{A})}^{-1} = \prescript{t}{}{(A^{-1})}$.
\end{proof}

\begin{exercise}
    Let $A$ be a complex matrix, $A = {(a_{i.j})}$, and let $\bar{A} = {(\bar{a}_{i.j})}$, where the bar means complex conjugate. Show that
    \[
        \prescript{t}{}{(\bar{A})} = \overline{\prescript{t}{}{A}}.
    \]

    We then write simply $\prescript{t}{}{\bar{A}}$.
\end{exercise}

\begin{proof}
    Let
    \[
        A = \begin{bmatrix}
            a_{1.1} & \cdots & a_{1.n} \\
            \vdots  &        & \vdots  \\
            a_{m.1} & \cdots & a_{m.m}
        \end{bmatrix}.
    \]

    Then $\prescript{t}{}{(\bar{A})}$ and $\overline{\prescript{t}{}{A}}$ are matrices of size $n\times m$.

    The $ij$-entry of $\prescript{t}{}{\bar{A}}$ is the $ji$-entry of $\bar{A}$, and is equal to $\overline{a_{j.i}}$.

    The $ij$-entry of $\overline{\prescript{t}{}{A}}$ is the conjugate of the $ji$-entry of $A$, and is equal to $\overline{a_{j.i}}$.

    Hence $\prescript{t}{}{(\bar{A})} = \overline{\prescript{t}{}{A}}$.
\end{proof}

\begin{exercise}
    Let $A$ be a diagonal matrix:
    \[
        A = \begin{bmatrix}
            a_{1}  & 0      & \cdots & 0      \\
            0      & a_{2}  & \cdots & 0      \\
            \vdots & \vdots &        & \vdots \\
            0      & 0      & \cdots & a_{n}
        \end{bmatrix}.
    \]

    If $a_{i}\ne 0$ for all $i$, show that $A$ is invertible. What is its inverse?
\end{exercise}

\begin{proof}
    We find a square $n\times n$ matrix $X = (x_{i.j})$ such that $XA = AX = I_{n}$. In terms of coordinates, for every $1\le i,j,k\le n$
    \[
        \begin{split}
            a_{i}\cdot x_{i.j} = \delta_{i.j}
        \end{split}
    \]

    Hence $x_{i.i} = {a_{i}}^{-1}$ and $x_{i.j} = 0$ when $i\ne j$. Therefore, $A$ is invertible, and its inverse is
    \[
        A^{-1} = \begin{bmatrix}
            {a_{1}}^{-1} & 0            & \cdots & 0            \\
            0            & {a_{2}}^{-1} & \cdots & 0            \\
            \vdots       & \vdots       &        & \vdots       \\
            0            & 0            & \cdots & {a_{n}}^{-1}
        \end{bmatrix}.
    \]
\end{proof}

\begin{exercise}
    Let $A$ be a \textbf{strictly upper triangular matrix}, i.e.\@ a square matrix $(a_{i.j})$ having all its components below and on the diagonal equal to $0$. We many express this by writing $a_{i.j} = 0$ if $i\geq j$:
    \[
        A = \begin{bmatrix}
            0      & a_{1.2} & a_{1.3} & \cdots & a_{1.n}     \\
            0      & 0       & a_{2.3} & \cdots & a_{2.n}     \\
            \vdots & \vdots  & \vdots  &        & \vdots      \\
            0      & 0       & 0       & \cdots & a_{(n-1).n} \\
            0      & 0       & 0       & \cdots & 0
        \end{bmatrix}.
    \]

    Prove that $A^{n} = O$.
\end{exercise}

\begin{proof}
    \textbf{Definition.} A square matrix $A$ is called $k$-upper triangle matrix if $a_{i.j} = 0$ when $i - j\ge -k$, where $k$ is a non-negative integer.

    When $k = 0$, we obtain \textbf{strictly upper triangular matrix}.

    We prove the following statement: Product of a $k$-upper triangular matrix $X$ and a $0$-upper triangular matrix $Y$ is a $(k+1)$-upper triangular matrix.

    Let $X = (x_{i.j})$ and $Y = (y_{i.j})$ be $n\times n$ matrix, $X$ is $k$-upper triangular, $Y$ is $0$-upper triangular. The $i.j$-entry of $XY$ is
    \[
        \sum^{n}_{r=1}x_{i.r}y_{r.j}.
    \]

    If $i - j\geq -(k + 1)$
    \[
        \sum^{n}_{r=1}x_{i.r}y_{r.j} = \sum^{i+k}_{r=1}x_{i.r}y_{r.j} + \sum^{n}_{r=i+k+1}x_{i.r}y_{r.j} = 0 + 0 = 0
    \]

    $\displaystyle\sum^{i+k}_{r=1}x_{i.r}y_{r.j} = 0$ because $i - r \geq i - (i + k) = -k$; $\displaystyle\sum^{n}_{r=i+k+1}x_{i.r}y_{r.j} = 0$ because $r - j \geq i + k + 1 - j \geq 0$.

    Hence $XY$ is a $(k+1)$-upper triangular matrix. So $A^{n}$ is a $(n-1)$-upper triangular matrix. On the other hand, for every $1\leq i, j\leq n$, the inequality $i - j \geq 1 - n$ is always true, so $A^{n} = O$.
\end{proof}

\begin{exercise}
    Let $A$ be a triangular matrix with components $1$ on the diagonal:
    \[
        A = \begin{bmatrix}
            1      & a_{1.2} & \cdots & a_{1.(n-1)} & a_{1.n}     \\
            0      & 1       & \cdots & a_{2.(n-1)} & a_{2.n}     \\
            \vdots & \vdots  &        & \vdots      & \vdots      \\
            0      & 0       & \cdots & 1           & a_{(n-1).n} \\
            0      & 0       & \cdots & 0           & 1
        \end{bmatrix}.
    \]

    Let $N = A - I_{n}$. Show that $N^{n+1} = O$. Note that $A = I + N$. Show that $A$ is invertible, and that its inverse is
    \[
        {(I + N)}^{-1} = I - N + N^{2} - \cdots + {(-1)}^{n}N^{n}.
    \]
\end{exercise}

\begin{proof}
    According to Exercise 35, $N^{n} = O$, so $N^{n+1} = O$.
    \begin{align*}
        (I + N)\sum^{n}_{k=0}{(-1)}^{k}N^{k}              & = \sum^{n}_{k=0}{(-1)}^{k}N^{k} + \sum^{n}_{k=0}{{(-1)}^{k}}N^{k+1}                           \\
                                                          & = I + \sum^{n}_{k=1}{(-1)}^{k}N^{k} + \sum^{n-1}_{k=0}{(-1)}^{k+1}N^{k+1} + {(-1)}^{n}N^{n+1} \\
                                                          & = I + {(-1)}^{n}N^{n+1}                                                                       \\
                                                          & = I,                                                                                          \\
        \left(\sum^{n}_{k=0}{(-1)}^{k}N^{k}\right)(I + N) & = \sum^{n}_{k=0}{(-1)}^{k}N^{k} + \sum^{n}_{k=0}{(-1)}^{k}N^{k+1}                             \\
                                                          & = I + \sum^{n}_{k=1}{(-1)}^{k}N^{k} + \sum^{n}_{k=1}{(-1)}^{k-1}N^{k} + {(-1)}^{n}N^{n+1}     \\
                                                          & = I.
    \end{align*}

    Hence $A$ is invertible and its inverse is
    \[
        {(I + N)}^{-1} = \sum^{n}_{k=0}{(-1)}^{k}N^{k}.
    \]
\end{proof}

\begin{exercise}
    If $N$ is a square matrix such that $N^{r+1} = O$ for some positive integer $r$, show that $I - N$ is invertible and that its inverse is $I + N + \cdots + N^{r}$.
\end{exercise}

\begin{proof}
    \begin{align*}
        (I - N)\sum^{r}_{k=0}N^{k}              & = \sum^{r}_{k=0}N^{k} - \sum^{r}_{k=0}N^{k+1}                 \\
                                                & = I + \sum^{r}_{k=1}N^{k} - \sum^{r}_{k=1}N^{k} - N^{r+1}     \\
                                                & = I - N^{r+1}                                                 \\
                                                & = I,                                                          \\
        \left(\sum^{r}_{k=0}N^{k}\right)(I - N) & = \sum^{r}_{k=0}N^{k} - \sum^{r}_{k=0}N^{k+1}                 \\
                                                & = I + \sum^{r}_{k=1}N^{k} - \sum^{r-1}_{k=0}N^{k+1} - N^{r+1} \\
                                                & = I - N^{r+1}                                                 \\
                                                & = I.
    \end{align*}

    Hence $I - N$ is invertible and its inverse is $I + N + \cdots + N^{r}$.
\end{proof}

\begin{exercise}
    Let $A$ be a triangular matrix:
    \[
        A = \begin{bmatrix}
            a_{1.1} & a_{1.2} & \cdots & a_{1.n} \\
            0       & a_{2.2} & \cdots & a_{2.n} \\
            \vdots  & \vdots  &        & \vdots  \\
            0       & 0       & \cdots & a_{n.n}
        \end{bmatrix}.
    \]

    Assume that no diagonal element is $0$, and let
    \[
        B = \begin{bmatrix}
            {a_{1.1}}^{-1} & 0              & \cdots & 0              \\
            0              & {a_{2.2}}^{-1} & \cdots & 0              \\
            \vdots         & \vdots         &        & \vdots         \\
            0              & 0              & \cdots & {a_{n.n}}^{-1}
        \end{bmatrix}
    \]

    Show that $BA$ and $AB$ are triangular matrices with components $1$ on the diagonal.
\end{exercise}

\begin{proof}
    The $i.j$-entry of $BA$ is equal to
    \[
        \sum^{n}_{r=1}b_{i.r}a_{r.j} = b_{i.i}a_{i.j} = {a_{i.i}}^{-1}a_{i.j}.
    \]

    If $i = j$, the $i.j$-entry is equal to $1$. If $i > j$, the $i.j$-entry is equal to $0$. Hence $BA$ is an upper triangular matrix with components $1$ on the diagonal.

    The $i.j$-entry of $AB$ is equal to
    \[
        \sum^{n}_{r=1}a_{i.r}b_{r.j} = a_{i.j}b_{j.j} = a_{i.j}{a_{j.j}}^{-1}.
    \]

    If $i = j$, the $i.j$-entry is equal to $1$. If $i > j$, the $i.j$-entry is equal to $0$. Hence $AB$ is an upper triangular matrix with components $1$ on the diagonal.
\end{proof}

\begin{exercise}
    A square matrix $A$ is said to be \textbf{nilpotent} if $A^{r} = O$ for some integer $r\geq 1$. Let $A, B$ be nilpotent matrices, of the same size, and assume $AB = BA$. Show that $AB$ and $A + B$ are nilpotent.
\end{exercise}

\begin{proof}
    Since $A, B$ are nilpotent, there exists positive integer $r, s$ such that $A^{r} = O$ and $B^{s} = O$.

    Let $m = \max\{ r, s \}$. Since $AB = BA$, then ${(AB)}^{m} = (AB)\cdots (AB) = A^{m}B^{m} = O$. So $AB$ is nilpotent.

    Let $n = 2(r + s)$. Since $AB = BA$, the binomial expansion holds
    \[
        {(A + B)}^{n} = \sum^{n}\binom{n}{k}A^{n-k}B^{k}.
    \]

    Since $(n - k) + k = n$, then $\max\{ n - k, k \} \geq \frac{n}{2} = r + s > r, s$. So ${(A + B)}^{n} = O$, which means $A + B$ is nilpotent.
\end{proof}
