% chktex-file 1
% chktex-file 8
% chktex-file 36
\chapter{Curves}

\section{Parametrized curves}

\begin{exercise}{1.1}
	An interval means a nonempty connected subset of $\mathbb{R}$. Prove that every interval has one of the following forms:
	\begin{equation*}
		\openinterval{a, b}, \closedinterval{a, b}, \halfopenleft{a, b}, \halfopenright{a, b}, \openinterval{-\infty, b}, \halfopenleft{-\infty, b}, \openinterval{a, \infty}, \halfopenright{a, \infty}, \openinterval{-\infty, \infty}
	\end{equation*}
\end{exercise}

\begin{proof}
	Let $S$ be an interval of $\mathbb{R}$. If $S$ is a singleton then $S$ has the form $\closedinterval{a, b}$. Assume $S$ has at least two elements.

	Let $x, y$ be two elements of $S$ such that $x < y$ and $c \in \openinterval{x, y}$. If $c \notin S$ then $S$ is disconnected by $S \cap \openinterval{-\infty, c}$ and $S \cap \openinterval{c, \infty}$, which is a contradiction. Therefore $S$ contains $\closedinterval{x, y}$ whenever $x, y\in S$.

	The following cases are exhaustive.
	\begin{enumerate}[label={\textbf{Case \arabic*.}}, itemindent=1cm, itemsep=0pt]
		\item $S$ is bounded, $S$ has no minimum or maximum.

		      Let $a = \inf S, b = \sup S$ then $S = \openinterval{a, b}$.
		\item $S$ is bounded, $S$ has a minimum and a maximum.

		      Let $a = \min S, b = \max S$ then $S = \closedinterval{a, b}$.
		\item $S$ is bounded, $S$ has a minimum but no maximum.

		      Let $a = \min S, b = \sup S$ then $S = \halfopenright{a, b}$.
		\item $S$ is bounded, $S$ has a maximum but no minimum.

		      Let $a = \inf S, b = \max S$ then $S = \halfopenleft{a, b}$.
		\item $S$ is bounded above, not bounded below and doesn't have a maximum.

		      Let $b = \sup S$ then $S = \openinterval{-\infty, b}$.
		\item $S$ is bounded above, not bounded below and has a maximum.

		      Let $b = \max S$ then $S = \halfopenleft{-\infty, b}$.
		\item $S$ is bounded below, not bounded above and doesn't have a minimum.

		      Let $a = \inf S$ then $S = \openinterval{a, \infty}$.
		\item $S$ is bounded below, not bounded above and has a minimum.

		      Let $a = \min S$ then $S = \halfopenright{a, \infty}$.
		\item $S$ is unbounded below and above.

		      $S = \mathbb{R} = \openinterval{-\infty, \infty}$.
	\end{enumerate}

	It remains to prove that sets of these forms are connected. Let $J$ be a set of one of these 9 forms. Assume that $J$ is not connected then there are open subsets $U, V\subseteq \mathbb{R}$ such that $U\cap J$ and $V\cap J$ disconnect $J$. Let $a\in U\cap J$ and $b\in V\cap J$. Without loss of generality, assume that $a < b$. $\closedinterval{a, b}\subseteq J$ because $J$ is a set of one of those 9 forms. $U$ and $V$ are open subsets of $\mathbb{R}$ so there exist $r_{1}, r_{2} > 0$ such that $\halfopenright{a, a + r_{1}} \subseteq U\cap J$ and $\halfopenleft{b - r_{2}, b} \subseteq V\cap J$. There exists $\varepsilon > 0$ such that $a < a + \varepsilon < a + r_{1}$ and $b - r_{2} < b - \varepsilon$.

	Let $c = \sup(U\cap \closedinterval{a,b})$. Because $U\cap J, V\cap J$ are disjoint, $\halfopenright{a, a + \varepsilon}$ and $\halfopenleft{b - \varepsilon, b}$ are disjoint. Therefore $a + \varepsilon \leq c \leq b - \varepsilon$. $c$ is either in $U$ or $V$. If $c \in U$ then there is a neighborhood $\openinterval{c - \delta, c + \delta} \subseteq U$, which contradicts the definition of $c$. If $c \in V$ then there is a neighborhood $\openinterval{c - \delta, c + \delta} \subseteq V$, which also contradicts the definition of $c$. Thus $J$ is connected.
\end{proof}

\begin{exercise}{1.2}[Definition of Smoothness at a Boundary Point]
	If $I$ is an interval containing at least one boundary point, then a function $f: I\to \mathbb{R}$ is called \textbf{smooth} if it extends to a smooth function on an open interval containing $I$. In this case, the $k$th-order \textbf{derivative} of $f$, denoted by $f^{(k)}: I\to \mathbb{R}$, means the restriction to $I$ of the $k$th-order derivative of such an extension.

	For example, $f: \closedinterval{a, b}\to \mathbb{R}$ is called smooth if there exist $\varepsilon > 0$ and a smooth function $\hat{f}: \openinterval{a-\varepsilon, b+\varepsilon}\to \mathbb{R}$ such that $f(t) = \hat{f}(t)$ for all $t\in\closedinterval{a, b}$. In this case, $f^{(k)}: \closedinterval{a, b}\to \mathbb{R}$ is defined as the restriction to $\closedinterval{a, b}$ of ${\hat{f}}^{(k)}$.

	If $f: \closedinterval{a, b}\to \mathbb{R}$ is smooth, show that $f^{(k)}: \closedinterval{a, b}\to \mathbb{R}$ can equivalently be defined without reference to any extension (so the above definition is independent of the choice of extension). For example, $f'(a)$ can be defined as the \textit{right-hand} limit $f'(a) = \lim\limits_{h\to 0^{+}}\frac{f(a + h) - f(a)}{h}$, and $f'(b)$ can be similarly defined as a left-hand limit.
\end{exercise}

\begin{proof}
	Let $\hat{f}: \openinterval{a - \varepsilon_{1}, b + \varepsilon_{2}} \to \mathbb{R}$ be an extension of $f: \closedinterval{a, b} \to \mathbb{R}$.

	$f(a) = \hat{f}(a)$ and $f(b) = \hat{f}(b)$. $f^{(1)}$ is independent of the choice of extension because
	\begin{align*}
		f^{(1)}(a) & = {\hat{f}}^{(1)}(a) = \lim\limits_{h\to 0}\frac{\hat{f}(a + h) - \hat{f}(a)}{h} = \lim\limits_{h \to 0^{+}}\frac{\hat{f}(a + h) - \hat{f}(a)}{h} = \lim\limits_{h \to 0^{+}}\frac{f(a + h) - f(a)}{h} \\
		f^{(1)}(b) & = {\hat{f}^{(1)}}(b) = \lim\limits_{h\to 0}\frac{\hat{f}(b + h) - \hat{f}(b)}{h} = \lim\limits_{h\to 0^{-}}\frac{\hat{f}(b + h) - \hat{f}(b)}{h} = \lim\limits_{h\to 0^{-}}\frac{f(b + h) - f(b)}{h}
	\end{align*}

	and for every $c \in \openinterval{a,b}$
	\begin{align*}
		f^{(1)}(c) & = {\hat{f}}^{(1)}(c) = \lim\limits_{h\to 0}\frac{\hat{f}(c + h) - \hat{f}(c)}{h} = \lim\limits_{h\to 0}\frac{f(c + h) - f(c)}{h}.
	\end{align*}

	Assume $f^{(k)}$ is independent of the choice of extension. We will show that $f^{(k+1)}$ is independent of the choice of extension. Apply the previous part (first-order derivative) to $f^{(k)}$, we conclude that $f^{(k+1)}$ is independent of the choice of extension.

	From the principle of mathematical induction, $f^{(k)}$ is independent of the choice of extension for every positive integer $k$.
\end{proof}

\begin{exercise}{1.3}\label{exercise:1.3}
	A \textbf{logarithmic spiral} means a plane curve of the form
	\begin{equation*}
		\gamma(t) = c(e^{\lambda t}\cos(t), e^{\lambda t}\sin(t)), t \in \mathbb{R},
	\end{equation*}

	where $c, \lambda \in \mathbb{R}$ with $c\ne 0$. Figure 1.4 shows the restriction to $\halfopenright{0, \infty}$ of a logarithmic spiral with $\lambda < 0$. Use an improper integral to prove that such a restriction has finite arc length even though it makes infinitly many loops around the origin.
\end{exercise}

\begin{proof}
	The arc length of $\gamma$ between $0$ and $\infty$ is
	\begingroup
	\allowdisplaybreaks
	\begin{align*}
		\int^{\infty}_{0}\abs{\gamma'(t)}dt & = \int^{\infty}_{0}\abs{c\tuple{\lambda e^{\lambda t}\cos(t) - e^{\lambda t}\sin(t), \lambda e^{\lambda t}\sin(t) + e^{\lambda t}\cos(t)}}dt                          \\
		                                    & = \abs{c} \int^{\infty}_{0}\sqrt{\tuple{\lambda e^{\lambda t}\cos(t) - e^{\lambda t}\sin(t)}^{2} + \tuple{\lambda e^{\lambda t}\sin(t) + e^{\lambda t}\cos(t)}^{2}}dt \\
		                                    & = \abs{c} \int^{\infty}_{0} \sqrt{\lambda^{2}e^{2\lambda t} + e^{2\lambda t}}dt                                                                                       \\
		                                    & = \abs{c}\sqrt{\lambda^{2} + 1}\int^{\infty}_{0} e^{\lambda t}dt                                                                                                      \\
		                                    & =  \abs{c}\sqrt{\lambda^{2} + 1} \lim\limits_{A\to \infty}\frac{1}{\lambda}e^{\lambda t}\Bigl{\vert}^{A}_{0}                                                          \\
		                                    & = \frac{-\abs{c}\sqrt{\lambda^{2} + 1}}{\lambda}
	\end{align*}
	\endgroup

	which is finite.
\end{proof}

\begin{exercise}{1.4}
	The curve $\gamma(t) = (\sin(t), \cos(t) + \ln(\tan(t/2)))$, $t\in \openinterval{\pi/2, \pi}$, is shown in Fig. 1.5. Demonstrate that for every point $\mathbf{p}$ of its image, the segment of the tangent line at $\mathbf{p}$ between $\mathbf{p}$ and the $y$-axis has length 1. This curve is called a \textbf{tractrix}, because it represents the path of an object tethered by a length-1 rope to a tractor moving upward along the positive $y$-axis.
\end{exercise}

\begin{proof}
	The derivative of the parametrized curve $\gamma$ is
	\begin{align*}
		\gamma'(t) & = \tuple{ \cos(t), -\sin(t) + \frac{1}{2}\cot(t/2)\cdot\frac{1}{\cos^{2}(t/2)} } \\
		           & = \tuple{ \cos(t), -\sin(t) + \frac{1}{\sin(t)} }                                \\
		           & = \tuple{ \cos(t), \frac{\tuple{\cos(t)}^{2}}{\sin(t)} }.
	\end{align*}

	The tangent line of $\gamma$ at time $t$ has parametric equation
	\begin{align*}
		\begin{cases}
			x = \sin(t) + u\cos(t) \\
			y = \cos(t) + \ln(\tan(t/2)) + u\dfrac{\tuple{\cos(t)}^{2}}{\sin(t)}
		\end{cases}.
	\end{align*}

	This tangent line intersects $y$-axis at the point which corresponds to $u = -\tan(t)$, and it has coordinates
	\begin{align*}
		\tuple{0, \cos(t) + \ln(\tan(t/2)) + \tan(t)\frac{\tuple{\cos(t)}^{2}}{\sin(t)}} = \tuple{0, 2\cos(t) + \ln(\tan(t/2))}.
	\end{align*}

	So the segment of the tangent line of $\gamma$ at time $t$ between $\gamma(t)$ and the $y$-axis has length
	\begin{align*}
		\sqrt{\tuple{\sin(t) - 0}^{2} + \tuple{2\cos(t) + \ln(\tan(t/2)) - \cos(t) - \ln(\tan(t/2))}^{2}} = \sqrt{\tuple{\sin(t)}^{2} + \tuple{\cos(t)}^{2}} = 1.
	\end{align*}
\end{proof}

\begin{exercise}{1.5}
	Prove Proposition 1.7.

	The derivative of a curve $\gamma: I\to \mathbb{R}^{n}$ at time $t\in I$ is given by the formula
	\begin{align*}
		\gamma'(t) = \lim\limits_{h\to 0} \frac{\gamma(t + h) - \gamma(t)}{h}.
	\end{align*}
\end{exercise}

\begin{proof}
	Let $f_{1}, \ldots, f_{n}$ be the component functions of $\gamma$. By definition $\gamma'(t) = \tuple{f_{1}'(t), \ldots, f_{n}'(t)}$.

	Let $\varepsilon > 0$.
	\begin{align*}
		\abs{\frac{\gamma(t + h) - \gamma(t)}{h} - \gamma'(t)} & = \abs{\tuple{\frac{f_{1}(t + h) - f_{1}(t)}{h} - f_{1}'(t), \ldots, \frac{f_{n}(t + h) - f_{n}(t)}{h}} - f_{n}'(t)} \\
		                                                       & = \sqrt{\sum^{n}_{k=1} \abs{\frac{f_{k}(t + h) - f_{k}(t)}{h} - f_{k}'(t)}^{2}}.
	\end{align*}

	$f_{k}$ is smooth for every $k \in \set{1, \ldots, n}$ so for each $k \in \set{1, \ldots, n}$, there is a neighborhood $\openinterval{-\delta_{k}, \delta_{k}}$ of 0 such that
	\begin{align*}
		\abs{\frac{f_{k}(t + h) - f_{k}(t)}{h} - f_{k}'(t)} < \frac{\varepsilon}{\sqrt{n}}
	\end{align*}

	whenever $t \in \openinterval{-\delta_{k}, \delta_{k}}$. Let $\delta = \min\set{\delta_{1}, \ldots, \delta_{n}}$ then whenever $t \in \openinterval{-\delta, \delta}$
	\begin{align*}
		\abs{\frac{\gamma(t + h) - \gamma(t)}{h} - \gamma'(t)} < \varepsilon.
	\end{align*}

	By the definition of limit, we conclude that $\gamma'(t) = \lim\limits_{h\to 0} \dfrac{\gamma(t + h) - \gamma(t)}{h}$.
\end{proof}

\begin{exercise}{1.6}\label{exercise:1.6}
	If all three component functions of a space curve $\gamma$ are quadratic functions, prove that the image of $\gamma$ is contained in a plane.
\end{exercise}

\begin{proof}
	Let $\gamma: I \to \mathbb{R}^{3}$ be a space curve whose component functions are quadratic functions. Assume those component functions are given by
	\begin{align*}
		f_{1}(t) & = a_{1}t^{2} + b_{1}t + c_{1} \\
		f_{2}(t) & = a_{2}t^{2} + b_{2}t + c_{2} \\
		f_{3}(t) & = a_{3}t^{2} + b_{3}t + c_{3}
	\end{align*}

	Consider four arbitrary points $\mathbf{p_{1}} = \tuple{f_{1}(t_{1}), f_{2}(t_{1}), f_{3}(t_{1})}$, $\mathbf{p_{2}} = \tuple{f_{1}(t_{2}), f_{2}(t_{2}), f_{3}(t_{2})}$, $\mathbf{p_{3}} = \tuple{f_{1}(t_{3}), f_{2}(t_{3}), f_{3}(t_{3})}$, and $\mathbf{p_{4}} = \tuple{f_{1}(t_{4}), f_{2}(t_{4}), f_{3}(t_{4})}$
	\begin{align*}
		D = \begin{vmatrix}
			    f_{1}(t_{1}) - f_{1}(t_{4}) & f_{2}(t_{1}) - f_{2}(t_{4}) & f_{3}(t_{1}) - f_{3}(t_{4}) \\
			    f_{1}(t_{2}) - f_{1}(t_{4}) & f_{2}(t_{2}) - f_{2}(t_{4}) & f_{3}(t_{2}) - f_{3}(t_{4}) \\
			    f_{1}(t_{3}) - f_{1}(t_{4}) & f_{2}(t_{3}) - f_{2}(t_{4}) & f_{3}(t_{3}) - f_{3}(t_{4})
		    \end{vmatrix} = \begin{vmatrix}
			                    f_{1}(t_{1}) & f_{2}(t_{1}) & f_{3}(t_{1}) & 1 \\
			                    f_{1}(t_{2}) & f_{2}(t_{2}) & f_{3}(t_{2}) & 1 \\
			                    f_{1}(t_{3}) & f_{2}(t_{3}) & f_{3}(t_{3}) & 1 \\
			                    f_{1}(t_{4}) & f_{2}(t_{4}) & f_{3}(t_{4}) & 1
		                    \end{vmatrix}
	\end{align*}

	Denote $\mathbf{v_{0}} = \begin{pmatrix}1 \\ 1 \\ 1 \\ 1\end{pmatrix}$, $\mathbf{v_{1}} = \begin{pmatrix}t_{1} \\ t_{2} \\ t_{3} \\ t_{4}\end{pmatrix}$, $\mathbf{v_{2}} = \begin{pmatrix}t_{1}^{2} \\ t_{2}^{2} \\ t_{3}^{2} \\ t_{4}^{2}\end{pmatrix}$. Due to the multilinear property
	\begin{align*}
		D & = \begin{vmatrix}a_{1}\mathbf{v_{2}} + b_{1}\mathbf{v_{1}} + c_{1}\mathbf{v_{0}} & a_{2}\mathbf{v_{2}} + b_{2}\mathbf{v_{1}} + c_{2}\mathbf{v_{0}} & a_{3}\mathbf{v_{2}} + b_{3}\mathbf{v_{1}} + c_{3}\mathbf{v_{0}} & \mathbf{v_{0}}\end{vmatrix} \\
		  & = \begin{vmatrix}a_{1}\mathbf{v_{2}} + b_{1}\mathbf{v_{1}} & a_{2}\mathbf{v_{2}} + b_{2}\mathbf{v_{1}} & a_{3}\mathbf{v_{2}} + b_{3}\mathbf{v_{1}} & \mathbf{v_{0}}\end{vmatrix}.
	\end{align*}

	On the other hand, $a_{1}\mathbf{v_{2}} + b_{1}\mathbf{v_{1}}, a_{2}\mathbf{v_{2}} + b_{2}\mathbf{v_{1}}, a_{3}\mathbf{v_{2}} + b_{3}\mathbf{v_{1}}$ are linearly dependent as
	\begin{align*}
		\dim \operatorname{span}(a_{1}\mathbf{v_{2}} + b_{1}\mathbf{v_{1}}, a_{2}\mathbf{v_{2}} + b_{2}\mathbf{v_{1}}, a_{3}\mathbf{v_{2}} + b_{3}\mathbf{v_{1}}) \leq \dim\operatorname{span}(\mathbf{v_{1}}, \mathbf{v_{2}}) \leq 2
	\end{align*}

	because $\operatorname{span}(a_{1}\mathbf{v_{2}} + b_{1}\mathbf{v_{1}}, a_{2}\mathbf{v_{2}} + b_{2}\mathbf{v_{1}}, a_{3}\mathbf{v_{2}} + b_{3}\mathbf{v_{1}})$ is a subspace of $\operatorname{span}(\mathbf{v_{1}}, \mathbf{v_{2}})$ so $D = 0$. Therefore $\mathbf{p_{1}}, \mathbf{p_{2}}, \mathbf{p_{3}}, \mathbf{p_{4}}$ are coplanar, which means any four points in the image of $\gamma$ are coplanar.

	If any three points on $\gamma$ are collinear then it follows that the image of $\gamma$ is contained in a line. Otherwise, there exists three points on $\gamma$ which are not collinear, then any other point lies on the plane passing through those three points. In either case, the image of $\gamma$ is contained in a plane.
\end{proof}

\begin{exercise}{1.7}
	Find a plane curve parametrized by arc length that traverses the unit circle clockwise starting at $(0, -1)$.
\end{exercise}

\begin{proof}
	Such a parametrized plane curve is $\gamma: \closedinterval{0,1} \to \mathbb{R}^{2}$ given by
	\begin{align*}
		\gamma(t) = \tuple{\sin(t), -\cos(t)}.
	\end{align*}

	It is parametrized by arc length because
	\begin{align*}
		\abs{\gamma'(t)} = \sqrt{\tuple{\cos(t)}^{2} + \tuple{\sin(t)}^{2}} = 1
	\end{align*}

	for every $t\in \closedinterval{0, 1}$.
\end{proof}

\begin{exercise}{1.8}
	Compute the arc length of $\gamma(t) = (2t, 3t^{2})$, $t\in \closedinterval{0,1}$.
\end{exercise}

\begin{proof}
	The arc length of $\gamma$ from time 0 to 1 is
	\begin{align*}
		\int^{1}_{0}\abs{\gamma'(t)}dt & = \int^{1}_{0}\sqrt{\tuple{2}^{2} + \tuple{6t}^{2}}dt = 6\int^{1}_{0} \sqrt{t^{2} + \frac{1}{9}}dt                                         \\
		                               & = 6\left( \frac{t}{2}\sqrt{t^{2} + \frac{1}{9}} + \frac{1}{18}\ln\left(t + \sqrt{t^{2} + \frac{1}{9}}\right) \right){\Bigg{\vert}}^{1}_{0} \\
		                               & = \sqrt{10} + \frac{1}{3}\ln(3 + \sqrt{10}).\qedhere
	\end{align*}
\end{proof}

\begin{exercise}{1.9}
	Let $a, b > 0$. Find the maximum and minimum speed of the ellipse $\gamma(t) = (a\cos(t), b\sin(t))$.
\end{exercise}

\begin{proof}
	The speed of $\gamma$ at time $t$ is
	\begin{align*}
		\sqrt{\tuple{-a\sin(t)}^{2} + \tuple{b\cos(t)}^{2}} & = \sqrt{a^{2}\tuple{\sin(t)}^{2} + b^{2}\tuple{\cos(t)}^{2}} \\
		                                                    & = \sqrt{a^{2} + (b^{2} - a^{2})\tuple{\cos(t)}^{2}}          \\
		                                                    & = \sqrt{b^{2} + (a^{2} - b^{2})\tuple{\sin(t)}^{2}}.
	\end{align*}

	If $a \geq b$ then the maximum and minimum speed of $\gamma$ are $a$ and $b$, respectively.

	If $a < b$ then the maximum and minimum speed of $\gamma$ are $b$ and $a$, respectively.
\end{proof}

\begin{exercise}{1.10}
	Prove that the arc length, $L$, of the graph of the polar coordinate function $f(\theta)$, $\theta \in \closedinterval{a, b}$ is
	\begin{equation*}
		L = \int^{b}_{a}\sqrt{{r(\theta)}^{2} + {r'(\theta)}^{2}}\mathrm{d}\theta.
	\end{equation*}
\end{exercise}

\begin{proof}
	In the Cartesian plane, $x = r(\theta)\cos(\theta)$ and $y = r(\theta)\sin(\theta)$. The arc length is
	\begin{align*}
		L & = \int^{b}_{a}\sqrt{\tuple{\frac{dx}{d\theta}}^{2} + \tuple{\frac{dy}{d\theta}}^{2}}d\theta                                                         \\
		  & = \int^{b}_{a}\sqrt{\tuple{r'(\theta)\cos(\theta) - r(\theta)\sin(\theta)}^{2} + \tuple{r'(\theta)\sin(\theta) + r(\theta)\cos(\theta)}^{2}}d\theta \\
		  & = \int^{b}_{a}\sqrt{\tuple{r(\theta)}^{2} + \tuple{r'(\theta)}^{2}}d\theta.\qedhere
	\end{align*}
\end{proof}

\begin{exercise}{1.11}
	Figure 1.6 shows a polygonal approximation of the regular curve $\gamma: \closedinterval{a, b}\to \mathbb{R}^{n}$.  This polygonal approximation is determined by a partition, $a = t_{0} < t_{1} < t_{2} < \cdots < t_{k} = b$. The sum of the lengths of the line segments equals $L = \sum^{k-1}_{i=0}\abs{\gamma(t_{i+1}) - \gamma(t_{i})}$. The \textit{mesh} of the partition is defined as $\delta = \max\limits_{i}\set{t_{i+1} - t_{i}}$. Prove that $L$ converges to the arc length of $\gamma$ for every sequence of partitions for which $\delta\to 0$.
\end{exercise}

\begin{exercise}{1.12}
	See the Jupyter Notebook. You might want to use Google Colab instead.
\end{exercise}

\begin{exercise}{1.13}
	See the Jupyter Notebook. You might want to use Google Colab instead.
\end{exercise}

\begin{exercise}{1.14}
	Research the mathematical definition of a \textbf{catenary} curve, the physics explanation of why a heavy chain fixed at both ends will dangle in the shape of a catenary, and the history of the study and use of the catenary. Discuss the relationship to the square-wheel tricycle exhibit at the Museum of Mathematics in New York.
\end{exercise}

\section{The inner product}

\begin{exercise}{1.15}
	Prove the Schwarz inequality from Lemma 1.12.
\end{exercise}

\begin{proof}
	If $\mathbf{y} = 0$ then $\abs{\innerprod{\mathbf{x}, \mathbf{y}}} = 0 = \abs{\mathbf{x}}\abs{\mathbf{y}}$. Otherwise,
	\begin{align*}
		\mathbf{x} = \underbrace{\frac{\innerprod{\mathbf{x}, \mathbf{y}}}{\abs{\mathbf{y}}^{2}}\mathbf{y}}_{\mathbf{y}^{\parallel}} + \underbrace{\left(\mathbf{x} - \frac{\innerprod{\mathbf{x}, \mathbf{y}}}{\abs{\mathbf{y}}^{2}}\mathbf{y}\right)}_{\mathbf{y}^{\perp}}
	\end{align*}

	in which $\mathbf{y}^{\parallel}$ is orthogonal to $\mathbf{y}^{\perp}$ and $\abs{\mathbf{x}}^{2} = \abs{\mathbf{y}^{\parallel}}^{2} + \abs{\mathbf{y}^{\perp}}^{2}$. So
	\begin{align*}
		\frac{\abs{\innerprod{\mathbf{x}, \mathbf{y}}}}{\abs{\mathbf{y}}} = \abs{\frac{\innerprod{\mathbf{x}, \mathbf{y}}}{\abs{\mathbf{y}}^{2}}\mathbf{y}} = \abs{\mathbf{y}^{\parallel}} \leq \abs{\mathbf{x}}
	\end{align*}

	which means $\abs{\innerprod{\mathbf{x}, \mathbf{y}}} \leq \abs{\mathbf{x}}\abs{\mathbf{y}}$.

	Thus $\innerprod{\mathbf{x}, \mathbf{y}} \leq \abs{\innerprod{\mathbf{x}, \mathbf{y}}} \leq \abs{\mathbf{x}}\abs{\mathbf{y}}$.
\end{proof}

\begin{exercise}{1.16}
	Let $\gamma$ be a \textbf{logarithmic spiral}, as defined in Exercise~\ref{exercise:1.3}. Prove that the angle between $\gamma(t)$ and $\gamma'(t)$ is a constant function of $t$.
\end{exercise}

\begin{proof}
	\begin{align*}
		\innerprod{\gamma(t), \gamma'(t)} & = \innerprod{c\tuple{e^{\lambda t}\cos(t), e^{\lambda t}\sin(t)}, ce^{\lambda t}\tuple{\lambda\cos(t) - \sin(t), \lambda \sin(t) + \cos(t)}} \\
		                                  & = c^{2}e^{2\lambda t}\innerprod{\tuple{\cos(t), \sin(t)}, \tuple{\lambda \cos(t) - \sin(t), \lambda \sin(t) + \cos(t)}}                      \\
		                                  & = c^{2}e^{2\lambda t}\tuple{\lambda \tuple{\cos(t)}^{2} - \sin(t)\cos(t) + \lambda \tuple{\sin(t)}^{2} + \cos(t)\sin(t)}                     \\
		                                  & = \lambda c^{2}e^{2\lambda t},                                                                                                               \\
		\abs{\gamma(t)}                   & = ce^{\lambda t},                                                                                                                            \\
		\abs{\gamma'(t)}                  & = ce^{\lambda t}\sqrt{1 + \lambda^{2}}.
	\end{align*}

	Therefore
	\begin{align*}
		\cos \angle{\tuple{\gamma(t), \gamma'(t)}} = \frac{\lambda c^{2}e^{\lambda t}}{c^{2}e^{2\lambda t}\sqrt{1 + \lambda^{2}}} = \frac{\lambda}{\sqrt{1 + \lambda^{2}}}
	\end{align*}

	which means the angle between $\gamma(t)$ and $\gamma'(t)$ is a constant function of $t$.
\end{proof}

\begin{exercise}{1.17}
	Let $\mathbf{x} = (0, 2), \mathbf{y} = (3, 4) \in \mathbb{R}^{2}$. Find the component and the projection of $\mathbf{x}$ in the direction of $\mathbf{y}$. Write $\mathbf{x}$ as a sum of two vectors, one parallel to $\mathbf{y}$ and the other orthogonal to $\mathbf{y}$.
\end{exercise}

\begin{proof}
	The component of $\mathbf{x}$ in the direction of $\mathbf{y}$ is $\frac{\innerprod{\mathbf{x}, \mathbf{y}}}{\abs{\mathbf{y}}^{2}} = \frac{8}{25}$.
	\begin{align*}
		\tuple{0,2} & = \mathbf{x} = \frac{\innerprod{\mathbf{x}, \mathbf{y}}}{\abs{\mathbf{y}}^{2}}\mathbf{y} + \left(\mathbf{x} - \frac{\innerprod{\mathbf{x}, \mathbf{y}}}{\abs{\mathbf{y}}^{2}}\mathbf{y}\right) \\
		            & = \frac{8}{25}\tuple{3, 4} + \left(\tuple{0, 2} - \frac{8}{25}\tuple{3, 4}\right)                                                                                                              \\
		            & = \tuple{\frac{24}{25}, \frac{32}{25}} + \tuple{\frac{-24}{25}, \frac{18}{25}}.\qedhere
	\end{align*}
\end{proof}

\begin{exercise}{1.18}
	Let $\mathbf{x} = (1,2,3) \in \mathbb{R}^{3}$ and let $\mathcal{V} = \operatorname{span}\set{(1,0,1), (1,1,0)}$. Write $\mathbf{x}$ as a sum of two vectors, one in $\mathcal{V}$ and the other orthogonal to every member of $\mathcal{V}$.
\end{exercise}

\begin{proof}
	$\set{\tuple{1, 0, 1}, \tuple{1, 1, 0}, \mathbf{x}}$ is a basis of $\mathbb{R}^{3}$. Apply the Gram-Schmidt process to this set of vectors, we obtain the following orthonormal basis
	\begin{align*}
		\mathbf{e_{1}} & = \tuple{\frac{1}{\sqrt{2}}, 0, \frac{1}{\sqrt{2}}},                   \\
		\mathbf{e_{2}} & = \tuple{\frac{1}{\sqrt{6}}, \frac{2}{\sqrt{6}}, \frac{-1}{\sqrt{6}}}, \\
		\mathbf{e_{3}} & = \tuple{\frac{-1}{\sqrt{3}}, \frac{1}{\sqrt{3}}, \frac{1}{\sqrt{3}}}
	\end{align*}

	in which $\operatorname{span}(\mathbf{e_{1}}) = \operatorname{span}(\tuple{1,0,1})$, $\operatorname{span}(\mathbf{e_{1}}, \mathbf{e_{2}}) = \operatorname{span}(\tuple{1,0,1}, \tuple{1,1,0})$, $\operatorname{span}(\mathbf{e_{1}}, \mathbf{e_{2}}, \mathbf{e_{3}}) = \operatorname{span}(\tuple{1,0,1}, \tuple{1,1,0}, \mathbf{x})$. As $\mathbf{e_{1}}, \mathbf{e_{2}}, \mathbf{e_{3}}$ is an orthonormal basis of $\mathbb{R}^{3}$
	\begin{align*}
		\mathbf{x} & = (\innerprod{\mathbf{x}, \mathbf{e_{1}}}\mathbf{e_{1}} + \innerprod{\mathbf{x}, \mathbf{e_{2}}}\mathbf{e_{2}}) + \innerprod{\mathbf{x}, \mathbf{e_{3}}}\mathbf{e_{3}}        \\
		           & = \left(\tuple{2,0,2} + \tuple{\frac{1}{3}, \frac{2}{3}, \frac{-1}{3}}\right) + \tuple{\frac{-4}{3}, \frac{4}{3}, \frac{4}{3}}                                                \\
		           & = \underbrace{\tuple{\frac{7}{3}, \frac{2}{3}, \frac{5}{3}}}_{\in\mathcal{V}} + \underbrace{\tuple{\frac{-4}{3}, \frac{4}{3}, \frac{4}{3}}}_{\in\mathcal{V}^{\perp}}.\qedhere
	\end{align*}
\end{proof}

\begin{exercise}{1.19}
	Prove that every orthonormal set in $\mathbb{R}^{n}$ must be linearly independent.
\end{exercise}

\begin{proof}
	Let $\set{\mathbf{v_{1}}, \ldots, \mathbf{v_{m}}}$ be an orthonormal set in $\mathbb{R}^{n}$. Assume $a_{1}\mathbf{v_{1}} + \cdots + a_{m}\mathbf{v_{m}} = \mathbf{0}$ for some $a_{1}, \ldots, a_{m} \in \mathbb{R}$. For each $i \in \set{1, \ldots, m}$
	\begin{align*}
		0 = \innerprod{0, \mathbf{v_{i}}} = \innerprod{\sum^{m}_{j=1}a_{j}\mathbf{v_{j}}, \mathbf{v_{i}}} = \sum^{m}_{j=1}a_{j}\innerprod{\mathbf{v_{j}}, \mathbf{v_{i}}} = a_{i}
	\end{align*}

	which implies $\set{\mathbf{v_{1}}, \ldots, \mathbf{v_{m}}}$ is linearly independent.
\end{proof}

\begin{exercise}{1.20}
	Prove that every subspace $\mathcal{V} \subseteq \mathbb{R}^{n}$ has an orthonormal basis.
\end{exercise}

\begin{proof}
	$\mathbb{R}^{n}$ is finite dimensional then so is any of its subspace. $\mathcal{V}$ is finite dimensional so it has a basis. Apply Gram-Schmidt process to a basis of $\mathcal{V}$, we obtain an orthonormal basis. Hence every subspace of $\mathbb{R}^{n}$ has an orthonormal basis.
\end{proof}

\begin{exercise}{1.21}
	Prove Lemma 1.16.

	If $\gamma, \beta: I \to \mathbb{R}^{n}$ is a pair of curves, and $c: I\to \mathbb{R}$ is a smooth function, then
	\begin{enumerate}[label={(\arabic*)}]
		\item $\frac{d}{dt}\innerprod{\gamma(t), \beta(t)} = \innerprod{\gamma'(t), \beta(t)} + \innerprod{\gamma(t), \beta'(t)}$.
		\item $\frac{d}{dt}(c(t)\gamma(t)) = c'(t)\gamma(t) + c(t)\gamma'(t)$.
	\end{enumerate}
\end{exercise}

\begin{proof}
	Let $\gamma_{1}, \ldots, \gamma_{n}$ be the component functions of $\gamma$, $\beta_{1}, \ldots, \beta_{n}$ be the component functions of $\beta$.
	\begin{align*}
		\frac{d}{dt}\innerprod{\gamma(t), \beta(t)} & = \frac{d}{dt}\left(\sum^{n}_{i=1}\gamma_{i}(t)\beta_{i}(t)\right)                                                \\
		                                            & = \sum^{n}_{i=1}(\gamma_{i}'(t)\beta_{i}(t) + \gamma_{i}(t)\beta_{i}'(t))                                         \\
		                                            & = \sum^{n}_{i=1}\gamma_{i}'(t)\beta_{i}(t) + \sum^{n}_{i=1}\gamma_{i}(t)\beta_{i}'(t)                             \\
		                                            & = \innerprod{\gamma'(t), \beta(t)} + \innerprod{\gamma(t), \beta'(t)},                                            \\
		\frac{d}{dt}(c(t)\gamma(t))                 & = \frac{d}{dt}\tuple{c(t)\gamma_{1}(t), \ldots, c(t)\gamma_{n}(t)}                                                \\
		                                            & = \tuple{c'(t)\gamma_{1}(t) + c(t)\gamma_{1}'(t), \ldots, c'(t)\gamma_{n}(t) + c(t)\gamma_{n}'(t)}                \\
		                                            & = \tuple{c'(t)\gamma_{1}(t), \ldots, c'(t)\gamma_{n}(t)} + \tuple{c(t)\gamma_{1}'(t), \ldots, c(t)\gamma_{n}'(t)} \\
		                                            & = c'(t)\gamma(t) + c(t)\gamma'(t).\qedhere
	\end{align*}
\end{proof}

\begin{exercise}{1.22}
	Is the converse of part (1) of Proposition 1.17 true?
\end{exercise}

\begin{proof}
	Yes.

	Assume that $\gamma(t)$ is orthogonal to $\gamma'(t)$ for all $t\in I$ then
	\begin{align*}
		0 = 2\innerprod{\gamma(t), \gamma'(t)} = \innerprod{\gamma(t), \gamma'(t)} + \innerprod{\gamma'(t), \gamma(t)} = \frac{d}{dt}\innerprod{\gamma(t), \gamma(t)}
	\end{align*}

	which means $\abs{\gamma(t)}^{2} = \innerprod{\gamma(t), \gamma(t)}$ is a constant on $I$. Therefore $\gamma$ has constant norm on $I$.
\end{proof}

\begin{exercise}{1.23}
	Let $\gamma: I \to \mathbb{R}^{3}$ be a regular space curve, and let $P \subseteq \mathbb{R}^{3}$ be a plane that does not intersect the image of $\gamma$. If $\gamma$ comes closest to $P$ at time $t_{0}$, prove that $\gamma'(t_{0})$ is parallel to $P$.
\end{exercise}

\begin{proof}
	Let the equation of $P$ be $ax + by + cz + d = 0$ in which $a, b, c, d\in\mathbb{R}$ and $a, b, c$ are not all zero. Without loss of generality, we can assume that $a^{2} + b^{2} + c^{2} = 1$ Let the component function of $\gamma: I \to \mathbb{R}^{3}$ be $\gamma_{x}, \gamma_{y}, \gamma_{z}$. The distance from $\gamma(t)$ to $P$ is
	\begin{align*}
		\frac{\abs{a\gamma_{x}(t) + b\gamma_{y}(t) + c\gamma_{z}(t) + d}}{\sqrt{a^{2} + b^{2} + c^{2}}} = \abs{a\gamma_{x}(t) + b\gamma_{y}(t) + c\gamma_{z}(t) + d}.
	\end{align*}

	Consider the expression $\abs{a\gamma_{x}(t) + b\gamma_{y}(t) + c\gamma_{z}(t) + d}^{2}$ in which $t\in I$. Denote $\mathbf{n} = \tuple{a, b, c}$.
	\begin{align*}
		\abs{a\gamma_{x}(t) + b\gamma_{y}(t) + c\gamma_{z}(t) + d}^{2} & = \abs{\innerprod{\mathbf{n}, \gamma(t)} + d}^{2} = \innerprod{\mathbf{n},\gamma(t)}^{2} + 2d\innerprod{\mathbf{n}, \gamma(t)} + d^{2}.
	\end{align*}

	Because this distance is minimum at time $t_{0}$ and the distance function is smooth, then $t = t_{0}$ is an extremum. According to Fermat's theorem, the derivative of the distance function at $t = t_{0}$ is zero. From Lemma 1.16
	\begin{align*}
		0 = \frac{d}{dt}\abs{a\gamma_{x}(t) + b\gamma_{y}(t) + c\gamma_{z}(t) + d}^{2} \Bigl{\vert}_{t=t_{0}} & = \left(2\innerprod{\mathbf{n}, \gamma(t)} \innerprod{\mathbf{n}, \gamma'(t)} + 2d\innerprod{\mathbf{n}, \gamma'(t)}\right) \Bigl{\vert}_{t=t_{0}} \\
		                                                                                                      & = 2\innerprod{\mathbf{n}, \gamma(t_{0})} \innerprod{\mathbf{n}, \gamma'(t_{0})} + 2d\innerprod{\mathbf{n}, \gamma'(t_{0})}                         \\
		                                                                                                      & = 2\innerprod{\mathbf{n}, \gamma'(t_{0})} (a\gamma_{x}(t_{0}) + b\gamma_{y}(t_{0}) + c\gamma_{z}(t_{0}) + d).
	\end{align*}

	On the other hand, $a\gamma_{x}(t_{0}) + b\gamma_{y}(t_{0}) + c\gamma_{z}(t_{0}) + d \ne 0$ because $P$ does not intersect the image of $\gamma$, so $\innerprod{\mathbf{n}, \gamma'(t_{0})} = 0$, which means $\gamma'(t_{0})$ is perpendicular to the normal vector $\mathbf{n}$ of $P$. Thus $\gamma'(t_{0})$ is parallel to $P$.
\end{proof}

\begin{exercise}{1.24}
	If $t_{0}\in I$ is the time at which the curve $\gamma: I \to \mathbb{R}^{n}$ is farthest from the origin, prove that $\gamma(t_{0})$ is orthogonal to $\gamma'(t_{0})$.
\end{exercise}

\begin{proof}
	The function $f: I \to \mathbb{R}$ given by $f(t) = \abs{\gamma(t)}^{2}$ is smooth. $f$ attains the maximum when $t = t_{0}$ so $f'(t_{0}) = 0$. Moreover, $\frac{d}{dt}\abs{\gamma(t)}^{2} = \frac{d}{dt}\innerprod{\gamma(t), \gamma(t)} = 2\innerprod{\gamma(t), \gamma'(t)}$ according to Lemma 1.16. Therefore $\innerprod{\gamma(t_{0}), \gamma'(t_{0})} = 0$, which implies $\gamma(t_{0})$ is orthogonal to $\gamma'(t_{0})$.
\end{proof}

\begin{exercise}{1.25}\label{exercise:1.25}
	If $\gamma: I \to \mathbb{R}^{n}$ is a regular curve, prove that
	\begin{equation*}
		\frac{d}{dt}\abs{\gamma(t)} = \innerprod{\gamma'(t), \frac{\gamma(t)}{\abs{\gamma(t)}}}.
	\end{equation*}
\end{exercise}

\begin{proof}
	Using the chain rule and Lemma 1.16
	\begin{align*}
		\frac{d}{dt}\abs{\gamma(t)} & = \frac{d}{dt}\sqrt{\innerprod{\gamma(t), \gamma(t)}} = \frac{1}{2\sqrt{\gamma(t), \gamma(t)}}(\innerprod{\gamma'(t),\gamma(t)} + \innerprod{\gamma(t),\gamma'(t)}) \\
		                            & = \frac{1}{\abs{\gamma(t)}}\innerprod{\gamma'(t), \gamma(t)}                                                                                                        \\
		                            & = \innerprod{\gamma'(t), \frac{\gamma(t)}{\abs{\gamma(t)}}}.\qedhere
	\end{align*}
\end{proof}

\section{Acceleration}

\begin{exercise}{1.26}
	What can be said about a space curve with constant acceleration? Compare to Exercise~\ref{exercise:1.6}.
\end{exercise}

\begin{proof}
	If the space curve $\gamma: I \to \mathbb{R}^{2}$ has constant acceleration then $\gamma_{x}'', \gamma_{y}'', \gamma_{z}''$ are constant functions in which $\gamma_{x}'', \gamma_{y}'', \gamma_{z}''$ are the component functions of $\gamma$. Therefore $\gamma_{x}, \gamma_{y}, \gamma_{z}$ are quadratic functions. From Exercise~\ref{exercise:1.6}, it follows that the image of $\gamma$ is contained in a plane.
\end{proof}

\begin{exercise}{1.27}
	If $\gamma$ is a curve in $\mathbb{R}^{n}$ with $\abs{\gamma(t)} = C$ (a constant), prove that $\innerprod{\mathbf{a}(t), -\gamma(t)} = {\abs{\mathbf{v}(t)}}^{2}$. Rewrite this as $\innerprod{\mathbf{a}(t), -\frac{\gamma(t)}{\abs{\gamma(t)}}} = \frac{{\abs{\mathbf{v}(t)}}^{2}}{C}$, and  notice that the left side is the component of $\mathbf{a}(t)$ in the direction of the center-pointing vector. Interpret this physically in terms of centripetal force.
\end{exercise}

\begin{proof}
	$\abs{\gamma(t)} = C$ so $\innerprod{\gamma'(t), \gamma(t)} = 0$.
	\begin{align*}
		0 = \frac{d}{dt}\innerprod{\gamma'(t), \gamma(t)} & = \innerprod{\gamma''(t), \gamma(t)} + \innerprod{\gamma'(t), \gamma'(t)} \\
		                                                  & = \innerprod{\mathbf{a}(t), \gamma(t)} + \abs{\mathbf{v}(t)}^{2}
	\end{align*}

	from which we have
	\begin{align*}
		\innerprod{\mathbf{a}(t), -\frac{\gamma(t)}{\abs{\gamma(t)}}} = -\frac{1}{\abs{\gamma(t)}}\innerprod{\mathbf{a}(t), \gamma(t)} = \frac{\abs{\mathbf{v}(t)}^{2}}{\abs{\gamma(t)}} = \frac{\abs{\mathbf{v}(t)}^{2}}{C}.
	\end{align*}

	My interpretation: As the object moves, its acceleration ``pull'' it towards the origin; The magnitude of the centripetal force is proportional to $\frac{\abs{\mathbf{v}(t)}^{2}}{C}$.
\end{proof}

\begin{exercise}{1.28}
	Find a space curve $\gamma: \mathbb{R} \to \mathbb{R}^{3}$ with acceleration function $\mathbf{a}(t) = (t^{2} - 1, t^{3}, t^{2} + 1)$. How unique is the solution?
\end{exercise}

\begin{proof}
	Such a curve is $\gamma(t) = \tuple{\frac{t^{4}}{12} - \frac{t^{2}}{2}, \frac{t^{5}}{60}, \frac{t^{4}}{12} + \frac{t^{2}}{2}}$. Every solution is of the form
	\begin{align*}
		\tuple{\frac{t^{4}}{12} - \frac{t^{2}}{2}, \frac{t^{5}}{60}, \frac{t^{4}}{12} + \frac{t^{2}}{2}} + \tuple{f_{x}(t), f_{y}(t), f_{z}(t)}
	\end{align*}

	in which $f_{x}, f_{y}, f_{z}$ are either quadratic, linear, or constant functions.
\end{proof}

\section{Reparametrization}

\begin{exercise}{1.29}
	Consider the following pair of plane curves:
	\begin{align*}
		\gamma(s) = (\cos(s), \sin(s)), \quad s\in \openinterval{-\pi, \pi}, \\
		\tilde{\gamma}(t) = \left(\frac{1-t^{2}}{1+t^{2}}, \frac{2t}{1+t^{2}}\right),\quad t\in\mathbb{R}.
	\end{align*}

	Verify that $\tilde{\gamma}$ is a reparametrization of $\gamma$.
\end{exercise}

\begin{exercise}{1.30}
	Let $\gamma: I \to \mathbb{R}^{n}$ be a regular curve, and let $\tilde{\gamma} = \gamma \circ \phi: \tilde{I} \to \mathbb{R}^{n}$ be a reparametrization of $\gamma$, as in Definition 1.22.
	\begin{enumerate}[label={(\arabic*)}]
		\item Is it possible that $I$ is unbounded while $\tilde{I}$ is bounded?
		\item Is it possible that $I$ is noncompact while $\tilde{I}$ is compact?
		\item Is it possible that $I = \openinterval{a, \infty}$ while $\tilde{I} = \halfopenright{b, \infty}$?
	\end{enumerate}
\end{exercise}

\begin{exercise}{1.31}
	Precisely state and prove the assertion that the definition of arc length is independent of parametrization for regular curves.
\end{exercise}

\begin{exercise}{1.32}
	Let $\gamma(t) = (t, t^{2})$, $t\in \mathbb{R}$, be the parabolic curve from Example 1.4. Find a unit-speed reparametrization, $\tilde{\gamma}(t) = (\tilde{x}(t), \tilde{y}(t))$, with $\tilde{\gamma}(0) = (0, 0)$. For this, use a computer algebra system to implement the method of the proof of Proposition 1.25.  Separately plot the components $\tilde{x}$ and $\tilde{y}$, which are defined by integrals. Is the computer able to simplify these components into closed-form expressions without integrals?
\end{exercise}

\begin{exercise}{1.33}
	Prove Proposition 1.27.
\end{exercise}

\begin{exercise}{1.34}
	Some texts define a \textit{closed curve} to mean a periodic regular curve $\gamma: \mathbb{R} \to \mathbb{R}^{n}$. This viewpoint is equivalent to ours because of Proposition 1.27. Assuming that this alternative definition of a closed curve has been adopted, complete the following necessary modification to Definition 1.28: A reparametrization of a closed curve $\gamma: \mathbb{R}\to \mathbb{R}^{n}$ is a function of the form $\tilde{\gamma} = \gamma\circ \phi: \mathbb{R} \to \mathbb{R}^{n}$, where $\phi: \mathbb{R} \to \mathbb{R}$ is a smooth bijection with nowhere-vanishing derivative satisfying the following condition: \ldots

	Fill in the blank so that a reparametrization of a closed curve is a closed curve.
\end{exercise}

\begin{exercise}{1.35}[Curve vs Parametrized Curve]
	Although this book uses the term ``curve'' as an abbreviation for ``parametrized curve'', some other texts define the term `curve'' ” in an alternative manner that is explored in this exercise.
	\begin{enumerate}[label={(\arabic*)}]
		\item  Consider two regular parametrized curves in $\mathbb{R}^{n}$ to be equivalent if one is a reparametrization of the other (as in Definition 1.22). Prove that this is an equivalence relation on the set of all regular parametrized curves in $\mathbb{R}^{n}$.
		\item Consider two parametrized \textit{closed} curves in $\mathbb{R}^{n}$ to be equivalent if one is a reparametrization of the other (as in Definition 1.28). Prove that this is an equivalence relation on the set of all parametrized closed curves in $\mathbb{R}^{n}$.
		\item Repeat (1) and (2) with ``reparametrization'' replaced by ``orientation-preserving reparametrization{.}''
		\item  Show that two parametrized simple closed curves have the same trace if and only if they are equivalent (that is, one is a reparametrization of the other).
		\item If two regular parametrized curves in $\mathbb{R}^{n}$ have the same trace, and each becomes one-to-one after removing finitely many points from its domain, must they be equivalent?
	\end{enumerate}
\end{exercise}

\section{Curvature}

\section{Plane Curves}

\section{Space Curves}

\section{Rigid Motions}
